{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarvNC/cs523/blob/master/s25_chapter5_handout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf6-j3ghE3hM"
      },
      "source": [
        "# Preface\n",
        "\n",
        "Update from the video: I introduce sklearn's built-in scalers, `MinMaxScaler` and `StandardScaler`, mostly as motivation for why we will want to write our own custom scaler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>Chapter Five</h1>\n",
        "</center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## LEARNING OBJECTIVES:\n",
        "- Look at next wrangling step up, scaling. Introduce several alternatives.\n",
        "- Capture final choice, RobustScaler, in a custom Transformer.\n",
        "- Place it correctly in Pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npa06nZLAvs-"
      },
      "source": [
        "# I. Scaling\n",
        "\n",
        "In this chapter I would like to look at the next step in our pipeline, scaling. The general idea is to try to \"smooth\" out our data so it is in the same \"ballpark\". As one example, the Fare column ranges from 0 to 500+. The Gender column ranges from 0 to 1. Quite a disparity! We will look at methods to deal with this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZiquu_S3vZG"
      },
      "source": [
        "## Set-up\n",
        "\n",
        "First bring in your library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ6MOQmuVewi"
      },
      "outputs": [],
      "source": [
        "github_name = 'smith'\n",
        "repo_name = 'cis423'\n",
        "source_file = 'library.py'\n",
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZqcJSxA6v1y"
      },
      "source": [
        "## Make sure transformers from chapter 4 are in your library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcGlW7YoSBbi"
      },
      "outputs": [],
      "source": [
        "customer_transformer  #6 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAUvxVCo5bta"
      },
      "outputs": [],
      "source": [
        "titanic_transformer  #4 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl6YCVfp1uyT"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/fickas/asynch_models/refs/heads/main/datasets/titanic_trimmed.csv'\n",
        "titanic_table = pd.read_csv(url)  #using our new package to read in an entire dataset - the coolest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoSy4udQ1uyT"
      },
      "outputs": [],
      "source": [
        "titanic_table.head()  #print first 5 rows of the table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-jtHSkd33ks"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic_table.drop(columns='Survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJHlx2tTBNNq"
      },
      "source": [
        "## Wrangle using your pipeline\n",
        "\n",
        "I added a Tukey check on `Fare`.  I am using the outer fence, meaning I am only interested in \"probables\".\n",
        "\n",
        "If you are still working on the `TukeyTransformer` from last chapter, you can leave it off for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-eO6gReCT-8"
      },
      "outputs": [],
      "source": [
        "transformed_df = titanic_transformer.fit_transform(titanic_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR2eIY3VUx86"
      },
      "outputs": [],
      "source": [
        "transformed_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YN4g2GQtqbN"
      },
      "outputs": [],
      "source": [
        "transformed_df.dtypes  #all numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au2AMeD0SaQG"
      },
      "outputs": [],
      "source": [
        "#making sure you have run Fare through Tukey\n",
        "transformed_df['Fare'].min(), transformed_df['Fare'].max()  #(0.0, 101.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y1huouejzY5"
      },
      "source": [
        "# II. Transforming columns as apples to apples\n",
        "<img src='https://www.dropbox.com/s/9fcc1crlxp19ijt/major_section.png?raw=1' width='300'>\n",
        "\n",
        "Looking ahead, I know some of the machine learning algorithms we will look at are sensitive to large range discrepencies between columns. As an example, look at the first 2 rows. If we calculate the difference between these 2 rows, we will get a difference of 20 for `Age` but a difference of 1 for `Class`. If we use something like Euclidean distance, treating each row as a point, we will use this formula to get the difference between the rows:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/y2pucqx4e8uokj6/Screen%20Shot%202021-09-01%20at%201.09.02%20PM.png?raw=1'>\n",
        "\n",
        "I hope you see that the `Age` difference (now squared to get to 400!) will swamp the `Class` difference (1 squared). We might as well drop all the columns with small values and just keep `Age` and `Fare`.\n",
        "\n",
        "But I want the other columns! They may carry valuable information. The solution is to get their ranges all in same ballpark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p_LKOQWWwcy"
      },
      "source": [
        "## Build a test table\n",
        "\n",
        "I'll use this for demo purposes. There are only 2 columns we need to worry about, `Age` and `Fare`. The remaining columns are binary or close to binary. So `Age` and `Fare` are the problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXmt0SpiW3A0"
      },
      "outputs": [],
      "source": [
        "demo_df = transformed_df[['Age', 'Fare']]\n",
        "demo_df.describe(include='all').T  # different than video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkH5dbKhS8X-"
      },
      "source": [
        "<pre>\n",
        "\n",
        "count\tmean\tstd\tmin\t25%\t50%\t75%\tmax\n",
        "Age\t1304.0\t31.574387\t14.346945\t1.0\t22.0\t30.5\t41.0\t74.0\n",
        "Fare\t1311.0\t25.253074\t28.750343\t0.0\t7.0\t13.0\t30.5\t101.0\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x29N4kIy6COG"
      },
      "source": [
        "## Reminder of what we have before scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c26cWUZ-52ir"
      },
      "outputs": [],
      "source": [
        "transformed_df['Age'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yJnCieu5_zP"
      },
      "outputs": [],
      "source": [
        "transformed_df['Fare'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7J-ZPU-4xj"
      },
      "source": [
        "# III. Min-Max scaling\n",
        "\n",
        "A very standard approach is to place each column in the range between 0 and 1 (inclusive). The formula for this is:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/j4j2jhkmuweej1s/Screen%20Shot%202021-09-01%20at%201.16.27%20PM.png?raw=1'>\n",
        "\n",
        "A little confusing. Read it as (a) take the min of the column, (b) take the max of the column, (c) take each value and scale as shown to get new value. This will give you a range of 0 to 1 for entire column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUV3oBwgClFw"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "Go ahead and test the formula above for the `Age` column. Apply the formula above to the column. See what the resulting min and max are. Should be 0 and 1.\n",
        "\n",
        "You should be able to use pandas methods to do what you want.\n",
        "\n",
        "BTW: I want the scaled Age in a new dataframe, `demo2`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8cqCF9zQzny"
      },
      "outputs": [],
      "source": [
        "demo2 = demo_df.copy()\n",
        "#your code below. You will need min and max of Age. Then use formula to scale the Age column. Note I needed no loops or comprehensions by using built-in pandas operators.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnlUB_klvYJc"
      },
      "outputs": [],
      "source": [
        "(demo2['Age'].min(), demo2['Age'].max())  #(0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-403nxhniXLl"
      },
      "outputs": [],
      "source": [
        "demo2['Age'].plot(kind='hist')  #shape has not changed, just scale of x-axis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J7LPiIBh-qO"
      },
      "source": [
        "Now do same for Fare column. Use demo2 as starting point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_LwwC_Ph9qO"
      },
      "outputs": [],
      "source": [
        "#your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Q-v1jVh9qP"
      },
      "outputs": [],
      "source": [
        "(demo2['Fare'].min(), demo2['Fare'].max())  #(0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBEwg3kMihZK"
      },
      "outputs": [],
      "source": [
        "demo2['Fare'].plot(kind='hist')  #ditto - no change to shape just to scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6Y4fxhinHQ"
      },
      "source": [
        "### You can see distribution has not changed\n",
        "\n",
        "`Age` still semi-normal.\n",
        "\n",
        "`Fare` still right-skewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLModKyPWlDC"
      },
      "source": [
        "### sklearn has a transformer\n",
        "\n",
        "<pre>\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit_transform(data)\n",
        "</pre>\n",
        "I hope you can see that this scaler  will give uniform ranges (between 0 and 1) to every column/feature. So apples to apples.\n",
        "\n",
        "Also note that this transformer is set up to work on the entire table, feature by feature. So you don't name a column/feature as target. The target is all the columns. But that is part of the problem. I may want to apply different scalers to different columns depending on their distributions. More later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEyu3z1DXFXG"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler =  MinMaxScaler()\n",
        "numpy_result = scaler.fit_transform(demo_df)  #does not return a dataframe!\n",
        "new_df = pd.DataFrame(numpy_result, columns=['Age', 'Fare'])  #turn it back into a dataframe\n",
        "new_df.describe(include='all').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McJoiDE8eBmX"
      },
      "outputs": [],
      "source": [
        "new_df['Age'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL61lKT9eBmX"
      },
      "outputs": [],
      "source": [
        "new_df['Fare'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRn47VsvJh4E"
      },
      "source": [
        "# VI. Standardization (Z-score Normalization) Apples to Pears\n",
        "\n",
        "This is a slightly different approach that is less senstive to outliers.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/bg7r231qrhfiiir/Screen%20Shot%202021-09-01%20at%201.56.09%20PM.png?raw=1'>\n",
        "\n",
        "We subtract the mean of the column and divide by the standard deviation (sigma) of the column.  In essence, use the old column to calculate how many sigmas a value is from the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksDu7F8KYa3p"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "\n",
        " Apply the formula above to the `Fare` column. See what the resulting min and max are. Are they 0 and 1?\n",
        "\n",
        "Remember that pandas has methods for mean and standard deviation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmvZzhdGRwgd"
      },
      "outputs": [],
      "source": [
        "demo2 = demo_df.copy()\n",
        "#your code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTVZbGdw9Yoc"
      },
      "outputs": [],
      "source": [
        "(demo2['Fare'].min(), demo2['Fare'].max())  #(-0.8783573098490415, 2.6346442491796966)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nLHAGdw-R5d"
      },
      "source": [
        "### Still mostly apples to apples\n",
        "\n",
        "Ranges from `-.87` to `2.63`. Seems ok."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrsWnZ8ynPfh"
      },
      "source": [
        "### pandas handles NaN values gracefully\n",
        "\n",
        "There are NaN values in both Fare and Age. Let's see what happens if we decide to do our own thing without pandas methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq07Gv5w0EFy"
      },
      "outputs": [],
      "source": [
        "from statistics import mean, stdev\n",
        "fares = demo_df['Fare'].to_list()\n",
        "me = mean(fares)\n",
        "me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvpYdCzNn-1u"
      },
      "source": [
        "The takehome message is to use pandas methods whenever you can - they know about NaN and deal with them gracefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5nkLejdYZwW"
      },
      "source": [
        "### There is a transformer for Z-score normalization\n",
        "\n",
        "<pre>\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit_transform(data)\n",
        "</pre>\n",
        "\n",
        "Note that this will not give you the same range across columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeOjCy0ZYCbZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "new_df = pd.DataFrame(scaler.fit_transform(demo_df), columns=['Age', 'Fare'])\n",
        "new_df.describe(include='all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk4yAAtJo0Hu"
      },
      "source": [
        "### Distribution roughly the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VXXI1SseEC0"
      },
      "outputs": [],
      "source": [
        "new_df['Age'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVSAbCzs4yYN"
      },
      "outputs": [],
      "source": [
        "new_df['Fare'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4qnf6P-osEG"
      },
      "source": [
        "But again, it only operates on the entire table and that may not be what we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPfgv21vZEcq"
      },
      "source": [
        "# V. Robust Scaler\n",
        "\n",
        "As we saw in chapter 4, any method that uses the mean and standard deviation has issues with outlier values. Outliers can skew a probability distribution and make data scaling using standardization difficult as the calculated mean and standard deviation will be skewed by the presence of the outliers.\n",
        "\n",
        "We also saw in chapter 4 that switching from mean and sigma to the use of quartiles could help with outliers. There is a scaler that uses the idea: the Robust Scaler.\n",
        "\n",
        "Here is the formula:\n",
        "\n",
        "`value_new = (value – median) / iqr #iqr = q3-q1`\n",
        "\n",
        "The resulting values have (approximately) a zero mean and median and a standard deviation of 1. If you want a more detailed discussion of this, see [Robust Scaler tutorial](https://proclusacademy.com/blog/robust-scaler-outliers/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P85bilcEZpZt"
      },
      "source": [
        "\n",
        "And as usual, sklearn has this transformer defined.\n",
        "\n",
        "<pre>\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scaler.fit_transform(data)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLX8owaFZfvg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "new_df = pd.DataFrame(scaler.fit_transform(demo_df), columns=['Age', 'Fare'])\n",
        "new_df.describe(include='all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvQpACyh38ip"
      },
      "source": [
        "## Still looking pretty good\n",
        "\n",
        "Both columns are in the apples to apples range. Note that if we had not applied Tukey to Fare, this would not be the case. When I tried RobustScaler without Tukey, I go a max of 35, which is too large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emKcIn50eGIu"
      },
      "outputs": [],
      "source": [
        "new_df['Age'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY47DuPO42s7"
      },
      "outputs": [],
      "source": [
        "new_df['Fare'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL220o760IFI"
      },
      "source": [
        "# VI. Box Cox normalizer\n",
        "\n",
        "Up until know, we have been looking at transforms that use linear methods. The next set we look at will attempt to normalize the dataset by unskewing it if necessary. This involves non-linear transformations that can be a bit murky in terms of interpretability. The first is a method called Box Cox. It takes a column of values in and does an exhaustive search on a parameter called lambda using this equation:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/esqhkw8cgyh4nya/Screen%20Shot%202022-09-22%20at%2010.09.45%20AM.png?raw=1' height=100>\n",
        "\n",
        "or simplified\n",
        "\n",
        "<img src='https://www.dropbox.com/s/xj6wxava4ofhgf4/Screen%20Shot%202022-09-22%20at%2010.10.24%20AM.png?raw=1' height=75>\n",
        "\n",
        "The old column values are `yi`. The new values are `yi(λ)`. A search is done over lambda between -5 to +5. The lambda value that gives the closest to a normal distribution of `yi(λ)` is chosen. See [Fit goodness paper](https://arxiv.org/pdf/1401.3812.pdf).\n",
        "\n",
        "Note the limit of Yi to the lambda minus 1 over lambda, as lambda approaches 0, is log yi. That covers the lambda=0 case.\n",
        "\n",
        "**The values `yi` must be strictly positive.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alEFW3sa_ZK9"
      },
      "source": [
        "<img src='https://capture.dropbox.com/NqYhzu3P2muKakkM?raw=1'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmXEafybaUJc"
      },
      "outputs": [],
      "source": [
        "demo_df['Fare'] += .00001  #to make all values positive\n",
        "demo_df['Age'] += .00001  #to make all values positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__5rcErmuQO2"
      },
      "outputs": [],
      "source": [
        "demo_df['Fare'].min()  #was 0 now slightly positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75mCT8GoY1aH"
      },
      "source": [
        "### Using another built-in sklearn transformer\n",
        "\n",
        "Which again operates on all columns: you cannot select column to work on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyi8QmRW0Ksi"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "scaler = PowerTransformer(method='box-cox')\n",
        "new_df = pd.DataFrame(scaler.fit_transform(demo_df), columns=['Age', 'Fare'])\n",
        "new_df.describe(include='all').T # different than video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRxYpHAsZHk8"
      },
      "source": [
        "### Mostly apples to apples\n",
        "\n",
        "`Age` ranges from -2.75 to 3.23. `Fare` from -1.18 to 1.16. I'd say this is acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzRQl8_RbFCb"
      },
      "outputs": [],
      "source": [
        "new_df['Age'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCw6qUQQdz25"
      },
      "outputs": [],
      "source": [
        "new_df['Fare'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lAHqaXIv-uj"
      },
      "outputs": [],
      "source": [
        "new_df['Fare'].skew()  #-0.7805798491257958"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLgtBdUm6m7"
      },
      "source": [
        "# VII. Yeo Johnson\n",
        "\n",
        "A modifiction of Box Cox that removes the restriction of positive numbers is the Yeo-Johnson transform:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/jgx6tc04jf3wvd8/Screen%20Shot%202021-09-02%20at%2010.43.07%20AM.png?raw=1' height=100>\n",
        "\n",
        "Like Box-Cox, a search is done over lambda to find the value that most closely produces a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AF_l_sEdzAg"
      },
      "outputs": [],
      "source": [
        "scaler = PowerTransformer(method='yeo-johnson')\n",
        "new_df = pd.DataFrame(scaler.fit_transform(demo_df), columns=['Age', 'Fare'])\n",
        "new_df.describe(include='all').T  #different than video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVFoM3uAZf7E"
      },
      "source": [
        "### Again, mostly apples to apples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2l_NCr_e6_s"
      },
      "outputs": [],
      "source": [
        "new_df['Age'].plot(kind='hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnJvhmQle6_t"
      },
      "outputs": [],
      "source": [
        "new_df['Fare'].plot(kind='hist') # different than video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeA5V0qJwuEo"
      },
      "outputs": [],
      "source": [
        "new_df['Fare'].skew()  #-0.07294022105966637"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGEJpcfpkTR8"
      },
      "source": [
        "# X. So which to choose?\n",
        "\n",
        "I see 3 questions we need to ask:\n",
        "\n",
        "1. Do the machine learning algorithms we will use care about skewness? Do they prefer a normal distribution? Related, are they sensitive to outliers?\n",
        "\n",
        "2. Does apples-to-apples matter? Should all columns be in roughly the same range?\n",
        "\n",
        "3. Do we care about interpretability, i.e., explaining what we did to a human in an understandable way?\n",
        "\n",
        "* The answer to 1 is \"some do care\". We will end up looking at more than one algorithm and some may care and some not. Note that we have done outlier clipping at least on the `Fare` column before we get to here. That can remove outliers but can still leave a skewed set of data.\n",
        "\n",
        "* The answer to 2 is \"most algorithms care\" and in at least one case it is critical.\n",
        "\n",
        "* The answer to 3 is yes, we will care about explaintability. We will be presenting results to an end user through a web site. I want to pay attention to being able to explain the analysis we do.\n",
        "\n",
        "My choice: Robust Scaler. It gives us  similar ranges and an extra benefit is that it is explainable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1obJeubv8xp"
      },
      "source": [
        "# Challenge 1\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "<img src='https://www.dropbox.com/s/esqhkw8cgyh4nya/Screen%20Shot%202022-09-22%20at%2010.09.45%20AM.png?raw=1' height=100>\n",
        "\n",
        "Try your hand at the Box-Cox scaler. I know we have seen it is built-in to the PowerTransformer. But I would like you to see if you can code it yourself. Here is what I would like you to do:\n",
        "\n",
        "1. Use `demo_df['Fare']` after you make it positive - no 0s allowed. We did that by adding a small positive constant. I'd like you to use the full column, not just the last 100.\n",
        "\n",
        "2. There may be a way to do this all within pandas, but I decided to pull the column out into a Python list and work with that. So do that. Rest of steps assume you are working with a Python list - no pandas involved.\n",
        "\n",
        "3. Run through lambda values between integers -5 and +5. Warning: `lambda` is a reserved word so don't name your variables with it.\n",
        "\n",
        "4. Compute the box-cox formula on the `Fare` list to give you a new set of scaled values. **Big warning**: you will have to skip over `NaN` values in the list. So the scaled list you come up with will be smaller than full column.\n",
        "\n",
        "4. I will actually give you a skew function to use as a \"goodness\" measure. Record the skew along with lambda value.\n",
        "\n",
        "5. Choose the lambda value that gets you closest to 0 skew. Use the Python `sorted` method to do this.\n",
        "\n",
        "6. Use that lambda value to compute final scaled new values. Warning: it will not match up with sklearn transformer but that's ok. You should end up with a new list that is scaled.\n",
        "\n",
        "I'll give you my results to match against.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHIOPdrwooYW"
      },
      "outputs": [],
      "source": [
        "#I'll give you the imports I used so you can match against my results\n",
        "import math\n",
        "from scipy.stats import skew\n",
        "\n",
        "#code to compute skew values (11 of them: -5...0...5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUGPXi8BDh2v"
      },
      "outputs": [],
      "source": [
        "skew_values  #see my answer below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm3d-K-Px0jQ"
      },
      "source": [
        "My skew values:\n",
        "\n",
        "<pre>\n",
        "[(-5, -1.665718952729915),\n",
        " (-4, -1.6657189527299139),\n",
        " (-3, -1.6657189527299143),\n",
        " (-2, -1.6657189527299139),\n",
        " (-1, -1.6657189527245735),\n",
        " (0, -1.5861097748492763),\n",
        " (1, 1.4799936335977462),\n",
        " (2, 2.259382730322621),\n",
        " (3, 2.6588580482287707),\n",
        " (4, 2.9168228215178744),\n",
        " (5, 3.092549932345237)]\n",
        " </pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFc21BqsrTEr"
      },
      "outputs": [],
      "source": [
        "#use sorted function on skew_values to find skew closest to 0\n",
        "final_l =\n",
        "final_l  #1 (value for lambda that gives result closest to 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz_SNaOsueST"
      },
      "outputs": [],
      "source": [
        "#compute final set of scaled values using final_l, i.e., best lambda value you found.\n",
        "#I used list comprehension. Remember to skip NaN values.\n",
        "\n",
        "final_fare ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keaUOUFu7A6I"
      },
      "outputs": [],
      "source": [
        "#before scaling\n",
        "\n",
        "print(fare_list[:10])  #[7.00001, 1e-05, 20.00001, nan, 24.00001, 1e-05, 7.00001, 7.00001, 8.00001, 1e-05]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTsMCoNbCSrz"
      },
      "outputs": [],
      "source": [
        "#after scaling\n",
        "\n",
        "print(final_fare[:10])  #[6.00001, -0.99999, 19.00001, 23.00001, -0.99999, 6.00001, 6.00001, 7.00001, -0.99999, -0.99999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBAef0sXvACq"
      },
      "outputs": [],
      "source": [
        "(min(final_fare), max(final_fare))  #(-0.99999, 100.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJGn0De5vl4p"
      },
      "source": [
        "# Challenge 2\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Build a custom RobustScaler, one that accepts a specific column to operate on. In this way, we can apply it to selected columns but not necessarily the entire table.\n",
        "\n",
        "The transformer you build should compute the iqr and median in the fit method. Then apply them in the transform method.\n",
        "\n",
        "I can see two ways of attacking this problem:\n",
        "\n",
        "1. Do it from scratch. Given pandas methods for doing most of the work, this seems the easiest to me. However, be careful with binary columns. They can give you zero values for `iqr` and `median` during `fit`. If so, want to skip doing any changes in `transform` method.\n",
        "\n",
        "2. Use sklearn's RobustScaler. To do this, you will have to create a new table with just the selected column, fit the new table then transform based on the fitted scaler instance. This has the benefit of using a built-in transformer. But it seems more complex to code.\n",
        "\n",
        "I'll accept either approach.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO9RiYibv-XD"
      },
      "outputs": [],
      "source": [
        "#from scratch option\n",
        "\n",
        "class CustomRobustTransformer(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"Applies robust scaling to a specified column in a pandas DataFrame.\n",
        "    This transformer calculates the interquartile range (IQR) and median\n",
        "    during the `fit` method and then uses these values to scale the\n",
        "    target column in the `transform` method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    column : str\n",
        "        The name of the column to be scaled.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    target_column : str\n",
        "        The name of the column to be scaled.\n",
        "    iqr : float\n",
        "        The interquartile range of the target column.\n",
        "    med : float\n",
        "        The median of the target column.\n",
        "  \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU_Tcia_1IkO"
      },
      "outputs": [],
      "source": [
        "#wrapping RobustScaler option\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "class CustomRobustTransformer_wrapped(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Applies robust scaling to a specified column using sklearn's RobustScaler.\n",
        "\n",
        "    This transformer wraps the sklearn RobustScaler to apply it to a single\n",
        "    column of a pandas DataFrame. It calculates the interquartile range (IQR)\n",
        "    and median during the `fit` method and then uses these values to scale the\n",
        "    target column in the `transform` method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    column : str\n",
        "        The name of the column to be scaled.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    target_column : str\n",
        "        The name of the column to be scaled.\n",
        "    scaler : sklearn.preprocessing.RobustScaler\n",
        "        The underlying RobustScaler instance.\n",
        "    \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQZ224PTv-XK"
      },
      "source": [
        "## Here are test cases\n",
        "\n",
        "Please do not change them. I give you the results I expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlFCbZXP9tat"
      },
      "source": [
        "### Errors first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWtUH8dB0fbK"
      },
      "outputs": [],
      "source": [
        "CustomRobustTransformer('Fare').transform(transformed_df)  #AssertionError: NotFittedError: This CustomRobustTransformer instance is not fitted yet. Call \"fit\" with appropriate arguments before using this estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7eLOY3k0uHx"
      },
      "outputs": [],
      "source": [
        "CustomRobustTransformer('fare').fit(transformed_df)  #AssertionError: CustomRobustTransformer.fit unrecognizable column fare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HV7MsQL9yWw"
      },
      "source": [
        "### Match my results on simple dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4Mzv9Tgv-XL"
      },
      "outputs": [],
      "source": [
        "training_data = [[1,2,3,4,5],\n",
        "                 [6,7,8,9,10],\n",
        "                 [11,12,13,14,15]]\n",
        "training_df = pd.DataFrame(training_data, columns=['a','b','c','d','e'])\n",
        "training_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PNeoaJ4v-XL"
      },
      "outputs": [],
      "source": [
        "scalers_for_columns = [(col, CustomRobustTransformer(col)) for col in training_df.columns]  #build separate scaler for each col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OEBQXDLv-XL"
      },
      "outputs": [],
      "source": [
        "scaled_df = training_df\n",
        "for col,scaler in scalers_for_columns:\n",
        "  scaler.fit(scaled_df)  #broke out on separate line - this should do most of the work\n",
        "  scaled_df = scaler.transform(scaled_df)\n",
        "\n",
        "scaled_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230jNoKp-Cym"
      },
      "source": [
        "|index|a|b|c|d|e|\n",
        "|---|---|---|---|---|---|\n",
        "|0|-1\\.0|-1\\.0|-1\\.0|-1\\.0|-1\\.0|\n",
        "|1|0\\.0|0\\.0|0\\.0|0\\.0|0\\.0|\n",
        "|2|1\\.0|1\\.0|1\\.0|1\\.0|1\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SL3Tm8Rv-XO"
      },
      "source": [
        "<img src='' height=100>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qd1Xt_jzD-a"
      },
      "outputs": [],
      "source": [
        "#Oracle\n",
        "robust_scaler = RobustScaler()\n",
        "robust_scaler.fit_transform(training_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y4iCBmVv-XO"
      },
      "outputs": [],
      "source": [
        "test_data2 = [[2,3,10],\n",
        "              [7,8,11],\n",
        "              [12,np.nan,12]]\n",
        "test2_df = pd.DataFrame(test_data2, columns=['b','c','f'])\n",
        "test2_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRSthk4nJDbM"
      },
      "outputs": [],
      "source": [
        "scalers_for_columns = [(col, CustomRobustTransformer(col)) for col in test2_df.columns]  #build separate scaler for each col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HjRZ5H6JDbY"
      },
      "outputs": [],
      "source": [
        "scaled_df = test2_df\n",
        "for col,scaler in scalers_for_columns:\n",
        "  scaler.fit(scaled_df)\n",
        "  scaled_df = scaler.transform(scaled_df)\n",
        "\n",
        "scaled_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed15Kw9H-UZA"
      },
      "source": [
        "|index|b|c|f|\n",
        "|---|---|---|---|\n",
        "|0|-1\\.0|-1\\.0|-1\\.0|\n",
        "|1|0\\.0|1\\.0|0\\.0|\n",
        "|2|1\\.0|NaN|1\\.0|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7ce2mWuzb2T"
      },
      "outputs": [],
      "source": [
        "#Oracle\n",
        "robust_scaler = RobustScaler()\n",
        "robust_scaler.fit_transform(test2_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgNyvFtX-aQP"
      },
      "source": [
        "### Notice the NaN value is left alone\n",
        "\n",
        "That is good. We will deal with it later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCBZo1s6nbHA"
      },
      "source": [
        "# Challenge 3\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Go ahead and try it on `transformed_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YifR7dTe_gKX"
      },
      "outputs": [],
      "source": [
        "#your code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iama7d-gi-6-"
      },
      "outputs": [],
      "source": [
        "scaled_df.describe(include='all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWAhk9ovoL2Y"
      },
      "source": [
        "### Apples to apples\n",
        "\n",
        "All columns roughly in 0 to 1 range.\n",
        "\n",
        "|index|count|mean|std|min|25%|50%|75%|max|\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "|Age|1304\\.0|0\\.0565466580561834|0\\.7551023683278205|-1\\.5526315789473684|-0\\.4473684210526316|0\\.0|0\\.5526315789473685|2\\.289473684210526|\n",
        "|Gender|1313\\.0|0\\.3488194973343488|0\\.47677833844440637|0\\.0|0\\.0|0\\.0|1\\.0|1\\.0|\n",
        "|Class|1312\\.0|0\\.3978658536585366|1\\.0430526490417245|-1\\.0|0\\.0|0\\.0|1\\.0|2\\.0|\n",
        "|Married|1312\\.0|0\\.34375|0\\.4751399890067327|0\\.0|0\\.0|0\\.0|1\\.0|1\\.0|\n",
        "|Fare|1311\\.0|0\\.5214074038008991|1\\.2234188537091832|-0\\.5531914893617021|-0\\.2553191489361702|0\\.0|0\\.7446808510638299|3\\.74468085106383|\n",
        "|Joined\\_Belfast|1313\\.0|0\\.06473724295506474|0\\.24615539898364328|0\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|Joined\\_Cherbourg|1313\\.0|0\\.19268849961919268|0\\.3945607792644899|0\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|Joined\\_Queenstown|1313\\.0|0\\.06930693069306931|0\\.2540721241868543|0\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|Joined\\_Southampton|1313\\.0|-0\\.32673267326732675|0\\.46919729323158754|-1\\.0|-1\\.0|0\\.0|0\\.0|0\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2tISPgUoVnU"
      },
      "source": [
        "# Challenge 4\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Try it on the cable customer dataset. Add it to the pipeline as the last step. Only need it for `'Time Spent'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mV_4aTzo1Bi"
      },
      "outputs": [],
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQPM6PqZXgmAHfRYTcDZseyALRyVwkBtKEo_rtaKq_C7T0jycWxH6QVEzTzJCRA0m8Vz0k68eM9tDm-/pub?output=csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEQwkk2Co1Bi"
      },
      "outputs": [],
      "source": [
        "customers_df = pd.read_csv(url)\n",
        "customers_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpJZr_rj-p7J"
      },
      "outputs": [],
      "source": [
        "customer_features = customers_df.drop(columns='Rating')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYxtig9qo1Bj"
      },
      "outputs": [],
      "source": [
        "customer_transformer = Pipeline(steps=[\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKykDlLCo1Bj"
      },
      "outputs": [],
      "source": [
        "transformed_customer_df = customer_transformer.fit_transform(customer_features)\n",
        "transformed_customer_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfCY0RsNo1Bo"
      },
      "source": [
        "### What I see\n",
        "\n",
        "|index|ID|Gender|Experience Level|Time Spent|OS|Age|ISP\\_AT&amp;T|ISP\\_Cox|ISP\\_HughesNet|ISP\\_Xfinity|\n",
        "|---|---|---|---|---|---|---|---|---|---|---|\n",
        "|0|3|1\\.0|1\\.0|NaN|1\\.0|NaN|0|0|0|1|\n",
        "|1|27|0\\.0|1\\.0|-1\\.6057441253263711|0\\.0|-0\\.6896551724137931|0|1|0|0|\n",
        "|2|30|1\\.0|1\\.0|0\\.6202909362178289|NaN|-0\\.7586206896551724|0|1|0|0|\n",
        "|3|40|1\\.0|1\\.0|-0\\.5315180902648265|0\\.0|-0\\.4827586206896552|0|0|0|1|\n",
        "|4|52|1\\.0|1\\.0|0\\.7814248414770603|1\\.0|-0\\.13793103448275862|0|0|0|1|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ayicj3o1Bo"
      },
      "source": [
        "# Challenge 5\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Add `CustomRobustTransformer` to your GitHub library.\n",
        "\n",
        "\n",
        "As a preprocessing step, I'd recommend asking Gemini to do full documentation and type hints on it and add that to your library."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs523",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
