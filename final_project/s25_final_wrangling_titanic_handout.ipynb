{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elty4rVxh2rm"
      },
      "source": [
        "# Announcement\n",
        "\n",
        "I made a big switch here. In video I use the Pima dataset from Midterm 1. But I decided it was too confusing to start messing with this dataset during the final project. It seemed a lot more consistent to stick with Titanic, so that is what I did. Steps are the same, with caveat below.\n",
        "\n",
        "I made a change to  *Feature reduction*. I have given you a new way (new library) that is more useful than the pandas `.corr` method.\n",
        "\n",
        "\n",
        "I also removed the need to save the test set. We really do not need it.\n",
        "\n",
        "Finally, I am asking you to build the markdown (md) desription of your pipeline including a screenshot of it. Refer back end to Chapter 7 for a model of what you need. You may be able to copy and paste what I have in Chapter 7 then edit it to fit your new pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>First Notebook for Final Project (Wrangling)</h1>\n",
        "</center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "I am using Titanic dataset. You will need to fill in with your own. So replace my csv, random_state and pipeline with your own.\n",
        "\n",
        "Reminder: here is a list of places you can find data to use: https://careerfoundry.com/en/blog/data-analytics/where-to-find-free-datasets/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QocLjVfejK-"
      },
      "source": [
        "# Notes on your choice of dataset\n",
        "\n",
        "* I would like you to find a csv dataset that has a mixture of numeric and categorical columns. So you can show off your transformers!\n",
        "\n",
        "* I'd like a binary label column, i.e., what you will predict.\n",
        "\n",
        "* Do not worry about number of rows. If they are below 500 you can use SMOTE to increase them. If they are over 5000, you can use downsample to decrease them.\n",
        "\n",
        "* Don't worry about number of feature columns. I can help you reduce a large number down to 10 or less.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba9zmGLRwRT2"
      },
      "source": [
        "# I'd like to know the dataset you choose. Email me a link.\n",
        "\n",
        "Do this before starting this notebook. I may have some guidance that will help you and potentially direct you off of dead-end paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZiquu_S3vZG"
      },
      "source": [
        "## Set-up\n",
        "\n",
        "First bring in your library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ6MOQmuVewi"
      },
      "outputs": [],
      "source": [
        "github_name = 'marvnc'\n",
        "repo_name = 'cs523'\n",
        "source_file = 'library.py'\n",
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfdayQezyI8p"
      },
      "source": [
        "## Demo with Titanic\n",
        "\n",
        "##Caveat\n",
        "\n",
        "You will not match my results. You have your own dataset. I am just giving you the outline of the steps I expect you to follow.\n",
        "\n",
        "But you will need your dataset stored in a way that you can load it (and I can load it). This may mean downloading from web (e.g., kaggle) and then uploading to GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVTLBYampYbo"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/fickas/asynch_models/main/datasets/titanic_trimmed.csv'  #trimmed version\n",
        "\n",
        "titanic_trimmed = pd.read_csv(url)\n",
        "titanic_trimmed.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvZy9mwFVrGf"
      },
      "outputs": [],
      "source": [
        "len(titanic_trimmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIHq9aUHNcxo"
      },
      "source": [
        "# Break out into features and labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH0g0IVGauIa"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic_trimmed.drop(columns='Survived')\n",
        "labels = titanic_trimmed['Survived'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdQPZk3nU9cY"
      },
      "outputs": [],
      "source": [
        "labels.count(1)/len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo2xS_S8VfAj"
      },
      "source": [
        "# Downsampling (optional)\n",
        "\n",
        "Here is code to reduce the number of rows while keeping the new table stratified on the target column.\n",
        "\n",
        "You can skip this step if your table has less than 5000 rows. You could probably skip it with less than 10000. The whole point is to avoid tuning (next notebook) that takes hours. In a real setting, you would probably have to bite the bullet and wait that long.\n",
        "\n",
        "Note that I am setting N=1000 just for demo purposes. You should use 5000 or even greater as your value if you do need to downsample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--vlfk6QRyCH"
      },
      "outputs": [],
      "source": [
        "N=1000  #size of table you want, really not needed for such a small table as Titanic\n",
        "target = 'Survived'\n",
        "original_df = titanic_trimmed.copy()\n",
        "\n",
        "downsample_df = original_df.groupby(target, group_keys=False).apply(lambda x: x.sample(int(np.rint(N*len(x)/len(original_df))))).sample(frac=1).reset_index(drop=True)\n",
        "downsample_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6f2kEcPVSdG"
      },
      "outputs": [],
      "source": [
        "downsample_df['Survived'].to_list().count(1)/len(downsample_df)  #same as original so seems to work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4esshSt-9zWx"
      },
      "source": [
        "### For those who want to know\n",
        "\n",
        "Here is brief explanation:\n",
        "\n",
        "* `original_df.groupby(target, group_keys=False)`: This groups the dataframe by the target column. The `group_keys=False` ensures that the original grouping columns are not included in the resulting dataframe.\n",
        "\n",
        "* The apply method is used to apply a function to each group:\n",
        "\n",
        " * `lambda x: x.sample(int(np.rint(N*len(x)/len(original_df))))`: This function samples from each group based on the ratio of the group size to the total dataframe size multiplied by N (the number of samples you want from the entire dataframe).\n",
        " * The `np.rint` function rounds the number to the nearest integer.\n",
        " * `sample(frac=1)`: This shuffles the rows of the resulting dataframe.\n",
        "\n",
        "* reset_index(drop=True): This resets the index of the dataframe and drops the original index.\n",
        "\n",
        "In essence, the code calculates the number of samples needed from each group directly, based on the proportion of the group in the original dataframe, and then samples that number of rows from each group. It then shuffles the combined downsampled dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27t4zzb9z-rZ"
      },
      "source": [
        "# Feature reduction (optional)\n",
        "\n",
        "Some will need to drop columns. I'll give you a new library.  I like it because it works with a table with a mixture of datatypes, e.g., categorical columns.\n",
        "\n",
        "You should be able to use `correlation_df` (below) in place of the code you worked on earlier that used the pandas correlation table.\n",
        "\n",
        "You can play around with the threshold to get to your target number of features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsGXLwEgzXO8"
      },
      "source": [
        "### New correlation method\n",
        "\n",
        "The `titanic_trimmed.corr()` given  will give you an error if you have mixed column types. This new method handles this fine. When comparing two columns, it uses:\n",
        "\n",
        "* Cramer's V for categorical-categorical.\n",
        "* correlation ratio for numeric-categorical.\n",
        "* Pearson for numeric-numeric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d783PEr7yDqY"
      },
      "outputs": [],
      "source": [
        "!pip install dython\n",
        "from dython.nominal import associations\n",
        "\n",
        "# Simple one-liner to get correlations:\n",
        "correlations = associations(titanic_trimmed, nominal_columns=['Joined', 'Gender', 'Class'])  #you have to tell it which are categorical\n",
        "correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaGKOxumgSFI"
      },
      "outputs": [],
      "source": [
        "# Convert to dataframe\n",
        "correlation_df = correlations['corr']\n",
        "correlation_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30EoCWYfgVNu"
      },
      "source": [
        "## If you downsample and/or do feature reduction\n",
        "\n",
        "Save the resulting table out to github so you do not have to keep repeating the process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5cttx1Qyw51"
      },
      "source": [
        "# Define your pipeline\n",
        "\n",
        "This is up to you for your own dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC3UL45mYZin"
      },
      "outputs": [],
      "source": [
        "\n",
        "titanic_transformer = Pipeline(steps=[\n",
        "    ('map_gender', CustomMappingTransformer('Gender', {'Male': 0, 'Female': 1})),\n",
        "    ('map_class', CustomMappingTransformer('Class', {'Crew': 0, 'C3': 1, 'C2': 2, 'C1': 3})),\n",
        "    ('target_joined', CustomTargetTransformer(col='Joined', smoothing=10)),\n",
        "    ('tukey_age', CustomTukeyTransformer(target_column='Age', fence='outer')),\n",
        "    ('tukey_fare', CustomTukeyTransformer(target_column='Fare', fence='outer')),\n",
        "    ('scale_age', CustomRobustTransformer(target_column='Age')),\n",
        "    ('scale_fare', CustomRobustTransformer(target_column='Fare')),\n",
        "    ('impute', CustomKNNTransformer(n_neighbors=5)),\n",
        "    ], verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPK8KUzPfFTm"
      },
      "source": [
        "## Take a screenshot of your pipeline\n",
        "\n",
        "Save it on GitHub. Then use Chapter 7 markdown example to build your own pipeline description. Also save this on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiA2A8YtMB6a"
      },
      "source": [
        "## Find random state for splitting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNTsKca8YZin"
      },
      "outputs": [],
      "source": [
        "titanic_transformed_df = titanic_transformer.fit_transform(titanic_features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzeY6slfZ1AD"
      },
      "outputs": [],
      "source": [
        "titanic_transformed_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8zeQ0qM06DQ"
      },
      "source": [
        "# Find random_state value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86fBb9mFYT5H"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "rs, _ = find_random_state(titanic_transformed_df, labels, titanic_transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWiFQNcLeH9f"
      },
      "outputs": [],
      "source": [
        "rs  #whatever it is for your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeWE3VHEeM1D"
      },
      "source": [
        "### Remember this value for next notebook, tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdK8fRNOnJLY"
      },
      "source": [
        "## Split your dataset and save fitted pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFbdEo7ynPQ8"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(titanic_features, labels, test_size=0.2, random_state=rs)\n",
        "production_pipeline = titanic_transformer.fit(X_train, y_train)  #now ready for production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYAh5XzDZrby"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(production_pipeline, \"final_fully_fitted_pipeline.pkl\")  #Move this to GitHub where you will use it in production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3NrbnIKaBEB"
      },
      "outputs": [],
      "source": [
        "ptransformer = joblib.load(\"final_fully_fitted_pipeline.pkl\")  # Make sure can get it back\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPXhZpe5X2J2"
      },
      "source": [
        "# Congrats\n",
        "\n",
        "You are done with the wrangling stage. Ready to move on to training and tuning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzw5fLmAdq7J"
      },
      "source": [
        "# Reminder\n",
        "\n",
        "You should have the following files stored on GitHub.\n",
        "\n",
        "1. Screenshot of pipeline.\n",
        "\n",
        "2. Markdown pipeline description that references screenshot.\n",
        "\n",
        "3. `final_fully_fitted_pipeline.pkl`\n",
        "\n",
        "4. Value for `rs` to use in train_test_split. Saving this to GitHub is optional. You can just remember it and use it in tuning notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
