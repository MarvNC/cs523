{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarvNC/cs523/blob/main/s25_chapter7_handout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preface\n",
        "\n",
        "Unfortunately, the `ce.TargetEncoder` seen in video has gone out of date; the `category_encoders` library does not appear to be adequately maintained. I will help you implement target-encoding from scratch in a custom encoder. It is not too bad and enlightening to see what is happening behind the scenes. And you know it will not go out of date!\n",
        "\n",
        "It is also the case that I will ask you to override two of your results in this notebook: random numbers to do data splitting. I am doing this to stay consistent with videos. In Challenge 5 I will explain more.\n",
        "\n",
        "Finally, I created a detailed description of the pipeline we are using. I think it is important for the end-user to know how we wrangled the data. Keep a note of it: I will ask you to recreate it for your own dataset in the final project."
      ],
      "metadata": {
        "id": "VG_k2Y1cpgZt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>Chapter Seven</h1>\n",
        "</center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## LEARNING OBJECTIVES:\n",
        "- Need to update Pipeline, replacing one-hot encoding with target encoding. Discussion of target encoding principles.\n",
        "- Pivot from wrangling to modeling (prediction, classification).\n",
        "- Use of train-test split concept to evaluate our models.\n",
        "- Tune one of `train_test_split` function's parameters, `random_state`, using variance modeling.\n",
        "- Concept of *data leakage* and how to avoid it.\n",
        "- Exploration of another of `train_test_split` function's parameters, `test_size`, and results compared to a published article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npa06nZLAvs-"
      },
      "source": [
        "#I. First steps toward modeling - data split\n",
        "\n",
        "This chapter is a bridge between wrangling and exploring machine learning models to predict (AKA classify). It introduces a key step in supervised learning: splitting data into a training set and a test set.\n",
        "\n",
        "Let's go ahead and bring in the raw dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZiquu_S3vZG"
      },
      "source": [
        "##Set-up\n",
        "\n",
        "First bring in your library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "github_name = 'smith'\n",
        "repo_name = 'mlops'\n",
        "source_file = 'library.py'\n",
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ],
      "metadata": {
        "id": "AZ6MOQmuVewi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAUvxVCo5bta"
      },
      "source": [
        "type(CustomKNNTransformer)  #from last chapter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpc1PfunSdiv"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/fickas/asynch_models/refs/heads/main/datasets/titanic_trimmed.csv'\n",
        "\n",
        "titanic_trimmed = pd.read_csv(url)\n",
        "len(titanic_trimmed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JXOj2s9VXHC"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic_trimmed.drop(columns='Survived')\n",
        "titanic_features.head()  #print first 5 rows of the table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s50JGZMKcCUF"
      },
      "source": [
        "labels = titanic_trimmed['Survived'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nAvadgfRfDd"
      },
      "source": [
        "#II. The foundation of model exploration: the train-test split\n",
        "\n",
        "Here is the problem. Assume we have historical data, i.e., data about what is happened in the past. We either collected it by hand (Titanic) or automatically logged it (Cable customers). We will assume that the data is \"labeled\". That means we know the outcome. So we know who survived. We know how customers rated their cable experience. We know how a medical diagnosis turned out. This is a big assumption. It puts the problem into what is called **Supervised Learning**. What if we are lacking labels, i.e., we do not know how things turned out? Then we fall under **Unsupervised Learning**. We will focus on Supervised Learning in this class. You might want to take a Data Mining class if you are interested in Unsupervised Learning.\n",
        "\n",
        "<img src='https://dfzljdn9uc3pi.cloudfront.net/2020/10317/1/fig-1-2x.jpg' height=250>\n",
        "\n",
        "The problem is that we want to train a model with our historical data so that it can predict future (as yet unseen) data. And btw, this future data will not be labeled. We have to predict the label! So we won't know if we are right or wrong (at least not immediately).\n",
        "\n",
        "One solution is to break our dataset into 2 pieces: a training set and a test set. We hold out the test set to act as the \"future\" data. The beauty of this is that this test set is labeled. So we can judge how we might do in the future. And have ammunition when we take it to the customer (a cable company, a medical clinic) that we will do a good job when they put our model into production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urFYo6HUmtig"
      },
      "source": [
        "#III. How to split?\n",
        "\n",
        "The simplest is to randomly choose some portion of the dataset to use for training and the remainder to set aside for testing. And sklearn has a function to do that. Check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzJJ_RornLXy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(titanic_features,   #all columns except target column\n",
        "                                                    labels,             #a list of values from target column, e.g., Survived\n",
        "                                                    test_size=0.2,      #percent to put in test set\n",
        "                                                    shuffle=True,        #whether to randomly shuffle rows (and labels) first\n",
        "                                                    random_state=0)     #a seed for the random shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqnGHkvUnYvc"
      },
      "source": [
        "Notice that I use `test_size=0.2` to say I want to hold 20% of the dataset out for testing. This is a hyperparameter that you can change. Also notice that I use `shuffle=True` to say I want a random (shuffled) selection.\n",
        "\n",
        "Let's see what is produced.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwGhOZdaoc0m"
      },
      "source": [
        "X_train.head()  #differs from video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6rHzNYJuN4B"
      },
      "source": [
        "y_train[:10]   #training labels that go with X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IV. We have an issue with One-hot encoding now\n",
        "\n",
        "The problem is that one-hot encoding adds new columns. What if `X_test` has a different set of categories than `X_train`? Maybe `X_train` is missing `'Belfast'` in the `Joined` column. If we apply `CustomOHETransformer` to `X_train` and `X_test` separately, we could end up with two tables that do not match in their features. That would be bad.\n",
        "\n",
        "One (complicated) solution is to rewrite CustomOHETransformer to deal with this. It would end up adding and/or removing columns to X_test. Kind of ugly.\n",
        "\n",
        "I am going to present several alternative approaches for handling nominal columns. **Note this greatly expands the video**.\n"
      ],
      "metadata": {
        "id": "gW5AU3GG4ImF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First up is `ce.TargetEncoder`  (what is seen in video)\n",
        "\n",
        "<pre>\n",
        "!pip install category_encoders\n",
        "import category_encoders as ce\n",
        "</pre>\n",
        "\n",
        "Here is how the simple target-encoding algorithm works.\n",
        "\n",
        "1. Choose a category from a nominal column, e.g., `Belfast` from `Joined`.\n",
        "2. Find all rows with `Belfast` and record the `Survived` value (AKA the target value). Compute average of the values. Given that `Survived` is a binary column, we will get the percentage of 1s.\n",
        "3. Map all Belfast values in the column to this percentage.\n",
        "4. Repeat for all categories in the column.\n",
        "\n",
        "Notice that the column is scaled between 0 and 1 once we finish. So that is good.\n",
        "\n",
        "For a deeper dive see [foundational paper](https://dl.acm.org/doi/10.1145/507533.507538).\n",
        "\n",
        "###Problems with target encoding\n",
        "\n",
        "We need something called smoothing that maybe best shown with an example. First here is formula:\n",
        "\n",
        "* Smoothing Formula: `(n * category_mean + smoothing * global_mean) / (n + smoothing)`\n",
        "\n",
        "Consider an extreme example:\n",
        "\n",
        "* Category A appears 1000 times, with target mean 0.5\n",
        "* Category B appears only 2 times, with target mean 1.0\n",
        "\n",
        "**Without smoothing**:\n",
        "\n",
        "We will confidently assign 1.0 to Category B.\n",
        "This is risky because those 2 samples might be outliers.\n",
        "You're essentially trusting 2 samples as much as 1000 samples\n",
        "\n",
        "**With smoothing (like in `ce.TargetEncoder`)**:\n",
        "\n",
        "Category B's encoding would be pulled toward the global mean.\n",
        "So Category B might get 0.6 instead of 1.0, making it more conservative.\n",
        "\n",
        "**Real-world impact of lack of smoothing**:\n",
        "\n",
        "* Small/rare categories can cause overfitting without smoothing. (More on this later.)\n",
        "* Cross-validation scores might look deceptively good. (More on this later.)\n",
        "* Production performance could degrade, especially if rare categories are important.\n",
        "\n",
        "###The good and bad news (problems encountered post-video)\n",
        "\n",
        "The `ce.TargetEncoder` indeed does smoothing so is a great fit for our pipeline. The bad news is that it has lost compatibility (since the video) with Colab. Once you try to `fit_transform`, you will get an error. I suppose the lesson is that you (and I) need to be careful when relying on libraries that might not be actively maintained."
      ],
      "metadata": {
        "id": "XBsTlLgNVlZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Next up is MeanEncoder (not seen in video)\n",
        "\n",
        "<pre>\n",
        "from feature_engine.encoding import MeanEncoder\n",
        "\n",
        "# Set unseen='ignore' to encode new categories as nan\n",
        "encoder = MeanEncoder(\n",
        "    variables=['categorical_column'],\n",
        "    unseen='ignore'  # This will make it return np.nan for unseen values\n",
        ")\n",
        "</pre>\n",
        "\n",
        "###More good and bad news\n",
        "\n",
        "The `feature_engine` library is well maintained and `MeanEncoder` will work for us. The bad news is that it uses the simple target encoding algorithm without smoothing. I think smoothing is important so will not consider the `MeanEncoder`."
      ],
      "metadata": {
        "id": "ru4QPLjkQc69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##sklearn (kind of) to the rescue\n",
        "\n",
        "The sklearn library does have a target-encoder:\n",
        "\n",
        "<pre>\n",
        "from sklearn.preprocessing import _encoders\n",
        "encoder = _encoders.TargetEncoder(cols=['your_categorical_column'], smoothing=1.0)  \n",
        "</pre>\n",
        "\n",
        "However, it is experimental and could change (or go away), so will not be using it."
      ],
      "metadata": {
        "id": "uB1uZEpFwFlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write our own (not seen in video)\n",
        "\n",
        "The target encoding algorithm, even with smoothing, is not too bad. Let's just implement it in a custom class.\n",
        "\n",
        "A note about the smoothing parameter.\n",
        "The smoothing parameter controls how much weight to give to the global mean:\n",
        "\n",
        "* Higher values (e.g., 100) = more smoothing, encodings closer to global mean.\n",
        "*Lower values (e.g., 1) = less smoothing, encodings closer to category means.\n",
        "\n",
        "A good starting point is often around 10-20."
      ],
      "metadata": {
        "id": "AtA-D2DZbi67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The code is a bit obscure in `fit` method below\n",
        "\n",
        "Let's go through it with simple example.\n",
        "\n"
      ],
      "metadata": {
        "id": "CyHQoICCWD0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "X_ = pd.DataFrame({\n",
        "    'neighborhood': ['Downtown', 'Suburbs', 'Downtown', 'Rural', 'Downtown', 'Suburbs', 'Downtown', 'Rural'],\n",
        "    'target': [1, 0, 1, 0, 1, 0, 0, 1]  # 1 = expensive, 0 = not expensive\n",
        "})\n",
        "X_"
      ],
      "metadata": {
        "id": "Hren8GEF28d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When we do:\n",
        "X_['target'].groupby(X_['neighborhood']).mean()"
      ],
      "metadata": {
        "id": "MM6yuaY16gzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then turn into dict\n",
        "means = X_['target'].groupby(X_['neighborhood']).mean().to_dict()"
      ],
      "metadata": {
        "id": "UJtDZ8-P3FYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get\n",
        "means"
      ],
      "metadata": {
        "id": "i7KBTo143S3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And when we apply smoothing:\n",
        "\n",
        "global_mean = X_['target'].mean()  # overall proportion of expensive houses\n",
        "global_mean # 0.5 (4 expensive out of 8 total)"
      ],
      "metadata": {
        "id": "l032LjiK3NVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = X_['neighborhood'].value_counts().to_dict()\n",
        "counts # {'Downtown': 4, 'Suburbs': 2, 'Rural': 2}\n"
      ],
      "metadata": {
        "id": "wVT-OJq34jhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Apply smoothing"
      ],
      "metadata": {
        "id": "uHF0FFtrT6g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smoothing = 10\n",
        "# For each category Calculate smoothed mean\n",
        "smoothed_means = {}\n",
        "for category in counts.keys():\n",
        "    n = counts[category]\n",
        "    category_mean = means[category]\n",
        "    # Apply smoothing formula: (n * cat_mean + m * global_mean) / (n + m)\n",
        "    smoothed_mean = (n * category_mean + smoothing * global_mean) / (n + smoothing)\n",
        "    smoothed_means[category] = smoothed_mean\n",
        "\n",
        "# Downtown: (4 * 0.75 + 10 * 0.5) / (4 + 10) = 0.571\n",
        "# Less extreme than 0.75 due to smoothing\n",
        "\n",
        "# Suburbs: (2 * 0.0 + 10 * 0.5) / (2 + 10) = 0.417\n",
        "# Pulled up from 0.0 toward global mean\n",
        "\n",
        "# Rural: (2 * 0.5 + 10 * 0.5) / (2 + 10) = 0.5\n",
        "# Stays at 0.5 since it equals global mean\n",
        "\n",
        "smoothed_means\n"
      ],
      "metadata": {
        "id": "Nyiyo4x541It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "</pre>\n",
        "The smoothing is especially important with binary targets because:\n",
        "\n",
        "* Small sample sizes can give extreme proportions (0% or 100%).\n",
        "* These extreme values might not generalize well.\n",
        "* Smoothing pulls these proportions toward the global mean."
      ],
      "metadata": {
        "id": "stHGDHU35eLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Be lazy - make pandas do the work\n",
        "\n",
        "Note I am using built-in pandas methods when I can in code below - let them do the work for me."
      ],
      "metadata": {
        "id": "c8b8Y55WcsGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTargetTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A target encoder that applies smoothing and returns np.nan for unseen categories.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    col: name of column to encode.\n",
        "    smoothing : float, default=10.0\n",
        "        Smoothing factor. Higher values give more weight to the global mean.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, col: str, smoothing: float =10.0):\n",
        "        self.col = col\n",
        "        self.smoothing = smoothing\n",
        "        self.global_mean_ = None\n",
        "        self.encoding_dict_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the target encoder using training data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target values.\n",
        "        \"\"\"\n",
        "        assert isinstance(X, pd.core.frame.DataFrame), f'{self.__class__.__name__}.fit expected Dataframe but got {type(X)} instead.'\n",
        "        assert self.col in X, f'{self.__class__.__name__}.fit column not in X: {self.col}. Actual columns: {X.columns}'\n",
        "        assert isinstance(y, Iterable), f'{self.__class__.__name__}.fit expected Iterable but got {type(y)} instead.'\n",
        "        assert len(X) == len(y), f'{self.__class__.__name__}.fit X and y must be same length but got {len(X)} and {len(y)} instead.'\n",
        "\n",
        "        #Create new df with just col and target - enables use of pandas methods below\n",
        "        X_ = X[[self.col]]\n",
        "        target = self.col+'_target_'\n",
        "        X_[target] = y\n",
        "\n",
        "        # Calculate global mean\n",
        "        self.global_mean_ = X_[target].mean()\n",
        "\n",
        "        # Get counts and means\n",
        "        counts = X_[self.col].value_counts().to_dict()    #dictionary of unique values in the column col and their counts\n",
        "        means = X_[target].groupby(X_[self.col]).mean().to_dict() #dictionary of unique values in the column col and their means\n",
        "\n",
        "        # Calculate smoothed means\n",
        "        smoothed_means = {}\n",
        "        for category in counts.keys():\n",
        "            n = counts[category]\n",
        "            category_mean = means[category]\n",
        "            # Apply smoothing formula: (n * cat_mean + m * global_mean) / (n + m)\n",
        "            smoothed_mean = (n * category_mean + self.smoothing * self.global_mean_) / (n + self.smoothing)\n",
        "            smoothed_means[category] = smoothed_mean\n",
        "\n",
        "        self.encoding_dict_ = smoothed_means\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the data using the fitted target encoder.\n",
        "        Unseen categories will be encoded as np.nan.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Input data to transform.\n",
        "        \"\"\"\n",
        "\n",
        "        assert isinstance(X, pd.core.frame.DataFrame), f'{self.__class__.__name__}.transform expected Dataframe but got {type(X)} instead.'\n",
        "        assert self.encoding_dict_, f'{self.__class__.__name__}.transform not fitted'\n",
        "\n",
        "        X_ = X.copy()\n",
        "\n",
        "        # Map categories to smoothed means, naturally producing np.nan for unseen categories, i.e.,\n",
        "        # when map tries to look up a value in the dictionary and doesn't find the key, it automatically returns np.nan. That is what we want.\n",
        "        X_[self.col] = X_[self.col].map(self.encoding_dict_)\n",
        "\n",
        "        return X_\n",
        "\n",
        "    def fit_transform(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the target encoder and transform the input data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target values.\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X)"
      ],
      "metadata": {
        "id": "y6TpVyHEv3wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Small demo of use\n",
        "\n",
        "I'll fit with training data then transform test data. This mirrors what we will be doing once we split into train and test sets."
      ],
      "metadata": {
        "id": "fRRU31MdoVJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df = pd.DataFrame({\n",
        "    'A': ['cat', 'snake', 'dog'],\n",
        "    'target': [1, 0, 1]\n",
        "})\n",
        "demo_df\n"
      ],
      "metadata": {
        "id": "PPzkEUbjw-R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit with train_df\n",
        "target_encoder = CustomTargetTransformer(col='A', smoothing=10)\n",
        "target_encoder.fit(demo_df, demo_df['target'])"
      ],
      "metadata": {
        "id": "UjzhuuFrmsnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_df2 = pd.DataFrame({\n",
        "    'A': ['cat', 'snake', 'bear'],\n",
        "    'target': [0, 1, 0]\n",
        "})\n",
        "demo_df2\n"
      ],
      "metadata": {
        "id": "jDm9-RYrnWDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming with test_df\n",
        "test_encoded = target_encoder.transform(demo_df2)\n",
        "\n",
        "test_encoded  #notice that 'bear' leads to NaN given it was not seen in train_df"
      ],
      "metadata": {
        "id": "qQ0Yl7ebns0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I propose we swap out our `CustomOHETransformer` and use the `CustomTargetTransformer` instead going forward. You will need to edit your library pipelines accordingly."
      ],
      "metadata": {
        "id": "fZ6vCK_2A01M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJUuPgGcwpWp"
      },
      "source": [
        "#V. Stratified splitting\n",
        "\n",
        "With a random split, we do not know what will be in the test set. What if I want the test set to have the same distribution of labels as the training set? With plain random, I could end up with a test set of just those who perished. Or just customers who always rate highly. What I would like is for the same distribution. So if the overall dataset has 65% negative labels and 35% positive, I would like both the training and test sets to carry that distribution. Make sense?\n",
        "\n",
        "<img src='https://www.datasciencemadesimple.com/wp-content/uploads/2020/02/Stratified-Sampling-in-SAS-0.png?ezimgfmt=ng%3Awebp%2Fngcb1%2Frs%3Adevice%2Frscb1-1' height=100>\n",
        "\n",
        "Good news: it is not hard to do. We just need to add a parameter to what we have. Check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxXx_-57xjAm"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(titanic_features, labels, test_size=0.2, shuffle=True,\n",
        "                                                    random_state=0,\n",
        "                                                    stratify=labels)  #added parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbMVQGs2yTqp"
      },
      "source": [
        "Reminder: `labels = titanic_trimmed['Survived'].to_list()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghClKvva1MYC"
      },
      "source": [
        "#VI. Splitting on `random_state`\n",
        "\n",
        "This is the article that introduces this idea: [Variance splitting](https://towardsdatascience.com/future-proof-your-data-partitions-d2047de9ead2).  The general idea is that we write a loop that tries different splits by changing the `random_state` value. For each separate value, we train a model and predict on both training and test data. We log the test accuracy divided by the training accuracy, i.e., how far they vary.\n",
        "\n",
        "If we loop 200 times, we will get 200 variances. We take the average.  We now choose whatever value for `random_state` gets us closest to that average.\n",
        "\n",
        "The code is not too bad so let's try it. One problem is that we need to wrangle the raw titanic dataset to get to spot where we can loop. I'll do that now. Notice the imputation step has been added and target encoding now in play."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF63ZEKdyJmw"
      },
      "source": [
        "#Only instantiating once then passing in as parameter. See \"Caveat\" below.\n",
        "\n",
        "titanic_transformer = Pipeline(steps=[\n",
        "    ('map_gender', CustomMappingTransformer('Gender', {'Male': 0, 'Female': 1})),\n",
        "    ('map_class', CustomMappingTransformer('Class', {'Crew': 0, 'C3': 1, 'C2': 2, 'C1': 3})),\n",
        "    ('target_joined', CustomTargetTransformer(col='Joined', smoothing=10)),   #swapped this in\n",
        "    ('tukey_age', CustomTukeyTransformer(target_column='Age', fence='outer')),\n",
        "    ('tukey_fare', CustomTukeyTransformer(target_column='Fare', fence='outer')),\n",
        "    ('scale_age', CustomRobustTransformer(target_column='Age')),\n",
        "    ('scale_fare', CustomRobustTransformer(target_column='Fare')),\n",
        "    ('impute', CustomKNNTransformer(n_neighbors=5)),\n",
        "    ], verbose=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgzn4_HyAnR0"
      },
      "source": [
        "###The choice of model we use is up to us\n",
        "\n",
        "Remember, we are not trying to find the best accuracy. That comes in later chapters. We are trying to produce a set of variances that we can use to find an average.\n",
        "\n",
        "I am going to choose KNN because we have already seen it in action in the `KNNImputer`. There is a version that will predict the value of a specific column, in our case, `Survived`. Uses the same crowd sourcing idea we discussed last chapter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier  #the KNN model\n",
        "model = KNeighborsClassifier(n_neighbors=5)  #instantiate with k=5."
      ],
      "metadata": {
        "id": "vdhn0dUd70Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjPKBn4eBG8v"
      },
      "source": [
        "###The choice of goodness metric is up to us\n",
        "\n",
        "We could choose just plain accuracy. But\n",
        "I am going to use something called the F1 score because it is so common.\n",
        "The higher the F1 score, the better. The value 1 is perfect. But remember, we are looking at this ratio: test_F1/train_F1. Our goal is to compute the average ratio across 200 tries. So we really are not concerned with the F1 values, just the average ratio.\n",
        "\n",
        "That said, F1 can be misleading with unbalanced data. I almost replaced it with MCC or ROC-AUC, both of which we will discuss later."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Caveat\n",
        "\n",
        "I am only instantiating the `Pipeline` once (see code above) and passing it in. This means the `Pipeline` only instantiates each transformer once. So when we do a `transformer.fit_transform` in the loop body, we are counting on the `fit` method in each transformer to reset its state. Otherwise, calling `fit` repeatedly in the loop could lead to undesired accumulation of state in the transformer object. Make sense?\n",
        "\n",
        "I am confident that each of our transformers resets its state information on a call to `fit` so I think we are good. But it is something to be aware of. The alternative is to place the entire `Pipeline` definition (see code above) within the loop body so we get a fresh pipeline every time through the loop."
      ],
      "metadata": {
        "id": "CUru1wtJ2u8w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLU55Vjg4aFh"
      },
      "source": [
        "from sklearn.metrics import f1_score  #typical metric used to measure goodness of a model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_random_state_old(features_df, labels, transformer, n=200):\n",
        "  model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "  ratios = []  #collect test_f1/train_f1 values\n",
        "  for i in range(0, n):\n",
        "    train_X, test_X, train_y, test_y = train_test_split(features_df, labels, test_size=0.2, shuffle=True,\n",
        "                                                    random_state=i, stratify=labels)\n",
        "\n",
        "    #apply pipeline\n",
        "    transform_train_X = transformer.fit_transform(train_X, train_y)\n",
        "    transform_test_X = transformer.transform(test_X)\n",
        "\n",
        "    #train model then predict\n",
        "    model.fit(transform_train_X, train_y)  #train model\n",
        "    train_pred = model.predict(transform_train_X)  #predict against training set\n",
        "    test_pred = model.predict(transform_test_X)    #predict against test set\n",
        "    train_f1 = f1_score(train_y, train_pred)  #how well did we do with prediction on training data?\n",
        "\n",
        "    if train_f1 < .1:\n",
        "        continue  # Skip if train_f1 is too low or zero\n",
        "\n",
        "    test_f1 = f1_score(test_y, test_pred)     #how well did we do with prediction on test data?\n",
        "    f1_ratio = test_f1/train_f1        #take the ratio - closer to 1 the better\n",
        "    ratios.append(f1_ratio)\n",
        "\n",
        "  mean = np.mean(ratios)\n",
        "  rs_value = np.abs(ratios - mean).argmin()  # Returns index of value closest to mean\n",
        "\n",
        "  return rs_value, ratios\n"
      ],
      "metadata": {
        "id": "z1Y3FMAG4u_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_random_state(\n",
        "    features_df: pd.DataFrame,\n",
        "    labels: Iterable,\n",
        "    transformer: TransformerMixin,\n",
        "    n: int = 200\n",
        "                  ) -> Tuple[int, List[float]]:\n",
        "    \"\"\"\n",
        "    Finds an optimal random state for train-test splitting based on F1-score stability.\n",
        "\n",
        "    This function iterates through `n` different random states when splitting the data,\n",
        "    applies a transformation pipeline, and trains a K-Nearest Neighbors classifier.\n",
        "    It calculates the ratio of test F1-score to train F1-score and selects the random\n",
        "    state where this ratio is closest to the mean.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    features_df : pd.DataFrame\n",
        "        The feature dataset.\n",
        "    labels : Union[pd.Series, List]\n",
        "        The corresponding labels for classification (can be a pandas Series or a Python list).\n",
        "    transformer : TransformerMixin\n",
        "        A scikit-learn compatible transformer for preprocessing.\n",
        "    n : int, default=200\n",
        "        The number of random states to evaluate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rs_value : int\n",
        "        The optimal random state where the F1-score ratio is closest to the mean.\n",
        "    Var : List[float]\n",
        "        A list containing the F1-score ratios for each evaluated random state.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - If the train F1-score is below 0.1, that iteration is skipped.\n",
        "    - A higher F1-score ratio (closer to 1) indicates better train-test consistency.\n",
        "    \"\"\"\n",
        "\n",
        "    model = KNeighborsClassifier(n_neighbors=5)\n",
        "    Var: List[float] = []  # Collect test_f1/train_f1 ratios\n",
        "\n",
        "    for i in range(n):\n",
        "        train_X, test_X, train_y, test_y = train_test_split(\n",
        "            features_df, labels, test_size=0.2, shuffle=True,\n",
        "            random_state=i, stratify=labels  # Works with both lists and pd.Series\n",
        "        )\n",
        "\n",
        "        # Apply transformation pipeline\n",
        "        transform_train_X = transformer.fit_transform(train_X, train_y)\n",
        "        transform_test_X = transformer.transform(test_X)\n",
        "\n",
        "        # Train model and make predictions\n",
        "        model.fit(transform_train_X, train_y)\n",
        "        train_pred = model.predict(transform_train_X)\n",
        "        test_pred = model.predict(transform_test_X)\n",
        "\n",
        "        train_f1 = f1_score(train_y, train_pred)\n",
        "\n",
        "        if train_f1 < 0.1:\n",
        "            continue  # Skip if train_f1 is too low\n",
        "\n",
        "        test_f1 = f1_score(test_y, test_pred)\n",
        "        f1_ratio = test_f1 / train_f1  # Ratio of test to train F1-score\n",
        "\n",
        "        Var.append(f1_ratio)\n",
        "\n",
        "    mean_f1_ratio: float = np.mean(Var)\n",
        "    rs_value: int = np.abs(np.array(Var) - mean_f1_ratio).argmin()  # Index of value closest to mean\n",
        "\n",
        "    return rs_value, Var\n"
      ],
      "metadata": {
        "id": "q1LHWgSWw4D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#My apologies\n",
        "\n",
        "Something has changed within the KNN classifier since I created the videos and the notebooks. You will see in the video that I get results below that do not match the new reality.\n",
        "\n",
        "Your goal is to match what I have in the notebook, not the video. Sorry about that. But libraries change often in ML land and sometimes without warning."
      ],
      "metadata": {
        "id": "NsthCmYKtaQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "titanic_rs, all_values = find_random_state(titanic_features, labels, titanic_transformer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KXGnGe7hvZTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_rs  #24 gives us value closest to mean of all_values"
      ],
      "metadata": {
        "id": "bmVeMLvgv2x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Just to check"
      ],
      "metadata": {
        "id": "QQ_9WV1TdxE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(all_values)  #0.8511360176628843"
      ],
      "metadata": {
        "id": "F3d7HdHHa927"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_values[24]  #0.851497005988024"
      ],
      "metadata": {
        "id": "7BLkp7CsbTfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Here is plot to visualize"
      ],
      "metadata": {
        "id": "qr50eM0drpul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "# Original plots\n",
        "plt.plot(range(0, 200), all_values, color='royalblue', label=\"test_error/train_error\")\n",
        "plt.plot([0, 199], [np.mean(all_values), np.mean(all_values)],\n",
        "         color='darkorange', linewidth=3, label=\"mean ratio\")\n",
        "\n",
        "# Add point at best random state\n",
        "plt.scatter(titanic_rs, all_values[titanic_rs], color='red', s=100, zorder=5)\n",
        "\n",
        "# Add annotation for the value\n",
        "plt.annotate(f'Best RS: {titanic_rs}',\n",
        "            xy=(titanic_rs, all_values[titanic_rs]),\n",
        "            xytext=(10, 10),\n",
        "            textcoords='offset points',\n",
        "            color='red',\n",
        "            fontsize=12)\n",
        "\n",
        "plt.xlabel('random_state')\n",
        "plt.ylabel('test_error/train_error')\n",
        "plt.legend(loc='lower center', shadow=True, fontsize='medium')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zDyjI6lorJ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOQ1WQi72wpb"
      },
      "source": [
        "#VII. Ready for final split but consider data leakage\n",
        "\n",
        "We know what `random_state` value to use. Let's do the split using it.\n",
        "\n",
        "The next issue is of data leakage. We do not want any info on the test set to leak into the training set. Where could this happen? In our pipeline. If we are not careful, when we compute scale and impute values, we might end up including the test set in these computations. That would be bad. For instance, we could end up imputing missing values in the training set by relying on information in the test set. We want a firewall between training and testing sets. Transforms on the training set do not get to look at the test set for help.\n",
        "\n",
        "The good news is that our transformers take this into account. That is why some have a `fit` step. We should fit with the training set only. Then we can transform both the training and test sets from that without leakage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5XN6ImKT1qn"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(titanic_features, labels, test_size=0.2, shuffle=True,\n",
        "                                                    random_state=titanic_rs, stratify=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reality check on stratification of other columns\n",
        "\n",
        "We chose to stratify on `Survived`. But might be worth looking at feature columns to see if they are roughly stratified as well."
      ],
      "metadata": {
        "id": "t5CK4DpckZdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe(include='all').T.round(2)"
      ],
      "metadata": {
        "id": "1V7-sY52dbE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.describe(include='all').T.round(2)"
      ],
      "metadata": {
        "id": "oKiGhuG1db3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just eyeballing the two tables with the mean and std, it looks like a fairly balanced distribution among the two sets. Also, the `Joined` column has all 4 possible values in the training set. See my argument above that we should be able to guarantee this if we rewrote `train_test_split`. Or at least throw an error if it is not the case."
      ],
      "metadata": {
        "id": "1OmvjHCEo-3G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8G9VTEna75R"
      },
      "source": [
        "#VIII. Apply pipeline to X_train\n",
        "\n",
        "Do a `fit_transform` and remember to pass in labels as 2nd argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Itol-pEzSPn"
      },
      "source": [
        "titanic_transformer = Pipeline(steps=[\n",
        "    ('map_gender', CustomMappingTransformer('Gender', {'Male': 0, 'Female': 1})),\n",
        "    ('map_class', CustomMappingTransformer('Class', {'Crew': 0, 'C3': 1, 'C2': 2, 'C1': 3})),\n",
        "    ('target_joined', CustomTargetTransformer(col='Joined', smoothing=10)),\n",
        "    ('tukey_age', CustomTukeyTransformer(target_column='Age', fence='outer')),\n",
        "    ('tukey_fare', CustomTukeyTransformer(target_column='Fare', fence='outer')),\n",
        "    ('scale_age', CustomRobustTransformer('Age')),\n",
        "    ('scale_fare', CustomRobustTransformer('Fare')),\n",
        "    ('impute', CustomKNNTransformer(n_neighbors=5)),\n",
        "    ], verbose=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed = titanic_transformer.fit_transform(X_train, y_train)   #need both arguments for fit\n"
      ],
      "metadata": {
        "id": "HIeZb0h9x5NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBZJCM0I1X0O"
      },
      "source": [
        "X_train_transformed.round(2).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|index|Age|Gender|Class|Joined|Married|Fare|\n",
        "|---|---|---|---|---|---|---|\n",
        "|1265|-0\\.2|1\\.0|2\\.0|0\\.4|0\\.0|-0\\.08|\n",
        "|407|-1\\.15|0\\.0|1\\.0|0\\.4|0\\.0|0\\.38|\n",
        "|1205|0\\.1|0\\.0|0\\.0|0\\.4|1\\.0|-0\\.58|\n",
        "|802|0\\.0|1\\.0|0\\.0|0\\.4|0\\.0|3\\.71|\n",
        "|260|-0\\.4|0\\.0|1\\.0|0\\.4|1\\.0|-0\\.04|"
      ],
      "metadata": {
        "id": "-yCMj0aQqnb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IX. Now transform X_test\n",
        "\n",
        "But do **not** do a `fit`, just a `transform`."
      ],
      "metadata": {
        "id": "GuLqS0_sj_l_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlD7cdqlfK2A"
      },
      "source": [
        "X_test_transformed = titanic_transformer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK9BeNXPfK2B"
      },
      "source": [
        "X_test_transformed.round(2).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|index|Age|Gender|Class|Joined|Married|Fare|\n",
        "|---|---|---|---|---|---|---|\n",
        "|217|-0\\.5|0\\.0|0\\.0|0\\.41|0\\.0|-0\\.58|\n",
        "|868|-0\\.05|1\\.0|2\\.0|0\\.4|1\\.0|0\\.5|\n",
        "|283|-0\\.3|0\\.0|0\\.0|0\\.41|1\\.0|-0\\.58|\n",
        "|1176|-0\\.05|0\\.0|2\\.0|0\\.58|0\\.0|-0\\.08|\n",
        "|675|-0\\.85|0\\.0|2\\.0|0\\.4|0\\.0|2\\.12|"
      ],
      "metadata": {
        "id": "9A0enPD2kqzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###About that warning: *FutureWarning: This Pipeline instance is not fitted yet* ...\n",
        "\n",
        "I've spent a lot of time trying to track down the reason for this warning. All of our transformers are fitted properly. Frankly, I think it is a bug in `sklearn`'s `Pipeline` class and I plan to report it.\n",
        "\n",
        "You can get rid of it by adding this as the **last** step of your `Pipeline`.\n",
        "\n",
        "<pre>\n",
        "    ('passthrough', FunctionTransformer(validate=False)),  #does nothing but does remove warning\n",
        "</pre>\n",
        "\n",
        "You will also need:\n",
        "<pre>\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "</pre>\n",
        "Feel free to add this code if you like. Or just live with the warning. My take is that by adding a built-in transformer as last step, Pipeline is happy. If, instead, the last step is a custom transformer, get a warning."
      ],
      "metadata": {
        "id": "bz-p0tF5kuGR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksDu7F8KYa3p"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "Just as a warm-up to challenge 4, help me figure out a way to loop over floats. Hint: `range` will only loop over ints.\n",
        "\n",
        "Find a way to loop over values between 0.01 and .9 in .01 increments. You can use whatever libraries you like. Gemini can probably help.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.round(np.arange(0.01,.91,.01), 2))"
      ],
      "metadata": {
        "id": "fadb0ZFThReW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Challenge 1 (needed on midterm)\n",
        "\n",
        "Add `CustomerTargetTransformer` to your library."
      ],
      "metadata": {
        "id": "eQRQSEhJFPTc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXrcdcpjammk"
      },
      "source": [
        "#Challenge 2 (results will be used on midterm)\n",
        "\n",
        "Add `find_random_state` to your library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFIy-ovNfUoi"
      },
      "source": [
        "#Challenge 3 (similar to midterm question)\n",
        "\n",
        "While we are here, go ahead and compute the best `random_state` value for the cable customer dataset. I did not leave you anything to code here. Just make sure you understand the steps.\n",
        "\n",
        "Note that I swapped out the `CustomOHETransformer` for the `CustomTargetTransformer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKDSTsL6qSIN"
      },
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQPM6PqZXgmAHfRYTcDZseyALRyVwkBtKEo_rtaKq_C7T0jycWxH6QVEzTzJCRA0m8Vz0k68eM9tDm-/pub?output=csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icPxiWs1qSIN"
      },
      "source": [
        "customers_df = pd.read_csv(url)\n",
        "customers_trimmed = customers_df.drop(columns='ID')  #this is a useless column which we will drop early\n",
        "customers_trimmed = customers_trimmed.drop_duplicates(ignore_index=True)  #get rid of any duplicates\n",
        "customers_trimmed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53sJ-STTZUDl"
      },
      "source": [
        "customers_features = customers_trimmed.drop(columns='Rating')\n",
        "labels = customers_trimmed['Rating'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYxtig9qo1Bj"
      },
      "source": [
        "#Build pipeline\n",
        "customer_transformer = Pipeline(steps=[\n",
        "    ('map_os', CustomMappingTransformer('OS', {'Android': 0, 'iOS': 1})),\n",
        "    ('target_isp', CustomTargetTransformer(col='ISP')),\n",
        "    ('map_level', CustomMappingTransformer('Experience Level', {'low': 0, 'medium': 1, 'high':2})),\n",
        "    ('map_gender', CustomMappingTransformer('Gender', {'Male': 0, 'Female': 1})),\n",
        "    ('tukey_age', CustomTukeyTransformer('Age', 'inner')),  #from chapter 4\n",
        "    ('tukey_time spent', CustomTukeyTransformer('Time Spent', 'inner')),  #from chapter 4\n",
        "    ('scale_age', CustomRobustTransformer(target_column='Age')), #from 5\n",
        "    ('scale_time spent', CustomRobustTransformer(target_column='Time Spent')), #from 5\n",
        "    ('impute', KNNImputer(n_neighbors=5, weights=\"uniform\", add_indicator=False)),\n",
        "    ], verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNzpsDllB_s0"
      },
      "source": [
        "#Test it out\n",
        "\n",
        "transformed_customer_df = customer_transformer.fit_transform(customers_features, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5m0UM1XgsJj"
      },
      "source": [
        "%%capture\n",
        "cust_rs, vars = find_random_state(customers_features, labels, customer_transformer, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cust_rs)  #58"
      ],
      "metadata": {
        "id": "CJ58BwJcA-Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx8QufL7I2Iu"
      },
      "source": [
        "#Challenge 4 (not on midterm)\n",
        "\n",
        "There is another parameter to `train_test_split` that is important. It is the `test_size`. What percentage of the entire dataset should the test dataset be? I have set it at .2 which is pretty common. So 80% is used for training and only 20% for testing.\n",
        "\n",
        "[Some have argued](https://towardsdatascience.com/why-training-set-should-always-be-smaller-than-test-set-61f087ed203c) that small test sets are bad. You want your test set to be larger than your training set. Again, this flies against common wisdom.\n",
        "\n",
        "I thought it would be fun to test this idea out. Here is what I would like you to do:\n",
        "\n",
        "1. Loop through percentages between .01 and .9 inclusive, with a step of .01.\n",
        "\n",
        "2. split (`test_size`) using the loop variable. You can use `titanic_rs` as `random_state` - we already figured that out.\n",
        "\n",
        "3. `fit_transform` on train and `transform` on test.\n",
        "\n",
        "4. train the model, i.e., `model.fit`. You can use the KNN model you defined when looking at random state. Remember to instatiate this anew each time through the loop. Otherwise you will be augmenting training as opposed to starting from scratch each time.\n",
        "\n",
        "5. test the model, i.e., predict values on test. You do **not** need to predict on training data.\n",
        "\n",
        "6. compute f1 score on test (ignore train).\n",
        "\n",
        "7. Record your results as pair (test-percent loop variable, f1 score).\n",
        "\n",
        "8. At end of loop you should have a list of pairs. Sort descendingly on f1 score - bigger f1 is better.\n",
        "\n",
        "The paper claims we should see large percents at top of list."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reminder\n",
        "titanic_features = titanic_trimmed.drop(columns='Survived')\n",
        "labels = titanic_trimmed['Survived'].to_list()"
      ],
      "metadata": {
        "id": "O1JczuOSuYz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(titanic_features), len(labels)"
      ],
      "metadata": {
        "id": "KGCgHFPwCcQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#your code below - scores is the list of pairs\n",
        "scores = []\n",
        "for i in np.round(np.arange(0.01,.91,.01),2):\n",
        "  print(i)\n",
        "  train_X, test_X, train_y, test_y = train_test_split(titanic_features, labels, test_size=i, shuffle=True,\n",
        "                                                random_state=titanic_rs, stratify=labels)\n",
        "\n",
        "  X_train_transformed = titanic_transformer.fit_transform(train_X, train_y)\n",
        "  X_test_transformed = titanic_transformer.transform(test_X)\n",
        "  model = KNeighborsClassifier(n_neighbors=5)\n",
        "  model.fit(X_train_transformed, train_y)\n",
        "  test_pred = model.predict(X_test_transformed)\n",
        "  test_error = round(f1_score(test_y, test_pred), 2)\n",
        "  scores.append((i,test_error))\n"
      ],
      "metadata": {
        "id": "lpCab-GOJzVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(scores)  #90"
      ],
      "metadata": {
        "id": "9jl6DHu5t04x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores  #see mine below"
      ],
      "metadata": {
        "id": "_Oo3U82XuIha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My raw values:\n",
        "\n",
        "<pre>\n",
        "[(np.float64(0.01), 0.71),\n",
        " (np.float64(0.02), 0.72),\n",
        " (np.float64(0.03), 0.65),\n",
        " (np.float64(0.04), 0.62),\n",
        " (np.float64(0.05), 0.65),\n",
        " (np.float64(0.06), 0.61),\n",
        " (np.float64(0.07), 0.59),\n",
        " (np.float64(0.08), 0.6),\n",
        " (np.float64(0.09), 0.62),\n",
        " (np.float64(0.1), 0.62),\n",
        " (np.float64(0.11), 0.64),\n",
        " (np.float64(0.12), 0.64),\n",
        " (np.float64(0.13), 0.64),\n",
        " (np.float64(0.14), 0.64),\n",
        " (np.float64(0.15), 0.66),\n",
        " (np.float64(0.16), 0.65),\n",
        " (np.float64(0.17), 0.65),\n",
        " (np.float64(0.18), 0.66),\n",
        " (np.float64(0.19), 0.66),\n",
        " (np.float64(0.2), 0.65),\n",
        " (np.float64(0.21), 0.64),\n",
        " (np.float64(0.22), 0.62),\n",
        " (np.float64(0.23), 0.63),\n",
        " (np.float64(0.24), 0.63),\n",
        " (np.float64(0.25), 0.63),\n",
        " (np.float64(0.26), 0.64),\n",
        " (np.float64(0.27), 0.65),\n",
        " (np.float64(0.28), 0.65),\n",
        " (np.float64(0.29), 0.66),\n",
        " (np.float64(0.3), 0.67),\n",
        " (np.float64(0.31), 0.65),\n",
        " (np.float64(0.32), 0.64),\n",
        " (np.float64(0.33), 0.66),\n",
        " (np.float64(0.34), 0.65),\n",
        " (np.float64(0.35), 0.65),\n",
        " (np.float64(0.36), 0.65),\n",
        " (np.float64(0.37), 0.66),\n",
        " (np.float64(0.38), 0.67),\n",
        " (np.float64(0.39), 0.65),\n",
        " (np.float64(0.4), 0.66),\n",
        " (np.float64(0.41), 0.65),\n",
        " (np.float64(0.42), 0.64),\n",
        " (np.float64(0.43), 0.66),\n",
        " (np.float64(0.44), 0.65),\n",
        " (np.float64(0.45), 0.65),\n",
        " (np.float64(0.46), 0.66),\n",
        " (np.float64(0.47), 0.66),\n",
        " (np.float64(0.48), 0.65),\n",
        " (np.float64(0.49), 0.66),\n",
        " (np.float64(0.5), 0.65),\n",
        " (np.float64(0.51), 0.65),\n",
        " (np.float64(0.52), 0.66),\n",
        " (np.float64(0.53), 0.66),\n",
        " (np.float64(0.54), 0.67),\n",
        " (np.float64(0.55), 0.66),\n",
        " (np.float64(0.56), 0.67),\n",
        " (np.float64(0.57), 0.66),\n",
        " (np.float64(0.58), 0.66),\n",
        " (np.float64(0.59), 0.66),\n",
        " (np.float64(0.6), 0.66),\n",
        " (np.float64(0.61), 0.66),\n",
        " (np.float64(0.62), 0.64),\n",
        " (np.float64(0.63), 0.64),\n",
        " (np.float64(0.64), 0.64),\n",
        " (np.float64(0.65), 0.66),\n",
        " (np.float64(0.66), 0.64),\n",
        " (np.float64(0.67), 0.64),\n",
        " (np.float64(0.68), 0.64),\n",
        " (np.float64(0.69), 0.63),\n",
        " (np.float64(0.7), 0.64),\n",
        " (np.float64(0.71), 0.64),\n",
        " (np.float64(0.72), 0.64),\n",
        " (np.float64(0.73), 0.64),\n",
        " (np.float64(0.74), 0.64),\n",
        " (np.float64(0.75), 0.63),\n",
        " (np.float64(0.76), 0.62),\n",
        " (np.float64(0.77), 0.63),\n",
        " (np.float64(0.78), 0.64),\n",
        " (np.float64(0.79), 0.64),\n",
        " (np.float64(0.8), 0.64),\n",
        " (np.float64(0.81), 0.64),\n",
        " (np.float64(0.82), 0.64),\n",
        " (np.float64(0.83), 0.63),\n",
        " (np.float64(0.84), 0.62),\n",
        " (np.float64(0.85), 0.63),\n",
        " (np.float64(0.86), 0.63),\n",
        " (np.float64(0.87), 0.6),\n",
        " (np.float64(0.88), 0.57),\n",
        " (np.float64(0.89), 0.54),\n",
        " (np.float64(0.9), 0.55)]\n",
        " </pre>"
      ],
      "metadata": {
        "id": "lpR_zbxbTyCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Please sort with largest F1 score first\n",
        "\n",
        "sorted(scores, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "3SKT4iUkMMNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After sorting:\n",
        "\n",
        "<pre>\n",
        "[(np.float64(0.02), 0.72),\n",
        " (np.float64(0.01), 0.71),\n",
        " (np.float64(0.3), 0.67),\n",
        " (np.float64(0.38), 0.67),\n",
        " (np.float64(0.54), 0.67),\n",
        " (np.float64(0.56), 0.67),\n",
        " (np.float64(0.15), 0.66),\n",
        " (np.float64(0.18), 0.66),\n",
        " (np.float64(0.19), 0.66),\n",
        " (np.float64(0.29), 0.66),\n",
        " (np.float64(0.33), 0.66),\n",
        " (np.float64(0.37), 0.66),\n",
        " (np.float64(0.4), 0.66),\n",
        " (np.float64(0.43), 0.66),\n",
        " (np.float64(0.46), 0.66),\n",
        " (np.float64(0.47), 0.66),\n",
        " (np.float64(0.49), 0.66),\n",
        " (np.float64(0.52), 0.66),\n",
        " (np.float64(0.53), 0.66),\n",
        " (np.float64(0.55), 0.66),\n",
        " (np.float64(0.57), 0.66),\n",
        " (np.float64(0.58), 0.66),\n",
        " (np.float64(0.59), 0.66),\n",
        " (np.float64(0.6), 0.66),\n",
        " (np.float64(0.61), 0.66),\n",
        " (np.float64(0.65), 0.66),\n",
        " (np.float64(0.03), 0.65),\n",
        " (np.float64(0.05), 0.65),\n",
        " (np.float64(0.16), 0.65),\n",
        " (np.float64(0.17), 0.65),\n",
        " (np.float64(0.2), 0.65),  #what we are using\n",
        " (np.float64(0.27), 0.65),\n",
        " (np.float64(0.28), 0.65),\n",
        " (np.float64(0.31), 0.65),\n",
        " (np.float64(0.34), 0.65),\n",
        " (np.float64(0.35), 0.65),\n",
        " (np.float64(0.36), 0.65),\n",
        " (np.float64(0.39), 0.65),\n",
        " (np.float64(0.41), 0.65),\n",
        " (np.float64(0.44), 0.65),\n",
        " (np.float64(0.45), 0.65),\n",
        " (np.float64(0.48), 0.65),\n",
        " (np.float64(0.5), 0.65),\n",
        " (np.float64(0.51), 0.65),\n",
        " (np.float64(0.11), 0.64),\n",
        " (np.float64(0.12), 0.64),\n",
        " (np.float64(0.13), 0.64),\n",
        " (np.float64(0.14), 0.64),\n",
        " (np.float64(0.21), 0.64),\n",
        " (np.float64(0.26), 0.64),\n",
        " (np.float64(0.32), 0.64),\n",
        " (np.float64(0.42), 0.64),\n",
        " (np.float64(0.62), 0.64),\n",
        " (np.float64(0.63), 0.64),\n",
        " (np.float64(0.64), 0.64),\n",
        " (np.float64(0.66), 0.64),\n",
        " (np.float64(0.67), 0.64),\n",
        " (np.float64(0.68), 0.64),\n",
        " (np.float64(0.7), 0.64),\n",
        " (np.float64(0.71), 0.64),\n",
        " (np.float64(0.72), 0.64),\n",
        " (np.float64(0.73), 0.64),\n",
        " (np.float64(0.74), 0.64),\n",
        " (np.float64(0.78), 0.64),\n",
        " (np.float64(0.79), 0.64),\n",
        " (np.float64(0.8), 0.64),\n",
        " (np.float64(0.81), 0.64),\n",
        " (np.float64(0.82), 0.64),\n",
        " (np.float64(0.23), 0.63),\n",
        " (np.float64(0.24), 0.63),\n",
        " (np.float64(0.25), 0.63),\n",
        " (np.float64(0.69), 0.63),\n",
        " (np.float64(0.75), 0.63),\n",
        " (np.float64(0.77), 0.63),\n",
        " (np.float64(0.83), 0.63),\n",
        " (np.float64(0.85), 0.63),\n",
        " (np.float64(0.86), 0.63),\n",
        " (np.float64(0.04), 0.62),\n",
        " (np.float64(0.09), 0.62),\n",
        " (np.float64(0.1), 0.62),\n",
        " (np.float64(0.22), 0.62),\n",
        " (np.float64(0.76), 0.62),\n",
        " (np.float64(0.84), 0.62),\n",
        " (np.float64(0.06), 0.61),\n",
        " (np.float64(0.08), 0.6),\n",
        " (np.float64(0.87), 0.6),\n",
        " (np.float64(0.07), 0.59),\n",
        " (np.float64(0.88), 0.57),\n",
        " (np.float64(0.9), 0.55),\n",
        " (np.float64(0.89), 0.54)]\n",
        "</pre>"
      ],
      "metadata": {
        "id": "TuObKojkT_pF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kind of a mixed bag. Top score is a very low percentage: .02. This is opposite of what paper argues. The top 5th and 6th scores are above .5 so kind of supports the paper.\n",
        "\n",
        "I'm kind of dubious of low percentages, i.e., in the single digit percentages. We might just get lucky with a very small test set.\n",
        "\n",
        " To truly explore this, I'd want to try a wider variety of models, not just KNN. But on surface, it appears the paper is inconclusive, at least for Titanic data.\n",
        "\n",
        "I'm going to stick to `.2` for our class (the common wisdom) even though .54 would give us a bit of a bump."
      ],
      "metadata": {
        "id": "-IIF2ctDUMsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Challenge 5 (necessary change)\n",
        "\n",
        "Add your random state variables to your library. I know what you should be doing is this:\n",
        "<pre>\n",
        "titanic_variance_based_split = 24  #currently correct but incompatible\n",
        "customer_variance_based_split = 58 #ditto\n",
        "</pre>\n",
        "After all, that is what we found. But this is different than what I found in the past. And the following notebooks all count on what I found in the past. So please use this instead:\n",
        "<pre>\n",
        "titanic_variance_based_split = 107   #add to your library\n",
        "customer_variance_based_split = 113  #add to your library\n",
        "</pre>\n",
        "Sorry about that. As noted, my conjecture is that something has changed in the KNN classifier (what we are using in `find_random_state`) since I created the video and notebooks. And it is giving us different results. So please use the old values to be compatible with notebooks that follow."
      ],
      "metadata": {
        "id": "6P2ApRcPflB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Challenge 6\n",
        "\n",
        "Add your two transformers to your library. They will not change again during the class so might as well record them now.\n",
        "\n",
        "As reminder, here they are."
      ],
      "metadata": {
        "id": "dg0u6ECQEdgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_transformer = Pipeline(steps=[\n",
        "    ('map_gender', CustomMappingTransformer('Gender', {'Male': 0, 'Female': 1})),\n",
        "    ('map_class', CustomMappingTransformer('Class', {'Crew': 0, 'C3': 1, 'C2': 2, 'C1': 3})),\n",
        "    ('target_joined', CustomTargetTransformer(col='Joined', smoothing=10)),\n",
        "    ('tukey_age', CustomTukeyTransformer(target_column='Age', fence='outer')),\n",
        "    ('tukey_fare', CustomTukeyTransformer(target_column='Fare', fence='outer')),\n",
        "    ('scale_age', CustomRobustTransformer(target_column='Age')),\n",
        "    ('scale_fare', CustomRobustTransformer(target_column='Fare')),\n",
        "    ('impute', CustomKNNTransformer(n_neighbors=5)),\n",
        "    ], verbose=True)\n"
      ],
      "metadata": {
        "id": "7IUzjHlAE3FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_transformer = Pipeline(steps=[\n",
        "    ('map_os', CustomMappingTransformer('OS', {'Android': 0, 'iOS': 1})),\n",
        "    ('target_isp', CustomTargetTransformer(col='ISP')),\n",
        "    ('map_level', CustomMappingTransformer('Experience Level', {'low': 0, 'medium': 1, 'high':2})),\n",
        "    ('map_gender', CustomMappingTransformer('Gender', {'Male': 0, 'Female': 1})),\n",
        "    ('tukey_age', CustomTukeyTransformer('Age', 'inner')),  #from chapter 4\n",
        "    ('tukey_time spent', CustomTukeyTransformer('Time Spent', 'inner')),  #from chapter 4\n",
        "    ('scale_age', CustomRobustTransformer(target_column='Age')), #from 5\n",
        "    ('scale_time spent', CustomRobustTransformer(target_column='Time Spent')), #from 5\n",
        "    ('impute', CustomKNNTransformer(n_neighbors=5)),\n",
        "    ], verbose=True)"
      ],
      "metadata": {
        "id": "os3RXrfRJlxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Challenge 7\n",
        "\n",
        "Save the fitted transformer to file (then to GitHub). We will use it later."
      ],
      "metadata": {
        "id": "OhUb9LtVf7MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_pipeline = titanic_transformer.fit(X_train, y_train)  #notice just fit method called\n",
        "import joblib\n",
        "joblib.dump(fitted_pipeline, 'fitted_pipeline.pkl')  #and next move to GitHub"
      ],
      "metadata": {
        "id": "trvxmEaxgF8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO-WXiXxzY7T"
      },
      "source": [
        "#One side note\n",
        "\n",
        "What I feel we are missing is a rationale for the pipeline steps we are using. View it as documentation of design choices. Why are we using Tukey and KNN? I wish the Pipeline class allowed us to attach meta information about why we chose the steps.\n",
        "\n",
        "My alternative is to create a markdown file that provides the documentation. I saved it under https://raw.githubusercontent.com/fickas/course_datasets/refs/heads/main/pipeline-documentation.md. You can see its contents rendered in the text cell below. We will get back to it later so make a note of it. For now, nothing to turn in.\n",
        "\n",
        "Notice I included the screenshot I took of my final pipeline. You can double-click the cell below to see my markdown."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic Data Pipeline Documentation\n",
        "\n",
        "## Pipeline Overview\n",
        "This pipeline preprocesses the Titanic dataset to prepare it for machine learning modeling. It handles categorical encoding, target encoding, outlier detection and treatment, feature scaling, and missing value imputation.\n",
        "\n",
        "![Pipeline Diagram](https://github.com/fickas/asynch_models/blob/main/transform_pipeline.png?raw=true)\n",
        "\n",
        "\n",
        "## Step-by-Step Design Choices\n",
        "\n",
        "### 1. Gender Mapping (`map_gender`)\n",
        "- **Transformer:** `CustomMappingTransformer('Gender', {'Male': 0, 'Female': 1})`\n",
        "- **Design Choice:** Binary encoding of gender with female as 1 and male as 0\n",
        "- **Rationale:** Simple categorical mapping that preserves the binary nature of the feature without increasing dimensionality\n",
        "\n",
        "### 2. Class Mapping (`map_class`)\n",
        "- **Transformer:** `CustomMappingTransformer('Class', {'Crew': 0, 'C3': 1, 'C2': 2, 'C1': 3})`\n",
        "- **Design Choice:** Ordinal encoding of passenger class from lowest (Crew) to highest (C1)\n",
        "- **Rationale:** Preserves the inherent ordering of passenger classes on the Titanic\n",
        "\n",
        "### 3. Target Encoding for Joined Column (`target_joined`)\n",
        "- **Transformer:** `CustomTargetTransformer(col='Joined', smoothing=10)`\n",
        "- **Design Choice:** Target encoding with smoothing factor of 10\n",
        "- **Rationale:**\n",
        "  - Replaces the categorical 'Joined' feature with its relationship to the target variable\n",
        "  - Smoothing=10 balances between using the global mean (high smoothing) and the category mean (low smoothing)\n",
        "  - Helps address potential overfitting from rare categories\n",
        "\n",
        "### 4. Outlier Treatment for Age (`tukey_age`)\n",
        "- **Transformer:** `CustomTukeyTransformer(target_column='Age', fence='outer')`\n",
        "- **Design Choice:** Tukey method with outer fence for identifying extreme outliers\n",
        "- **Rationale:**\n",
        "  - Outer fence (Q1-3IQR, Q3+3IQR) identifies only the most extreme outliers\n",
        "  - Age may have legitimate outliers (very young or old passengers) that should be preserved unless extreme\n",
        "\n",
        "### 5. Outlier Treatment for Fare (`tukey_fare`)\n",
        "- **Transformer:** `CustomTukeyTransformer(target_column='Fare', fence='outer')`\n",
        "- **Design Choice:** Tukey method with outer fence for identifying extreme outliers\n",
        "- **Rationale:**\n",
        "  - Fare prices have high variability and legitimate outliers for luxury accommodations\n",
        "  - Outer fence preserves most of the original distribution while handling extreme values\n",
        "\n",
        "### 6. Age Scaling (`scale_age`)\n",
        "- **Transformer:** `CustomRobustTransformer(target_column='Age')`\n",
        "- **Design Choice:** Robust scaling for Age feature\n",
        "- **Rationale:**\n",
        "  - Robust to outliers compared to standard scaling\n",
        "  - Uses median and interquartile range instead of mean and standard deviation\n",
        "  - Appropriate for Age which may not follow normal distribution\n",
        "\n",
        "### 7. Fare Scaling (`scale_fare`)\n",
        "- **Transformer:** `CustomRobustTransformer(target_column='Fare')`\n",
        "- **Design Choice:** Robust scaling for Fare feature\n",
        "- **Rationale:**\n",
        "  - Fare has high variability and skewed distribution\n",
        "  - Robust scaling reduces influence of remaining outliers after Tukey treatment\n",
        "\n",
        "### 8. Imputation (`impute`)\n",
        "- **Transformer:** `CustomKNNTransformer(n_neighbors=5)`\n",
        "- **Design Choice:** KNN imputation with 5 neighbors\n",
        "- **Rationale:**\n",
        "  - Uses relationships between features to estimate missing values\n",
        "  - k=5 balances between too few neighbors (overfitting) and too many (underfitting)\n",
        "  - More appropriate than simple mean/median imputation given the relationships in Titanic data\n",
        "\n",
        "## Pipeline Execution Order Rationale\n",
        "1. Categorical encoding first to prepare for subsequent numerical operations\n",
        "2. Target encoding next as it requires original categorical values\n",
        "3. Outlier treatment before scaling to prevent outliers from affecting scaling parameters\n",
        "4. Scaling before imputation so that distance metrics in KNN aren't skewed by unscaled features\n",
        "5. Imputation last to fill missing values using all preprocessed features\n",
        "\n",
        "## Performance Considerations\n",
        "- RobustScaler instead of StandardScaler due to presence of outliers\n",
        "- KNN imputation instead of simple imputation to preserve relationships between features\n",
        "- Target encoding with smoothing for categorical features with many levels"
      ],
      "metadata": {
        "id": "wAJ3vZdhKe6n"
      }
    }
  ]
}