{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Preface\n",
        "\n",
        "Updated notebook slightly. For challenge 2, in the past I resorted to Googling for answers. Yes, I confess, I do not have the entire pandas and numpy libraries memorized by heart. This year I decided to try Gemini instead. As noted, it can be quite powerful in that it can read the text and code cells of the notebook to help it formulate an answer. I was impressed."
      ],
      "metadata": {
        "id": "koHImDcUDPVB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>Chapter Three</h1>\n",
        "</center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## LEARNING OBJECTIVES:\n",
        "- Start your own GitHub library to store useful functions, classes, etc., that you can use in future.\n",
        "- Another look at feature engineering using Pearson correlations. Capture your work in a custom Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y1huouejzY5"
      },
      "source": [
        "#I. Capture past work\n",
        "<img src='https://www.dropbox.com/s/9fcc1crlxp19ijt/major_section.png?raw=1' width='300'>\n",
        "\n",
        "I'd like to avoid redefining classes and other things at the top of each subsequent chapter. I'd like you to use github as a place you can save the work you do, week by week, and then load it back in future weeks.\n",
        "\n",
        "In the past, I've asked students to build a full-blown library on github and then use `import` to load it in. This year I would like to try something simpler (even though maybe not as elegant).\n",
        "Just use `wget` command to download a python file (script) from github then run it in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For the brave\n",
        "\n",
        "I actually set up github to publish my repository/library to PyPi. I used this guide: [github to PyPi](https://packaging.python.org/en/latest/guides/publishing-package-distribution-releases-using-github-actions-ci-cd-workflows/). Then you can just do this in your notebook:\n",
        "\n",
        "<pre>\n",
        "!pip install mylibrary\n",
        "import mylibrary\n",
        "</pre>\n",
        "\n",
        "That said, it took me half a day to debug the guide and get things set up. If anyone wants to try, I'll help as I can.\n",
        "\n",
        "For now, let's just use a quick way to get library code loaded."
      ],
      "metadata": {
        "id": "bOWn_Ribr1LT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaXENKMpMRF3"
      },
      "source": [
        "#II. Save a script that will load past work\n",
        "\n",
        "Follow these steps along with me.\n",
        "\n",
        "1. Go to github and create a new repository. Call it what you want, e.g., `cis423`.\n",
        "\n",
        "1. In your github repository, create a new python file. Call it  `library.py` or something similar.\n",
        "\n",
        "2. Paste this code into that file.\n",
        "<pre>\n",
        "from __future__ import annotations  #must be first line in your library!\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import types\n",
        "from typing import Dict, Any, Optional, Union, List, Set, Hashable, Literal, Tuple, Self, Iterable\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import sklearn\n",
        "sklearn.set_config(transform_output=\"pandas\")  #says pass pandas tables through pipeline instead of numpy matrices\n",
        "</pre>\n",
        "\n",
        "3. Click raw then copy the url.\n",
        "\n",
        "4. Pull out the pieces to set the top 3 variables below.\n",
        "\n",
        "5. Now run the cell below.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "github_name = 'smith'  #fill in with your user name\n",
        "repo_name = 'cis423'   #fill in with your repo name\n",
        "source_file = 'library.py'  #fill in with file name\n",
        "\n",
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ],
      "metadata": {
        "id": "MR8flZfok2f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2uG1AG1qgZe"
      },
      "source": [
        "The `wget` command will retrieve a file from a url and store it locally.\n",
        "\n",
        "The `%run` command is called a Jupyter magic command. It allows you to run files from storage, local Colab storage in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8Q3xZYfzS1"
      },
      "source": [
        "#Make sure we got what we wanted - these all should be defined vars now\n",
        "\n",
        "annotations\n",
        "pd\n",
        "np\n",
        "types\n",
        "Dict, Any, Optional, Union, List, Set, Hashable, Literal, Tuple, Self, Iterable\n",
        "BaseEstimator, TransformerMixin\n",
        "Pipeline\n",
        "set_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RECPdgWWU1Jw"
      },
      "source": [
        "##Bring in titanic data (trimmed)\n",
        "\n",
        "This is from chapter 2. I did what I promised. I downloaded to my computer and then uploaded to a repository `course_datasets` on my GitHub account. I then chose the file and clicked Raw button. Finally I copied the url and pasted it in below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLcxR3ZthbC8"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/fickas/asynch_models/refs/heads/main/datasets/titanic_trimmed.csv'\n",
        "titanic_table = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq4mr3ishbC9"
      },
      "source": [
        "titanic_table.head()  #print first 5 rows of the table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HHuYDVOVaXJ"
      },
      "source": [
        "###Produce feature columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilJGPjQyVhzc"
      },
      "source": [
        "titanic_features = titanic_table.drop(columns='Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiS6go_Thr-k"
      },
      "source": [
        "#Challenge 1\n",
        "\n",
        "Let's start building up your library. We will be copying code over from chapter 2 mostly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1.1\n",
        "\n",
        "Add your transformer classes from chapter 2 to your library. Simple copy and paste. This should include\n",
        "\n",
        "* CustomMappingTransformer\n",
        "* CustomOHETransformer\n",
        "* CustomDropColumnsTransformer\n"
      ],
      "metadata": {
        "id": "cO3t65xX9GwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1.2\n",
        "\n",
        "Add your two pipelines from chapter 2:\n",
        "\n",
        "* titanic_transformer = Pipeline(...)\n",
        "* customer_transformer = Pipeline(...)\n",
        "\n",
        "Note that while your transformers will not change over time (unless you find a bug in them!), you will continue to build up these pipelines over the next several chapters."
      ],
      "metadata": {
        "id": "OFbbCx_e9OfY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1kZPelDvXfE"
      },
      "source": [
        "Now rerun your script."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ],
      "metadata": {
        "id": "7SDPTC01vkck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_transformer  #should produce nice picture"
      ],
      "metadata": {
        "id": "DDWC9MC653Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_transformer  #should produce nice picture"
      ],
      "metadata": {
        "id": "1IQqKZhC-MsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###You can click on the boxes to get more information"
      ],
      "metadata": {
        "id": "n02bbSLNWYZo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcdXLjcLvqrg"
      },
      "source": [
        "Run the Titanic pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df = titanic_transformer.fit_transform(titanic_features)"
      ],
      "metadata": {
        "id": "ZFx6kae86Jx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_df.head()"
      ],
      "metadata": {
        "id": "SO-3Kcp97tF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What I see\n",
        "\n",
        "|index|Age|Gender|Class|Married|Fare|Joined\\_Belfast|Joined\\_Cherbourg|Joined\\_Queenstown|Joined\\_Southampton|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|0|41\\.0|0|1\\.0|0\\.0|7\\.0|0|0|0|1|\n",
        "|1|21\\.0|0|0\\.0|0\\.0|0\\.0|0|0|0|1|\n",
        "|2|13\\.0|0|1\\.0|NaN|20\\.0|0|0|0|1|\n",
        "|3|16\\.0|0|1\\.0|0\\.0|NaN|0|0|0|1|\n",
        "|4|NaN|0|2\\.0|0\\.0|24\\.0|0|1|0|0|"
      ],
      "metadata": {
        "id": "rgy3m9rvW01l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_3SEFjRVirB"
      },
      "source": [
        "#Challenge 2\n",
        "\n",
        "In a chapter 1 challenge, we looked briefly at a method for computing column (feature) correlations. In this, challenge, I'd like to look at another called the Pearson correlation coefficient. You can read about it in link below. Probably good to know about the basics: it is the kind of question that could come up on a job interview. [Pearson CC](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n",
        "\n",
        "Good news:\n",
        "pandas has a method for computing a pairwise PCC for a dataframe and then putting that in a table. Check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9keKwACHrIM"
      },
      "source": [
        "df_corr = transformed_df.corr(method='pearson')\n",
        "df_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLRvHVjQIJ2g"
      },
      "source": [
        "A cell value is in the range -1 (perfectly reversed/negatively correlated) to 1 (perfectly positively correlated). Looking at the table above I can see that there is a value of `0.3896` between `Fare` and `Class`. Is this high? It is a subjective question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIMMIb1yaCNR"
      },
      "source": [
        "##pandas does not complete the process\n",
        "\n",
        "The question is what to do with this correlation information. I'm going to suggest that if columns C1 and C2 are correlated above some threshold, that we drop C2. We don't need it. We can get by with just C1. This is called feature reduction.\n",
        "\n",
        "The bad news is that pandas does not give us a way to get from the `corr` table to dropping columns. That's where you come in. Given the `corr` table above, I'll step you through getting to a place where we can drop columns. There are many ways to do this. I am using the one that makes the most sense to me.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBNRL1egIFk"
      },
      "source": [
        "##Step 1.1 Create a Boolean table\n",
        "\n",
        "Each cell is `True` if it has a correlation greater than a threshold and `False` if not.\n",
        "\n",
        "Your choice of threshold is, of course, critical. I chose `.4` because it gives us a column to drop. But it is probably too low in general.\n",
        "\n",
        "Here is your target. You can double check with `corr` table above to verify that cells with `True` have `abs(PCC)>.4`. Remember that we want a True for values `< -.4`. Highly positive and highly negative both are reasons for concern.\n",
        "\n",
        "|index|Age|Gender|Class|Married|Fare|Joined\\_Belfast|Joined\\_Cherbourg|Joined\\_Queenstown|Joined\\_Southampton|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|Age|true|false|false|false|false|false|false|false|false|\n",
        "|Gender|false|true|false|false|false|false|false|false|false|\n",
        "|Class|false|false|true|false|false|false|false|false|false|\n",
        "|Married|false|false|false|true|false|false|false|false|false|\n",
        "|Fare|false|false|false|false|true|false|false|false|false|\n",
        "|Joined\\_Belfast|false|false|false|false|false|true|false|false|false|\n",
        "|Joined\\_Cherbourg|false|false|false|false|false|false|true|false|true|\n",
        "|Joined\\_Queenstown|false|false|false|false|false|false|false|true|false|\n",
        "|Joined\\_Southampton|false|false|false|false|false|false|true|false|true|"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = .4"
      ],
      "metadata": {
        "id": "tFYKxBa3crP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHqepMQBZnZC"
      },
      "source": [
        "#Hint: there is a way to create a new table with only absolute values. No loops needed.\n",
        "#And there is a way to create a True/False table based on a condition. No loops needed.\n",
        "#And you can do it in one line if you feel brave!\n",
        "\n",
        "masked_df =\n",
        "masked_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What I get\n",
        "\n",
        "|index|Age|Gender|Class|Married|Fare|Joined\\_Belfast|Joined\\_Cherbourg|Joined\\_Queenstown|Joined\\_Southampton|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|Age|true|false|false|false|false|false|false|false|false|\n",
        "|Gender|false|true|false|false|false|false|false|false|false|\n",
        "|Class|false|false|true|false|false|false|false|false|false|\n",
        "|Married|false|false|false|true|false|false|false|false|false|\n",
        "|Fare|false|false|false|false|true|false|false|false|false|\n",
        "|Joined\\_Belfast|false|false|false|false|false|true|false|false|false|\n",
        "|Joined\\_Cherbourg|false|false|false|false|false|false|true|false|true|\n",
        "|Joined\\_Queenstown|false|false|false|false|false|false|false|true|false|\n",
        "|Joined\\_Southampton|false|false|false|false|false|false|true|false|true|"
      ],
      "metadata": {
        "id": "XVeKFiHfX6rX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGFWKxxOgfnM"
      },
      "source": [
        "##Step 1.2 Mask off bottom triangle\n",
        "\n",
        "The table is symmetrical. I only need to work on one half of it. I've chosen to work on upper half (triangle) somewhat arbitrarily. So I want to change all values below the diagonal to `False`.\n",
        "\n",
        "And oh, I want to change the diagonal, itself, to `False`. It has PCC values of 1 (True) that are spurious.\n",
        "\n",
        "Numpy has a nice method for doing what I want. I found it by asking Gemini for help.\n",
        "\n",
        "Here is your target.\n",
        "\n",
        "<pre>\n",
        "array([[False, False, False, False, False, False, False, False, False],\n",
        "       [False, False, False, False, False, False, False, False, False],\n",
        "       [False, False, False, False, False, False, False, False, False],\n",
        "       [False, False, False, False, False, False, False, False, False],\n",
        "       [False, False, False, False, False, False, False, False, False],\n",
        "       [False, False, False, False, False, False, False, False, False],\n",
        "       [False, False, False, False, False, False, False, False,  True], #one True here\n",
        "       [False, False, False, False, False, False, False, False, False],\n",
        "       [False, False, False, False, False, False, False, False, False]])\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIdPFcnOboXq"
      },
      "source": [
        "upper_mask =\n",
        "upper_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9NtbJO8hNJh"
      },
      "source": [
        "##Step 1.3 Find correlated columns\n",
        "\n",
        "I'll look at each column. If it has any True values, then it is correlated with another column or columns. I want to drop it.\n",
        "\n",
        "You can eyeball what I have above to see that the the `Joined_Southampton` has a `True` value. So that is what I expect to find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8T1kwrBd6bB"
      },
      "source": [
        "#I used a list comprehension  with enumerate in the generator. It gave me both a column name and its index in upper_mask.\n",
        "#numpy has a method for looking for any True values in a matrix column.\n",
        "\n",
        "correlated_columns =\n",
        "\n",
        "correlated_columns  #['Joined_Southampton']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdM0PXEXiMwh"
      },
      "source": [
        "##Step 1.4 Drop correlated column(s)\n",
        "\n",
        "We have seen how to do this before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NSXUIouelLF"
      },
      "source": [
        "new_df ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcD8NhHPdjf-"
      },
      "source": [
        "set(transformed_df.columns) - set(new_df.columns)  #{'Joined_Southampton'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0TsuzZ-R-bn"
      },
      "source": [
        "##Caveat 1\n",
        "\n",
        "The question of correlations is a deep one and we are just scratching the surface. Many popular methods do not simply drop columns, they build a brand new set of reduced columns. For your interview question prep, you should probably at least look at linear algebra methods such as PCA and SVD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSYgLFubeM64"
      },
      "source": [
        "##Caveat 2\n",
        "\n",
        "It would be interesting to compare `dcor` from chapter 1 with PCC. I would expect dcor to be better. The problem is that it will not work with NaN values so can't do it here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_evo4TfNDbxt"
      },
      "source": [
        "##Caveat 3\n",
        "\n",
        "One reason people look at correlations and feature reduction is to give machine learning models a less complex dataset. I am on the edge with this. It is true that in some cases reducing the number of columns can help a machine model learn. But in other cases, the model will learn to ignore columns that do not supply useful information. In the latter case, you could be wasting considerable time with a task that is unnecessary.\n",
        "\n",
        "It is also the case that there are automated tools for doing feature reduction, taking the burden off of you. If we have time, we might discuss a few later in the course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B--_3XwHrEqr"
      },
      "source": [
        "#Challenge 3\n",
        "\n",
        "Build a CustomPearsonTransformer. Plug in your code from Challenge 2 into the transform method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A Prelude to your class\n",
        "\n",
        "Up until now the `fit` method has done nothing and the `transform` method has done everything. I'd like you to change things up. Please have the `fit` method do most of the work. In particular, it computes the list of columns to drop (but does not drop them). The `transform` method simply drops the list of columns computed by `fit`.\n",
        "\n",
        "Note you will have to think a bit about how to know if transform has been called before fit. I want an assertion error in that case."
      ],
      "metadata": {
        "id": "GDW4BIA3gzNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomPearsonTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom scikit-learn transformer that removes highly correlated features\n",
        "    based on Pearson correlation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    threshold : float\n",
        "        The correlation threshold above which features are considered too highly correlated\n",
        "        and will be removed.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    correlated_columns : Optional[List[Hashable]]\n",
        "        A list of column names (which can be strings, integers, or other hashable types)\n",
        "        that are identified as highly correlated and will be removed.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "V5SPQGRONMdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test it out\n",
        "pt = CustomPearsonTransformer(.4)"
      ],
      "metadata": {
        "id": "-Ty6kpojr_w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pt.transform(transformed_df)  #AssertionError: PearsonTransformer.transform called before fit."
      ],
      "metadata": {
        "id": "QlKTt5kIecb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pt.fit_transform(transformed_df)  #list of columns to drop"
      ],
      "metadata": {
        "id": "AaqSsb98eshI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(transformed_df.columns) - set(new_df.columns)  #{'Joined_Southampton'}"
      ],
      "metadata": {
        "id": "TDDsWtOpei5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Try with lower threshold"
      ],
      "metadata": {
        "id": "lEGQnOab8p0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test it out\n",
        "pt = CustomPearsonTransformer(.35)\n",
        "new_df = pt.fit_transform(transformed_df)\n",
        "set(transformed_df.columns) - set(new_df.columns)  #{'Fare', 'Joined_Southampton'}"
      ],
      "metadata": {
        "id": "gLRwKLhPxi5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##You can add this transformer to your library if you want.\n",
        "\n",
        "I don't see us using it in future but good to have around.\n",
        "\n",
        "To repeat caveat 3: my preference is to let (a) the models, themselves, or (b) tools built specifically for this job, sort out which columns are imporant and which are not."
      ],
      "metadata": {
        "id": "WYuRqFkwyi1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A few follow up methods for feature reduction\n",
        "\n",
        "https://medium.com/@sumantabasak/know-these-already-few-powerful-feature-selection-algorithms-18a5a27c1cd3\n",
        "\n",
        "https://towardsdatascience.com/boruta-and-shap-for-better-feature-selection-20ea97595f4a. This link in particular talks about tools built just for the job, e.g., SHAP."
      ],
      "metadata": {
        "id": "KZGZimg6AdVF"
      }
    }
  ]
}