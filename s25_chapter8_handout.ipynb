{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>Chapter Eight</h1>\n",
        "</center>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZiquu_S3vZG"
      },
      "source": [
        "## Set-up\n",
        "\n",
        "First bring in your library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ6MOQmuVewi"
      },
      "outputs": [],
      "source": [
        "github_name = 'smith'\n",
        "repo_name = 'mlops'\n",
        "source_file = 'library.py'\n",
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAUvxVCo5bta"
      },
      "outputs": [],
      "source": [
        "type(find_random_state), type(titanic_transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGZzPA8QkCua"
      },
      "outputs": [],
      "source": [
        "titanic_variance_based_split, customer_variance_based_split  #(107, 113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hje9GNDR5bvs"
      },
      "outputs": [],
      "source": [
        "#just to be sure you match mine\n",
        "\n",
        "titanic_variance_based_split = 107\n",
        "customer_variance_based_split = 113"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npa06nZLAvs-"
      },
      "source": [
        "# I. Our first machine learning algorithm: linear regression\n",
        "\n",
        "In this chapter we will start the second part of the course, exploring machine learning models to predict (AKA classify). I know I said we would concentrate on binary classification, and we will. But it is a little easier to start with an algorithm called linear regression, which can be used to predict numerical values in general, e.g., Age, price of stock, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpc1PfunSdiv"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/fickas/asynch_models/refs/heads/main/datasets/titanic_trimmed.csv'\n",
        "titanic_trimmed = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JXOj2s9VXHC"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic_trimmed.drop(columns='Survived')\n",
        "titanic_features.head()  #print first 5 rows of the table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s50JGZMKcCUF"
      },
      "outputs": [],
      "source": [
        "labels = titanic_trimmed['Survived'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuVMbHSMiQvS"
      },
      "source": [
        "And split using the `random_state` value we found last chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJJ_RornLXy"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(titanic_features, labels, test_size=0.2, shuffle=True,\n",
        "                                                    random_state=titanic_variance_based_split, stratify=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwGhOZdaoc0m"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6rHzNYJuN4B"
      },
      "outputs": [],
      "source": [
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MxQkbcQi59k"
      },
      "source": [
        "# X. Run the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds9lMStb0f1H"
      },
      "source": [
        "## Remember want to transform train and test separately\n",
        "\n",
        "To avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl_YHIhK0TLW"
      },
      "outputs": [],
      "source": [
        "X_train_transformed = titanic_transformer.fit_transform(X_train, y_train)  #fit with train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBZJCM0I1X0O"
      },
      "outputs": [],
      "source": [
        "X_train_transformed.head().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKoleWOQQQ73"
      },
      "source": [
        "|index|Age|Gender|Class|Joined|Married|Fare|\n",
        "|---|---|---|---|---|---|---|\n",
        "|460|0\\.79|1\\.0|1\\.0|0\\.4|0\\.0|-0\\.26|\n",
        "|643|-1\\.32|0\\.0|1\\.0|0\\.4|0\\.0|0\\.61|\n",
        "|802|0\\.05|1\\.0|0\\.0|0\\.4|0\\.0|3\\.74|\n",
        "|788|-0\\.42|1\\.0|2\\.0|0\\.4|0\\.0|0\\.57|\n",
        "|582|0\\.32|0\\.0|2\\.0|0\\.4|0\\.0|-0\\.13|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlD7cdqlfK2A"
      },
      "outputs": [],
      "source": [
        "X_test_transformed = titanic_transformer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK9BeNXPfK2B"
      },
      "outputs": [],
      "source": [
        "X_test_transformed.head().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BawArin-QaCw"
      },
      "source": [
        "|index|Age|Gender|Class|Joined|Married|Fare|\n",
        "|---|---|---|---|---|---|---|\n",
        "|1105|-0\\.05|1\\.0|1\\.0|0\\.35|0\\.0|0\\.43|\n",
        "|791|-0\\.95|1\\.0|2\\.0|0\\.4|0\\.0|1\\.13|\n",
        "|984|1\\.0|1\\.0|3\\.0|0\\.61|1\\.0|2\\.74|\n",
        "|418|0\\.21|0\\.0|2\\.0|0\\.4|0\\.0|0\\.57|\n",
        "|103|-0\\.37|0\\.0|2\\.0|0\\.4|0\\.0|0\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZThyDCWjUuc"
      },
      "source": [
        "# II. Simple linear regression\n",
        "\n",
        "We can (and will shortly) use linear regression with multiple feature columns and one label column. But I am going to simplify the problem by assuming we have a table with only one feature column. Let's assume it is the `Age` column.\n",
        "\n",
        "And I assume our label column will be `Fare`. So our goal is to build an algorithm that can predict `Fare` given `Age`. Linear regression would be a good place to start with this problem.\n",
        "\n",
        "Here is our algorithm: pretty simple, right?\n",
        "\n",
        "<img src='https://www.dropbox.com/s/uxq0lu4nu4n0ads/Screen%20Shot%202021-09-17%20at%202.07.06%20PM.png?raw=1' height=100>\n",
        "\n",
        "`y-hat` is the fare we predict given an age of `x`. The values of `m` and `b` are values we have to discover, i.e., learn. The general idea is to keep trying values of `m` and `b` until we find ones that give us good predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqhoan4qmRwd"
      },
      "source": [
        "## What is a good prediction?\n",
        "\n",
        "On the surface, it is how far off our predicted fares are from the real fares. Remember this is a supervised learning problem. We know the real values of the fares.\n",
        "\n",
        "The general approach is to compute an \"error\" (sometimes called loss) that is the square of the difference between what we predict (the y-hat) and the actual value (y without the hat). If we have 1000 samples, then we will get 1000 squared differences. We take the mean of those to get something called the Mean Squared Error or MSE.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/5xgxywc6wo18qx2/Screen%20Shot%202021-09-17%20at%202.36.53%20PM.png?raw=1' height=100>\n",
        "\n",
        "Note there are other error or loss functions possible and we may see them in future, i.e., the error function is not cast in stone to be MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBGvkYWDeMut"
      },
      "source": [
        "### Let's do some computation to see how it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAwbptr0dsAn"
      },
      "outputs": [],
      "source": [
        "ages = X_train_transformed['Age'].to_list()\n",
        "fare_labels = X_train_transformed['Fare'].to_list()  #the yi values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU4lwgwsl2Z4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(ages,fare_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70omeFYmpUfa"
      },
      "source": [
        "## Good luck with that!\n",
        "\n",
        "We are looking for a line that is close to most points. Nice if all the points were grouped together. Kind of like this.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/0xrcgiafl4hm1z1/Screen%20Shot%202022-10-19%20at%202.57.36%20PM.png?raw=1' height=200>\n",
        "\n",
        "But let's give it a go anyway.\n",
        "\n",
        "And notice I switched notation on you a bit, from `(yi-yhat)**2` to `(yhat-yi)**2`. I did it to be more consistent with next set of formulas we will look at.\n",
        "\n",
        "Also, I'll start calling `m` as `w` instead to match formulas below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gk7Y7P2eS0_"
      },
      "outputs": [],
      "source": [
        "w = .5   #total guess\n",
        "b = .05  #total guess\n",
        "predictions = [age*w+b for age in ages]\n",
        "errors_squared = [(yhat-yi)**2 for yhat,yi in zip(predictions, fare_labels)]\n",
        "mse = sum(errors_squared)/len(errors_squared)\n",
        "mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pmA6N55nsOn"
      },
      "outputs": [],
      "source": [
        "ax = plt.axes()\n",
        "ax.scatter(ages, fare_labels)\n",
        "ax.plot(ages, predictions, 'ro')\n",
        "\n",
        "ax.set_xlabel('Ages')\n",
        "ax.set_ylabel('Fares')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_12uzwvnqsJ"
      },
      "source": [
        "## We have a goal: minimize MSE\n",
        "\n",
        "Choose values for `w` and `b`, our parameters, that minimize the MSE.\n",
        "\n",
        "Let's try increasing the weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMuUZ932qRmS"
      },
      "outputs": [],
      "source": [
        "w = .6   #increase\n",
        "b = .05  #stays same\n",
        "predictions = [age*w+b for age in ages]\n",
        "errors_squared = [(yhat-yi)**2 for yhat,yi in zip(predictions, fare_labels)]\n",
        "mse = sum(errors_squared)/len(errors_squared)\n",
        "mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsJi-epMqckF"
      },
      "source": [
        "That was worse than 1.7513294373063293. Let's try the other way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuIixMoRqhXv"
      },
      "outputs": [],
      "source": [
        "w = .4   #decrease\n",
        "b = .05  #stays same\n",
        "predictions = [age*w+b for age in ages]\n",
        "errors_squared = [(yhat-yi)**2 for yhat,yi in zip(predictions, fare_labels)]\n",
        "mse = sum(errors_squared)/len(errors_squared)\n",
        "mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlztv58Ck2pE"
      },
      "source": [
        "Better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4v1Hs7GgTPE"
      },
      "source": [
        "## Let's see if we can get a picture of the curve\n",
        "\n",
        "I'll generate a list of weights and keep b constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0TcJ9i3Es-Z"
      },
      "outputs": [],
      "source": [
        "wi = np.arange(-5, 5, 0.001)  #a bunch of guesses for the weight\n",
        "b = .5  #another guess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOFXw4nsgEg0"
      },
      "outputs": [],
      "source": [
        "wi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wFQeSnZghQb"
      },
      "source": [
        "### A couple helper functions\n",
        "\n",
        "`X` is the feature values, ages in our case.\n",
        "\n",
        "`Y` is the label values, fares in our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LOEU4bxF7ZD"
      },
      "outputs": [],
      "source": [
        "def predict(X, w, b):\n",
        "  yhat = [(x*w+b) for x in X]\n",
        "  return yhat\n",
        "\n",
        "def MSE(Y, Yhat):\n",
        "  sdiffs = [(yhat-y)**2 for y,yhat in zip(Y,Yhat)]\n",
        "  mse = sum(sdiffs)/len(sdiffs)\n",
        "  return mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPujXfDdgoRT"
      },
      "source": [
        "### Compute yhat and mse for different weights in wi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7ja5pMGEA5"
      },
      "outputs": [],
      "source": [
        "pairs = []\n",
        "for w in wi:\n",
        "  yhat = predict(ages, w, b)\n",
        "  mse = MSE(fare_labels, yhat)\n",
        "  pairs.append((w, mse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3eYtazarvZe"
      },
      "outputs": [],
      "source": [
        "min(pairs, key=lambda pair: pair[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8C9QOIWRRFC"
      },
      "source": [
        "So with a weight of .19 we got an mse of 1.5, which was the minimum mse out of all the weights we tried."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iEAOwWUguAC"
      },
      "source": [
        "### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtnyS-woG8XU"
      },
      "outputs": [],
      "source": [
        "ax = plt.axes()\n",
        "ax.plot([w for w,_ in pairs], [m for _,m in pairs])\n",
        "ax.set_xlabel('w')\n",
        "ax.set_ylabel('mse')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rWJGpMuhEbR"
      },
      "source": [
        "## Looks good so what's the problem?\n",
        "\n",
        "We can just try a list of weights and find the one with the least MSE. Choose it.\n",
        "\n",
        "The problem is that we really have 2 values to find, weight and bias.\n",
        "\n",
        "You say, so? Write nested for loops that try all combinations and find best MSE.\n",
        "\n",
        "<pre>\n",
        "for w in wi:\n",
        "   for b in bi:\n",
        "      ...\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPUhxPaFiKOo"
      },
      "source": [
        "## The problem is we have a whole row to predict Fare\n",
        "\n",
        "Why just use Age? (Simple Linear Regression) Why not use all the values in the row (Multiple Linear Regression)?\n",
        "\n",
        "<img src='https://miro.medium.com/max/2210/1*iz5-6i0TG23UxeExvT3qeg.png' height=200>\n",
        "\n",
        "Each value will have its own weight. So if we have 7 columns to predict Fare, we will have 7 different weights to contend with.\n",
        "\n",
        "Large numbers of nested loops is getting a bit much. I think there is a better way that works with both Simple and Multiple Linear Regression.\n",
        "\n",
        "Instead of blindly searching, we will try to learn. Big switch!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYhAYa3oWx_J"
      },
      "source": [
        "# The foundations of learning: gradient descent\n",
        "\n",
        "I think it is worth going into some detail about this now given it will crop up again later. We are looking for a principled way to learn from our mistakes. Our mistakes are called errors or loss. MSE is a means to quantify our mistakes.\n",
        "\n",
        "##But how do we learn from our mistakes????\n",
        "\n",
        "I want to switch focus to learning from individual mistakes. So don't compute all the errors first before considering how to change. Instead, look at individual errors, on a single row, and try to adjust weight and bias from the single error.\n",
        "\n",
        "###Here's a plot that may help.\n",
        "\n",
        "<img src='https://www.dropbox.com/scl/fi/gq7mmcr3yyoscmxce1mbi/Loss-MSE-function-Gradient-Descent-using-Backpropagation-with-respect-to-each-Weight.png?rlkey=viuvkpz8n4zhzo3tgmi71j0k3&raw=1' height=300>\n",
        "\n",
        "I have a separate plot for the weight and bias. Let's focus on the weight for now:\n",
        "\n",
        "* The x axis represents different weights we can try.\n",
        "\n",
        "* The y axis represensts the squared error for the weights.\n",
        "\n",
        "* The curve is unknown to us. Shown as a nice bell curve here. But could be any type of convex curve.\n",
        "\n",
        "* The minimum value `m` (the minimum error we can achieve) is unknown to us.\n",
        "\n",
        "##Let's think about point pi\n",
        "\n",
        "We chose some weight, computed yhat, took the difference with actual y, and squared to get an error value. So the weight and error give us point pi.\n",
        "\n",
        "###Question: looking at the plot, should we increase or decrease w, given we are at point pi?\n",
        "\n",
        "##Let's think about point pi+j on other side\n",
        "\n",
        "We chose some weight, computed yhat, took the difference with actual y, and squared to get an error value.\n",
        "\n",
        "###Question: looking at the plot, should we increase or decrease w given at pi+j?\n",
        "\n",
        "The problem is you don't get to see this plot. Without the curve, you can't tell me whether to increase or decrease given some arbitrary point, right?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOeLpwnwu0kg"
      },
      "source": [
        "## Diffie-Q to the rescue!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gyan-16uy6f"
      },
      "source": [
        "\n",
        "Here is my idea. We can get the slope (using the derivative) at any point on the curve using differential equations. And that slope will be either negative or positive.\n",
        "\n",
        "* If it is negative (like line on left) then we want to increase the weight. We are moving down the left hand side of the curve.\n",
        "\n",
        "* If it is positive (like line on right), then we want to decrease the weight. We are moving down the right hand side of the curve.\n",
        "\n",
        "Furthermore, the steepness of the slope tells us how far to move.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*N5WjbzwsCFse-KPjBWZZ6g.jpeg' height=150>\n",
        "\n",
        "So win-win. We know direction to move and how much to move in that direction!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-frqKaNg2H_D"
      },
      "source": [
        "## What we are left with is computing the slope (gradient)\n",
        "\n",
        "Choosing a row with a specific set of values `xi` (an age), `yi` (actual fare), `w` (weight), `b` (bias), compute the slope at the point (w,se), where `se = (yhat-yi)**2 = ((w * xi + b) - yi )**2`.\n",
        "\n",
        "We can use differential equations to get the slope at this point. It's a little bit complicated given we actually have (a) nested functions, and (b) 2 parameters to vary, `w` and `b`. For the nested functions problem, we will have to use the chain rule. For the 2 parameter problem, we will have to use partial differential equations and look at each separately.\n",
        "\n",
        "As a reminder of the chain rule, it looks like this:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/slqxz32445m7gya/Screen%20Shot%202021-09-23%20at%202.59.03%20PM.png?raw=1' height=200>\n",
        "\n",
        "In our case `f` is `(yhat-yi)**2` and `g` is `yhat` which is `((xi*w+b))`.\n",
        "\n",
        "We have 2 parameters to worry about, `w` and `b`. Let's focus on the value of `b`. So we are after the slope, using derivates, for point (b,se) where we hold w constant. In terms of partial differential equations and the chain rule, it looks like this, where L is se (squared error).\n",
        "\n",
        "<img src='https://www.dropbox.com/s/igvgqm7tuq8zqm7/Screen%20Shot%202021-09-23%20at%2011.22.56%20AM.png?raw=1' height=50>.\n",
        "\n",
        "In words, the derivate of the loss L with respect to b (i.e., the slope at point (b,se)) is equal to (i) the derivate of L with respect to yhat (the outer function f) times (ii) the derivate of yhat with respect to b (the inner function).\n",
        "\n",
        "Now we need to compute the 2 derivates on the right of the equation. Using the power rule, the outer part (i.e., `(yhat-yi)**2`) is given by:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/nx1w7ksmh4awrsw/Screen%20Shot%202021-09-23%20at%2011.28.53%20AM.png?raw=1' height=50>\n",
        "\n",
        "The inner part, where `yhat = w*xi+b`, is simply:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/ex6qem7zahu7mvp/Screen%20Shot%202021-09-23%20at%2012.03.18%20PM.png?raw=1' height=50>\n",
        "\n",
        "Why 1? Because I can ignore `w*xi` - it has no term involving `b`. I'm left with `1*b` whose derivative is `1`.\n",
        "\n",
        "So the derivate with respect to b is given by `2*(yhat-yi)*1`. Once we plug in values to get yhat, we get the slope or gradient at (b,se).\n",
        "\n",
        "###How about derivate for w?\n",
        "\n",
        "It is similar when looking at derivate of other parameter `w` (view `w1` as plain `w`):\n",
        "\n",
        "<img src='https://www.dropbox.com/s/2nxl57gy2cf0fer/Screen%20Shot%202021-09-23%20at%2012.06.17%20PM.png?raw=1' height=50>\n",
        "\n",
        "Outer is still `2*(yhat-yi)`. But inner has changed. Reminder that inner function is `(w * xi + b)`. We can ignore the b term. So left with derivate of `w*xi`. It is just `xi`.\n",
        "\n",
        "<img src = 'https://www.dropbox.com/s/urtjgv6cn974kq4/Screen%20Shot%202021-09-23%20at%2012.02.19%20PM.png?raw=1' height=50>\n",
        "\n",
        "In code terms, we have:\n",
        "<pre>\n",
        "outer_yhat = 2*(yhat-yi)  #same for both b and w\n",
        "inner_w = xi\n",
        "inner_b = 1\n",
        "gradient_w = outer_yhat * inner_w #chain rule\n",
        "gradient_b = outer_yhat * inner_b #chain rule\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld5U6hqJgKzk"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "\n",
        "Let's check out the gradient at a specific point. Fill in below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0uYGvhdqzeb"
      },
      "outputs": [],
      "source": [
        "xi = ages[0]  #using this to predict Fare\n",
        "w = .4\n",
        "b = .05\n",
        "yi = fare_labels[0]  #the actual value of Fare we can check against\n",
        "yhat = (w*xi+b)\n",
        "se = (yhat - yi)**2\n",
        "print(yhat, yi, se)  #0.36578947368421055 -0.2608695652173913 0.39270155103707927"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpsH4Nv1Tw9r"
      },
      "outputs": [],
      "source": [
        "\n",
        "#your code below - copy and paste above should do it\n",
        "\n",
        "outer_yhat =   #same for both b and w\n",
        "inner_w =\n",
        "inner_b =\n",
        "gradient_w =  #chain rule\n",
        "gradient_b =  #chain rule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmUSWhjNG1WO"
      },
      "outputs": [],
      "source": [
        "(gradient_w, gradient_b)  #(0.9894616403709502, 1.2533180778032036)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S6qHpTngJZD"
      },
      "source": [
        "\n",
        "Next question: should we add or subtract these gradients to move loss closer to 0? From the x-y plot above, we can see the point on the right has a positive slope and we want to decrease `w` and `b` to move to left. So if we subtract we will get a negative and actually subtract the gradients. That is what we want. And vice versa for point on left. So what we want is this:\n",
        "<pre>\n",
        "w = w - lr * gradient_w\n",
        "b = b - lr * gradient_b\n",
        "</pre>\n",
        "Cool. We will now move in the correct direction and by the correct magnitude. But what's up with that `lr`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlk2hFY-E-eZ"
      },
      "source": [
        "## The learning rate\n",
        "\n",
        "It is typical to tamp down weight changes. The problem is that raw gradients will swing you all over the place. The bottom shows this graphically. The top shows what happens when use small learning rate and bottom with large (i.e., close to 1.0) learning rate.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/hqcpkam704j58yk/Screen%20Shot%202021-09-23%20at%2012.15.20%20PM.png?raw=1' height=200>\n",
        "\n",
        "The choice for the learning rate is up to you. I will use `lr=.001` but it is something you can experiment with.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_WezcFi4hCm"
      },
      "source": [
        "## We can now compute new w and b\n",
        "\n",
        "Notice both slopes are positive so we will decrease both w and b. And from our prior messing around, that looked like a winning move at least for w."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k83FUoK14mEL"
      },
      "outputs": [],
      "source": [
        "lr = .001\n",
        "w = w - lr*gradient_w  #will decrease w in our case\n",
        "b = b - lr*gradient_b  #will decrease b in our case\n",
        "yhat = ages[0]*w+b\n",
        "error_squared = (yhat - yi)**2\n",
        "error_squared  #0.39015584957150207 (prior 0.39270155103707927)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qobUcTCY6PTK"
      },
      "source": [
        "## Whew!\n",
        "\n",
        "Looks like we are on right track. We did decrease the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGwwfoDl3STD"
      },
      "source": [
        "# II. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Using the SGD method for getting to the right w and b values means adjusting w and b after every row. So we don't wait to compute MSE at end, we actually adjust after every row. Here is a function that does this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VkEENfCdgEU"
      },
      "outputs": [],
      "source": [
        "#X is list of ages, Y is list of fare_labels, w is starting weight, b is stating bias, lr is learning rate\n",
        "\n",
        "def sgd(X,Y,w,b,lr=.001):\n",
        "  for i in range(len(X)):\n",
        "    #get values for rowi\n",
        "    xi = X[i]  #e.g., age on rowi\n",
        "    yi = Y[i]  #e.g., actual fare on rowi\n",
        "\n",
        "    yhat = w*xi+b  #prediction\n",
        "\n",
        "    #loss = (yhat - yi)**2 = ((w*xi+b) - yi)**2, i.e., f(g(x,w,b),yi)\n",
        "    #dloss/dw = dl/dyhat * dyhat/dw by the chain rule\n",
        "    #dloss/dyhat = 2*(yhat - yi) by the power rule (first part of chain)\n",
        "    #dyhat/dw = d((w*xi+b) - yi)/dw = xi (second part of chain)\n",
        "\n",
        "    gradient_w = 2*(yhat-yi)*xi  #take the partial derivative wrt w to get slope\n",
        "\n",
        "    #for b same first part of chain but then #dyhat/db = d((w*xi+b) - yi)/db = 1 for second part of chain\n",
        "\n",
        "    gradient_b = 2*(yhat-yi)*1   #take the partial derivative wrt b to get slope\n",
        "\n",
        "    w = w - lr*gradient_w  #if len(X) is 2000, will change 2000 times\n",
        "    b = b - lr*gradient_b\n",
        "\n",
        "  #return the last w and b of the loop\n",
        "  return w,b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDnG8D8x8KZP"
      },
      "source": [
        "## This function will give us final values for w and b\n",
        "\n",
        "After it runs through all rows. So it will do many adjustments to w and b."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMXa7lRI8hl1"
      },
      "outputs": [],
      "source": [
        "w1,b1 = sgd(ages, fare_labels, .5, .5)  #could also just choose random starting values for w and b\n",
        "print(w1,b1)  #0.2989362264334249 0.52546747073015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2bp4hcddwTW"
      },
      "source": [
        "### Compute MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfxLR2hx8Db_"
      },
      "outputs": [],
      "source": [
        "yhats = predict(ages, w1, b1)  #using w1 and b1 we got from sgd\n",
        "MSE(fare_labels, yhats)  #1.5033350952606843"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr7MV_ud9N6c"
      },
      "source": [
        "## Big question: can we do better?\n",
        "\n",
        "We have an MSE of 1.5 (rounded). Is there a way to reduce that?\n",
        "\n",
        "###Big idea: keep running SGD function.\n",
        "\n",
        "So don't stop with just one go-through of rows. Do it again and again. But start each new call with the w and b from previous call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfjYqEBlGqgv"
      },
      "source": [
        "# Idea of epoch\n",
        "\n",
        "Each time we run through the entire set of rows is called an epoch. And we don't re-initialize `w` and `b`! We pass on the values from epoch1 to epoch2. You typically need many epochs to get good results.\n",
        "\n",
        "But how do you know if you have enough epochs? If you choose 100 maybe 200 would give you even better results.\n",
        "\n",
        "You can use the MSE as a measure. If the MSE is not going down very much (or at all), then you are likely good to go.\n",
        "\n",
        "I'll try 100 epochs. And I will keep track of the MSE for each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_2fz0e5DUXG"
      },
      "source": [
        "### Video note\n",
        "\n",
        "Values below will be slightly different than video. However, the main points raised in the video still hold true. Let me know if confused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK4jIEgas8EK"
      },
      "outputs": [],
      "source": [
        "w = .5\n",
        "b = .5\n",
        "mse = []\n",
        "epochs = 100\n",
        "for i in range(epochs):\n",
        "  w,b = sgd(ages, fare_labels, w, b)  #note start with w and b from past epoch\n",
        "  yhats = predict(ages, w, b)\n",
        "  mse.append(MSE(fare_labels, yhats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKEbcyxRySsW"
      },
      "outputs": [],
      "source": [
        "min(mse)  #1.4972186612188838"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1z7Fbk7ybsp"
      },
      "outputs": [],
      "source": [
        "np.argmin(mse)  #so we got the least error at epoch 27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-55LGGuIr3o"
      },
      "outputs": [],
      "source": [
        "#look at last 10\n",
        "\n",
        "mse[-10:]  #holding steady - just slightly worse than best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSWfAPXHpnB-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mse, scaley=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwUL4GDSHkWN"
      },
      "source": [
        "### Flattens out quickly\n",
        "\n",
        "This agrees with where we found the min."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j92_ri5xHzWI"
      },
      "source": [
        "# Last step: testing\n",
        "\n",
        "So we trained our model, i.e., computed what look like good values for `w` and `b`. Let's try those on the data we held out in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omdUlDDAIBhk"
      },
      "outputs": [],
      "source": [
        "(w,b)  #last values computed in training - (w,b)  (0.20755603099245457, 0.5373887976208882)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvbn5uLhIMzl"
      },
      "outputs": [],
      "source": [
        "X = X_test_transformed['Age'].to_list()  #notice using test data not training data\n",
        "Y = X_test_transformed['Fare'].to_list()\n",
        "Yhat = predict(X, w, b)\n",
        "MSE(Y, Yhat)  #1.5755483777709391, best from training: 1.4973830317612864"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdEo90rvI6-G"
      },
      "source": [
        "Worse on testing. Very typical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5ewxadunefB"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "We looked at predicting fare from age. Let's try the reverse: predict age from fare. You might think you will get roughly the same MSE. But don't be too sure :)\n",
        "\n",
        "Hint: you should be able to copy code from above and just edit to try on this new problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSxtPvldjBQh"
      },
      "outputs": [],
      "source": [
        "w = .5\n",
        "b = .5\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYnyae9AoFVD"
      },
      "outputs": [],
      "source": [
        "#your code\n",
        "mse = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBYyGUqNzvNZ"
      },
      "outputs": [],
      "source": [
        "min(mse), np.argmin(mse)  #(0.5601494980426618, 1) - found on 2nd epoch so remaining 98 did no better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svp0vSEOkRSN"
      },
      "outputs": [],
      "source": [
        "mse[-10:]  #worse - [0.5602262156087735, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an8YVZK5lEws"
      },
      "source": [
        "Kind of interesting. Best we got trying to predict fare from age was `1.4973830317612864\n",
        "`. Going other way, a lot better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9X5l26VeGDJ"
      },
      "source": [
        "## Note: you have enough now to work on challenges 1 and 2\n",
        "\n",
        "If you want to pause the video here and work on those 2, no problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvNpqK-p48O0"
      },
      "source": [
        "# Idea of batch\n",
        "\n",
        "Within a single epoch, the concept of batch crops up. It says how many rows do we want to look at before actually changing `w` and `b`. With stochastic gradient descent, we have a batch size of 1: we change weights after every row. Let's look at the other extreme: we change weights only once, after we have seen all rows, i.e., we do one change per epoch.\n",
        "\n",
        "Notice what we are doing. We are collecting changes but not making them. We average the changes and use it to update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUSJyO2Q48O0"
      },
      "outputs": [],
      "source": [
        "def full_batch(X,Y,w,b,lr=.001):\n",
        "  gw = []\n",
        "  gb = []\n",
        "  for i in range(len(X)):\n",
        "    xi = X[i]\n",
        "    yi = Y[i]\n",
        "    yhat = w*xi+b  #prediction\n",
        "    gradient_w = 2*(yhat-yi)*xi\n",
        "    gradient_b = 2*(yhat-yi)*1\n",
        "    gw.append(gradient_w)  #just collect change, don't make it\n",
        "    gb.append(gradient_b)\n",
        "  w = w - lr*sum(gw)/len(gw)  #Now average and make the change.\n",
        "  b = b - lr*sum(gb)/len(gb)\n",
        "  return w,b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9vWXHNkl-1N"
      },
      "source": [
        "### Try full-batch for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nk3Gw_KpCBC"
      },
      "outputs": [],
      "source": [
        "X = X_train_transformed['Age'].to_list()\n",
        "Y = X_train_transformed['Fare'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgksptB348O0"
      },
      "outputs": [],
      "source": [
        "w = .5\n",
        "b = .05\n",
        "mse = []\n",
        "epochs = 100\n",
        "for i in range(epochs):\n",
        "  w,b = full_batch(X, Y, w, b)\n",
        "  Yhat = predict(X, w, b)\n",
        "  mse.append(MSE(Y, Yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1sZcaDyPQrD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mse, scaley=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpXxdMxDlpg7"
      },
      "outputs": [],
      "source": [
        "np.argmin(mse), min(mse)  #from last epoch so looks like still going down - add more epochs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsv999kK48O0"
      },
      "outputs": [],
      "source": [
        "mse[-10:]  #best from 100 x sgd:  1.4973830317612864"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkUb8IOV5tGj"
      },
      "source": [
        "### Still going down\n",
        "\n",
        "Let's increase epochs and see if it levels out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y909tv2N5PBw"
      },
      "outputs": [],
      "source": [
        "w = .5\n",
        "b = .05\n",
        "mse = []\n",
        "epochs = 1000\n",
        "for i in range(epochs):\n",
        "  w,b = full_batch(X, Y, w, b)\n",
        "  Yhat = predict(X, w, b)\n",
        "  mse.append(MSE(Y, Yhat))\n",
        "(w,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S48Hb8tnHxJS"
      },
      "outputs": [],
      "source": [
        "np.argmin(mse), min(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7T0Lu-l5PBx"
      },
      "outputs": [],
      "source": [
        "mse[-10:]  #still going down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZBNZobH5rU6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mse, scaley=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbkmR-Kh53Ji"
      },
      "source": [
        "100 x SGD still better than 1000 x full-batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiNuTus048O1"
      },
      "source": [
        "# Last step: testing\n",
        "\n",
        "So we trained our model, i.e., computed what look like good values for w and b. Let's try those on the data we held out in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NesuNq0b48O1"
      },
      "outputs": [],
      "source": [
        "(w,b)  #last values computed from full-batch at 1000 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKIaVYat48O1"
      },
      "outputs": [],
      "source": [
        "X = X_test_transformed['Age'].to_list()\n",
        "Y = X_test_transformed['Fare'].to_list()\n",
        "Yhat = predict(X, w, b)\n",
        "MSE(Y, Yhat)  #1.5976846580111859 - best from training: 1.5081800105509628\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM9M2csbnCVO"
      },
      "source": [
        "### Again, training better\n",
        "\n",
        "Typical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcXLznhj48O1"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "Full batch has the benefit of avoiding updating the weights constantly. And it might have a smoothing effect and not jump all over the place: it averages all the gradients to get one change.\n",
        "\n",
        "It has the problem of potentially taking many more epochs than plain sgd so it all might average out.\n",
        "\n",
        "There is a compromise called mini-batch. It says take k rows before doing update. If k=1 you have SGD. If k=all rows then you have full batch. You can choose other values between. Mini-batch is the typical way people train their models. The tough part is choosing k.\n",
        "\n",
        "Also note that there are alternatives to gradient descent. Here is one recent idea based on [Direct Feedback Alignment](https://arxiv.org/abs/2011.12428). No derivatives involved!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1KMg53y_Yh"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "This tests out your Gemini skills. See if you can find the linear regression model in sklearn and then train it using `X` and `y` below.\n",
        "\n",
        "Notice the hoops I had to jump through with reshaping to get data in shape for the sklearn model. Kind of a pain.\n",
        "\n",
        "<pre>\n",
        "a = np.array([ 1.,  2.,  3.,  4.,  5.,  6.])\n",
        "a.reshape(-1,1)\n",
        "array([[ 1.],\n",
        "       [ 2.],\n",
        "       [ 3.],\n",
        "       [ 4.],\n",
        "       [ 5.],\n",
        "       [ 6.]])\n",
        "</pre>\n",
        "But it kind of makes sense. Normally each row would have 6 features, not just 1. So we need an array to hold each row.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvt4ev_VDz5s"
      },
      "outputs": [],
      "source": [
        "X = X_train_transformed['Age'].to_numpy().reshape(-1, 1)  #see example above for reshape action\n",
        "y = X_train_transformed['Fare'].to_numpy().reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6XXN6vtDnJR"
      },
      "outputs": [],
      "source": [
        "#Find the model to import then train it using X and y (ages and fares)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNFishy7xa5g"
      },
      "outputs": [],
      "source": [
        "#This is way we can get w and b from the trained model\n",
        "\n",
        "print(model.coef_, model.intercept_)  #[[0.18813259]] [0.51682211] - I am getting different results here on different runs. Unclear why.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUCbN8p1oJL"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "Go ahead and get predictions. Remember that reshaping issue.\n",
        "\n",
        "And remember you are predicting on the test set not the training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3tlQMcZwt6c"
      },
      "outputs": [],
      "source": [
        "#google for method that will give you predictions from a model\n",
        "predictions =\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP6OfVvbyr96"
      },
      "outputs": [],
      "source": [
        "print(predictions[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZOFGM5ic2iD"
      },
      "source": [
        "<pre>\n",
        "[[0.5069204 ]\n",
        " [0.33859124]\n",
        " [0.7049547 ]\n",
        " [0.55642898]\n",
        " [0.44751011]]\n",
        " </pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4hLvlPX294L"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "Last step. Ask Gemini about sklearn and mean square error function. Apply it to `predictions` you have. You will see that sklearn out did us. Has lower mse.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9lFR0B2HTw"
      },
      "outputs": [],
      "source": [
        "\n",
        "mse ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGGQ5f4qzYbd"
      },
      "outputs": [],
      "source": [
        "print(mse)  #1.5703918246214141 vs home grown 1.5971286308679484"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx2eF2Zfjq6s"
      },
      "source": [
        "### Looks like sklearn beating our home-grown!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zki_T9X7dfrd"
      },
      "source": [
        "# X. Logistic Regression\n",
        "\n",
        "We have been looking at linear regression, where our target column is continuous (as opposed to categorical or binary). We can modify linear regression to get to a new algorithm that is good for binary prediction in particular, e.g., the `Survived` column. The new algorithm is called Logistic Regression. All it does is add another step to computing yhat. So we still do this:\n",
        "<pre>\n",
        "yraw = xi*w+b\n",
        "</pre>\n",
        "But we now add:\n",
        "<pre>\n",
        "yhat = sigmoid(yraw)  #value between 0 and 1 inclusive\n",
        "</pre>\n",
        "I'll give you a brief rundown of the sigmoid function (also called the logistic function). Its definition is below.\n",
        "<pre>\n",
        "S(x) = 1/(1 + e**-x)\n",
        "</pre>\n",
        "Here is its plot:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/o49viq5kz4lgm15/Screen%20Shot%202021-09-24%20at%208.49.07%20AM.png?raw=1' height=200>\n",
        "\n",
        "So the x axis is the raw output from `xi*w+b`. That gets mapped to values between 0 and 1 on the y axis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRUSIdF4hUcp"
      },
      "source": [
        "## New loss/cost function\n",
        "\n",
        "The typical loss function used with logistic regression is as follows (fancy name is *cross-entropy loss*):\n",
        "\n",
        "<img src='https://www.dropbox.com/s/2mwvgbp0mfxwkgs/Screen%20Shot%202021-09-24%20at%208.57.36%20AM.png?raw=1' height=75>\n",
        "\n",
        "Where `h(x)` is `x*w+b` which is the raw output of linear regression. And `y` is the actual value, i.e., the binary label.\n",
        "\n",
        "This new cost function is not exactly intuitive. I am going to point you to a good tutorial if you want to dig deeper: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdJJra11ko1_"
      },
      "source": [
        "## New functions mean expanding chain rule\n",
        "\n",
        "Gradient descent does not change. It just gets a bit more complicated. Before we were computing partial derivatives for MSE. Now we have cross-entropy loss. And further complicating things, we have added the sigmoid function into the mix. So now we have something like f(g(h(x))), i.e., 3 nested functions where f is loss, g is sigmoid and h is raw linear. But good news, the chain rule still works. And all 3 functions have derivatives. I'll skip the details but give you a place to follow along if interested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUKx6bJlsTgI"
      },
      "source": [
        "### But just for giggles, derivate of sigmoid\n",
        "\n",
        "<pre>\n",
        "S(x) = 1/(1 + e**-x)\n",
        "\n",
        "dS(x)/dx = S(x)*(1 - S(x))\n",
        "</pre>\n",
        "\n",
        "[Here is derivation in long hand](https://hausetutorials.netlify.app/posts/2019-12-01-neural-networks-deriving-the-sigmoid-derivative/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odU2k9XujgRy"
      },
      "source": [
        "### Further reading\n",
        "\n",
        "I found [this paper](https://web.stanford.edu/~jurafsky/slp3/5.pdf) a good intro with examples of the various aspects of logistic regression.\n",
        "\n",
        "We will put the logistic regression algorithm to use in the next chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9EIoEra4NA9"
      },
      "source": [
        "# III. Summary\n",
        "\n",
        "We introduced concepts that we will see over and over. The idea of training on one set then testing on another. The idea of gradient descent to accomplish \"learning\". The use of a learning rate to moderate changes. The ideas of batch and epochs used during training.\n",
        "\n",
        "We saw that we need a cost or loss function. For linear regression that is typically MSE. For logistic regression that is typically cross-entropy loss.\n",
        "\n",
        "One reason I like starting with the relatively simple examples of linear and logistic regression is because they have many of the features found in the highly touted concept of deep learning. We will get to deep learning later in the course, and can look back at concepts here as a grounding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CkFUuwPyFjg"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Compute `gradient_w` and `gradient_b` for xi = `ages[1]`. Use given weight values to start with.\n",
        "\n",
        "Use `(yhat - yi)**2` as squared error function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwP-wCQrnYmZ"
      },
      "outputs": [],
      "source": [
        "ages = X_train_transformed['Age'].to_list()\n",
        "fare_labels = X_train_transformed['Fare'].to_list()  #the yi values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0bjKipQX8oL"
      },
      "outputs": [],
      "source": [
        "ages[:10]  #Reminder what in ages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H587jb3AYCzL"
      },
      "outputs": [],
      "source": [
        "fare_labels[:10]  #Reminder what in fares (what we are tring to predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCTHK7dsx-Pb"
      },
      "outputs": [],
      "source": [
        "#starting values\n",
        "\n",
        "xi = ages[1]\n",
        "w,b = (0.4587647933330355, 0.002021042256967478)\n",
        "lr = .001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2exDuwJW3yJw"
      },
      "outputs": [],
      "source": [
        "#your code below to compute 2 gradients\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ag81TjOY8VY"
      },
      "outputs": [],
      "source": [
        "(gradient_w, gradient_b)  #(3.185032883898595, -2.420624991762932)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZXyXqzD4ehQ"
      },
      "source": [
        "### Notice positive slope\n",
        "\n",
        "So on right hand side of curve. Need to move left, i.e., decrease weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mGgdSTD4IJd"
      },
      "source": [
        "### Compute new weight values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llm7CAHO4ZN8"
      },
      "outputs": [],
      "source": [
        "#your code below\n",
        "w =\n",
        "b =\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxMKpRD-ZFre"
      },
      "outputs": [],
      "source": [
        "(w,b)  #new: (0.4555797604491369, 0.004441667248730409), old: (0.4587647933330355, 0.002021042256967478)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO830Kj844n1"
      },
      "source": [
        "They are smaller. That's what we expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aFJ_9BT5Wfz"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "Bring in customer database.\n",
        "\n",
        "Then follow steps below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjCxL2AQ5r7e"
      },
      "source": [
        "## Step 1.\n",
        "\n",
        "Split then pipeline both pieces. You should have value of `random_state` from prior notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mV_4aTzo1Bi"
      },
      "outputs": [],
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQPM6PqZXgmAHfRYTcDZseyALRyVwkBtKEo_rtaKq_C7T0jycWxH6QVEzTzJCRA0m8Vz0k68eM9tDm-/pub?output=csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEQwkk2Co1Bi"
      },
      "outputs": [],
      "source": [
        "customers_df = pd.read_csv(url)\n",
        "customers_trimmed = customers_df.drop(columns='ID')  #this is a useless column which we will drop early\n",
        "customers_trimmed = customers_trimmed.drop_duplicates(ignore_index=True)  #get rid of any duplicates\n",
        "customers_trimmed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFw3ajT5fN_d"
      },
      "outputs": [],
      "source": [
        "customers_features = customers_trimmed.drop(columns=['Rating'])\n",
        "labels = customers_trimmed['Rating'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5XN6ImKT1qn"
      },
      "outputs": [],
      "source": [
        "#remember to use the random seed that you found earlier\n",
        "X_train, X_test, y_train, y_test ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8G9VTEna75R"
      },
      "source": [
        "### Apply pipeline to X_train and X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpryVIeA647F"
      },
      "outputs": [],
      "source": [
        "X_train_transformed ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlWY9x_9647F"
      },
      "outputs": [],
      "source": [
        "X_train_transformed.head().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajI-9Bk-TkQX"
      },
      "source": [
        "|index|Gender|Experience Level|Time Spent|OS|ISP|Age|Rating|\n",
        "|---|---|---|---|---|---|---|---|\n",
        "|586|0\\.6|1\\.0|0\\.5|0\\.0|0\\.26|-0\\.36|0\\.0|\n",
        "|590|0\\.0|1\\.0|-0\\.5|1\\.0|0\\.28|-0\\.6|0\\.0|\n",
        "|137|0\\.0|0\\.8|-0\\.83|0\\.2|0\\.28|0\\.0|0\\.0|\n",
        "|900|1\\.0|1\\.0|0\\.69|0\\.6|0\\.38|-0\\.19|1\\.0|\n",
        "|967|1\\.0|0\\.0|0\\.62|1\\.0|0\\.38|0\\.07|1\\.0|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcy7RyRU647F"
      },
      "outputs": [],
      "source": [
        "X_test_transformed ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWNQQYI5647F"
      },
      "outputs": [],
      "source": [
        "X_test_transformed.head().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvJqUAuxTpHy"
      },
      "source": [
        "|index|Gender|Experience Level|Time Spent|OS|ISP|Age|Rating|\n",
        "|---|---|---|---|---|---|---|---|\n",
        "|329|0\\.0|1\\.0|-0\\.39|0\\.0|0\\.38|-0\\.4|0\\.0|\n",
        "|175|0\\.6|1\\.0|-0\\.16|1\\.0|0\\.34|-0\\.2|0\\.0|\n",
        "|756|0\\.0|2\\.0|-0\\.25|0\\.0|0\\.38|-0\\.16|0\\.0|\n",
        "|936|0\\.0|0\\.0|-1\\.64|0\\.0|0\\.28|-0\\.07|1\\.0|\n",
        "|360|1\\.0|1\\.0|0\\.83|1\\.0|0\\.38|-0\\.53|1\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVwcCK8E526p"
      },
      "source": [
        "## Step 2.\n",
        "\n",
        "Plot `Age` (x axis) against `Time Spent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syrIzjya750Q"
      },
      "outputs": [],
      "source": [
        "ages = X_train_transformed['Age'].to_list()\n",
        "time_labels = X_train_transformed['Time Spent'].to_list()  #the yi values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPz0O0oU750Q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(ages,time_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf1FSyEaTvw9"
      },
      "source": [
        "<img src='https://www.dropbox.com/scl/fi/gs991l1qw6lawc0q845xt/Screenshot-2023-09-08-at-5.20.25-PM.png?rlkey=98qn0xlj5qlrpawk4lajrpmwh&raw=1' height=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzPN3q9b8Ue5"
      },
      "source": [
        "### It looks better than titanic plot for sure\n",
        "\n",
        "Points more clustered.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclNOgYk8ckI"
      },
      "source": [
        "## Step 3.\n",
        "\n",
        "We are trying to predict `Time Spent` from `Age`. Run `sgd` with 500 epochs. Start with `w=.5` and `b=.5`. Use the default `lr` of `sgd`.\n",
        "\n",
        "Record MSE for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE7c9h9f88dt"
      },
      "outputs": [],
      "source": [
        "#your code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkRSBrOwGrzq"
      },
      "outputs": [],
      "source": [
        "np.argmin(mse), min(mse)  #(np.int64(55), 0.5700791544738669)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yzECpY488dt"
      },
      "outputs": [],
      "source": [
        "#look at last 10\n",
        "\n",
        "mse[-10:]  #slightly worse than min!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxgwWfe9o59"
      },
      "source": [
        "## Step 4.\n",
        "\n",
        "Plot epochs (x axis) against MSE values per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R59TLBc88du"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mse, scaley=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROo_Lm_0-LF2"
      },
      "source": [
        "## Step 5.\n",
        "\n",
        "Remember best epoch. I'll do this step for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhfapmLh-fNx"
      },
      "outputs": [],
      "source": [
        "best_epoch = np.argmin(mse)\n",
        "best_epoch  #55"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPvkOb3B_l20"
      },
      "source": [
        "## Step 6.\n",
        "\n",
        "We need the w and b that go with the best epoch. Run the loop again but now only for best number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLj59CeztI9I"
      },
      "outputs": [],
      "source": [
        "#your code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvrPiB-Gb1Ag"
      },
      "outputs": [],
      "source": [
        "(w,b)  #(-0.002657121483286056, 0.046704409344519374)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0GiRvbga6dY"
      },
      "source": [
        "## Step 7.\n",
        "\n",
        "Use best w,b to get error using `X_test_transformed`.\n",
        "\n",
        "No need for loops. We are done training and think we have best values for w and b. Time to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXHS5muZ_f6O"
      },
      "outputs": [],
      "source": [
        "#your code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1HAEJpnc_z6"
      },
      "outputs": [],
      "source": [
        "mse  #0.5505493458871747 compared with best from training of 0.5700791544738669"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AIEqufuda0L"
      },
      "source": [
        "So we are doing better on testing. This is atypical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nYFUYnvdntl"
      },
      "source": [
        "# Challenge 3\n",
        "\n",
        "There is another algorithm called mini-batch where you get to set that batch size between SGD (batch=1) and full batch. Let's say you set batch=10. Then you accumulate for 10 rows then adjust weights, acculate next 10 rows then adjust weights, etc.\n",
        "\n",
        "Build a new function mini_batch that allows you to choose batch size. As reminder, here is the full batch code:\n",
        "\n",
        "<pre>\n",
        "def full_batch(X,Y,w,b,lr=.001):\n",
        "  gw = []\n",
        "  gb = []\n",
        "  for i in range(len(X)):\n",
        "    xi = X[i]\n",
        "    yi = Y[i]\n",
        "    yhat = w*xi+b  #prediction\n",
        "    gradient_w = 2*(yhat-yi)*xi\n",
        "    gradient_b = 2*(yhat-yi)*1\n",
        "    gw.append(gradient_w)\n",
        "    gb.append(gradient_b)\n",
        "  w = w - lr*sum(gw)/len(gw)\n",
        "  b = b - lr*sum(gb)/len(gb)\n",
        "  return w,b\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSkZ4Zx1pLR2"
      },
      "source": [
        "One tricky part is that when you choose a value of k as your mini-batch value, k may not divide evenly into n (the length of full dataset). In that case, take the residue and average it and update weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjPttUBPfIlm"
      },
      "outputs": [],
      "source": [
        "#fill it out\n",
        "\n",
        "def mini_batch(X,Y,w,b,batch=1,lr=.001):\n",
        "  assert batch>=1\n",
        "  assert batch<=len(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VagOPwDdjXx-"
      },
      "outputs": [],
      "source": [
        "X = X_train_transformed['Age'].to_list()\n",
        "Y = X_train_transformed['Time Spent'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP-wxVVlDifl"
      },
      "outputs": [],
      "source": [
        "len(X)  #781"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOJcJCOR6MJt"
      },
      "source": [
        "## Let's see if we can show that mini_batch covers all bases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTVf-jzX5nqH"
      },
      "outputs": [],
      "source": [
        "w = .5\n",
        "b = .05\n",
        "(full_batch(X,Y,w,b), mini_batch(X,Y,w,b,batch=len(X)))  #mini-batch can simulate full-batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDokwx-DUlki"
      },
      "source": [
        "<pre>\n",
        "((0.49958844946297865, 0.0500973053438601),\n",
        " (np.float64(0.49958844946297865), np.float64(0.0500973053438601)))\n",
        " </pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7JSXu1w59m4"
      },
      "outputs": [],
      "source": [
        "w = .5\n",
        "b = .05\n",
        "sgd(X,Y,w,b), mini_batch(X,Y,w,b,batch=1)  #mini-batch can simulate sgd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xfrem6hUrhy"
      },
      "source": [
        "<pre>\n",
        "((0.26499131362527334, 0.08688112174484347),\n",
        " (np.float64(0.26499131362527334), np.float64(0.08688112174484347)))\n",
        " </pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRJ5YBRF6Zlz"
      },
      "source": [
        "## Try it on intermediate value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEuliYROfPDQ"
      },
      "outputs": [],
      "source": [
        "w = 0\n",
        "b = .05\n",
        "mse = []\n",
        "batch = int(.05*len(X))  #39\n",
        "epochs = 1000\n",
        "for i in range(epochs):\n",
        "  w,b = mini_batch(X, Y, w, b, batch=batch)\n",
        "  Yhat = predict(X, w, b)\n",
        "  mse.append(MSE(Y, Yhat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvEukWiQnL3W"
      },
      "outputs": [],
      "source": [
        "batch  #39"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H0HT9cno2nj"
      },
      "outputs": [],
      "source": [
        "(w,b)  #(np.float64(0.0030885999040983106), np.float64(0.019853134271679156))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpd784TvvSLT"
      },
      "outputs": [],
      "source": [
        "best_epoch = np.argmin(mse)\n",
        "(best_epoch, mse[best_epoch])  #(np.int64(18), np.float64(0.5698837838326896))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeVB8uY2n1LZ"
      },
      "outputs": [],
      "source": [
        "mse[-10:]  #worse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnm6rN0wIc3O"
      },
      "source": [
        "<pre>\n",
        "[np.float64(0.5700555056408864),\n",
        " np.float64(0.5700555056408505),\n",
        " np.float64(0.5700555056408148),\n",
        " np.float64(0.57005550564078),\n",
        " np.float64(0.5700555056407456),\n",
        " np.float64(0.5700555056407125),\n",
        " np.float64(0.5700555056406799),\n",
        " np.float64(0.570055505640647),\n",
        " np.float64(0.5700555056406141),\n",
        " np.float64(0.570055505640583)]\n",
        " </pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb41Rwz0wqT5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(mse, scaley=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Eqj9E-_wC4I"
      },
      "source": [
        "### Big drop then levels off\n",
        "\n",
        "<img src='https://www.dropbox.com/scl/fi/e93kkstk0a0fgxdz45041/Screenshot-2025-04-23-at-10.01.18-AM.png?rlkey=7xttflj6nzwrntgt9d66zc4mv&raw=1'>\n",
        "\n",
        "So 18 is the bottom (best) point.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaoVGfyvp8Jm"
      },
      "source": [
        "### Testing\n",
        "\n",
        "Re-train with best epoch, get the w and b values and plug in to get MSE for test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OLICSXh3nbI"
      },
      "outputs": [],
      "source": [
        "X = X_train_transformed['Age'].to_list()\n",
        "Y = X_train_transformed['Time Spent'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hGL4vn9VpPH"
      },
      "outputs": [],
      "source": [
        "#your code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe60Hl7U22tK"
      },
      "outputs": [],
      "source": [
        "Yhat = predict(X_test_transformed['Age'].to_list(), w, b)\n",
        "MSE(X_test_transformed['Time Spent'].to_list(), Yhat)  #0.5488488521821334, best from training: 0.5698837838326896"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLNh9nDb7Vx2"
      },
      "source": [
        "## Better than training\n",
        "\n",
        "Atypical.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
