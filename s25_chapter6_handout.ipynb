{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarvNC/cs523/blob/master/s25_chapter6_handout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mESqPOkEL3oQ"
      },
      "source": [
        "# Preface\n",
        "\n",
        "I changed challenge 2. The video suggests you just plug-in `KNNImputer`. I am now asking you to write a custom class that wraps `KNNImputer`. So you will need to add it, and some auxiliary code (e.g., `set_config`) that goes with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>Chapter Six</h1>\n",
        "</center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## LEARNING OBJECTIVES:\n",
        "- Look at final wrangling step, imputation. Introduce several alternatives.\n",
        "- Test processing time for alternatives. Becomes important in production system.\n",
        "- Capture final choice, KNNImputer, in a custom Transformer.\n",
        "- Place it correctly in Pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npa06nZLAvs-"
      },
      "source": [
        "# I. Imputation\n",
        "\n",
        "The last thing I would like to consider in our pipeline is dealing with those NaN values. Most machine learning algorithms will choke on them. So we need to replace them with numbers. The process is called imputation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZiquu_S3vZG"
      },
      "source": [
        "## Set-up\n",
        "\n",
        "First bring in your library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ6MOQmuVewi"
      },
      "outputs": [],
      "source": [
        "github_name = 'smith'\n",
        "repo_name = 'cis423'\n",
        "source_file = 'library.py'\n",
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HoABkzXubqy"
      },
      "outputs": [],
      "source": [
        "type(CustomRobustTransformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl6YCVfp1uyT"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/fickas/asynch_models/refs/heads/main/datasets/titanic_trimmed.csv'\n",
        "titanic_table = pd.read_csv(url)  #using our new package to read in an entire dataset - the coolest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoSy4udQ1uyT"
      },
      "outputs": [],
      "source": [
        "titanic_table.head()  #print first 5 rows of the table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNDkOpZ3Yg3D"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic_table.drop(columns='Survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJHlx2tTBNNq"
      },
      "source": [
        "## Wrangle using your pipeline\n",
        "\n",
        "I added a Tukey check on `Age`. As we know from the last chapter, this does not cause any clipping with Titanic data. However, we may see new data so want it in place. I am using the outer fence, meaning I am only interested in \"probables\".\n",
        "\n",
        "If you are still working on the `TukeyTransformer` from last chapter, you can leave it off for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGEsWzCTVBrc"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A6SchrlVqh2"
      },
      "source": [
        "## I'm going to add Tukey to Age and Fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-eO6gReCT-8"
      },
      "outputs": [],
      "source": [
        "#don't change this code - it loads from your library\n",
        "\n",
        "transformed_df = titanic_transformer.fit_transform(titanic_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR2eIY3VUx86"
      },
      "outputs": [],
      "source": [
        "transformed_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erzita4sNHly"
      },
      "source": [
        "|index|Age|Gender|Class|Married|Fare|Joined\\_Belfast|Joined\\_Cherbourg|Joined\\_Queenstown|Joined\\_Southampton|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|0|0\\.5526315789473685|0|1\\.0|0\\.0|-0\\.2553191489361702|0|0|0|1|\n",
        "|1|-0\\.5|0|0\\.0|0\\.0|-0\\.5531914893617021|0|0|0|1|\n",
        "|2|-0\\.9210526315789473|0|1\\.0|NaN|0\\.2978723404255319|0|0|0|1|\n",
        "|3|-0\\.7631578947368421|0|1\\.0|0\\.0|NaN|0|0|0|1|\n",
        "|4|NaN|0|2\\.0|0\\.0|0\\.46808510638297873|0|1|0|0|\n",
        "|5|0\\.18421052631578946|0|NaN|1\\.0|-0\\.5531914893617021|0|0|0|1|\n",
        "|6|0\\.7631578947368421|0|1\\.0|0\\.0|-0\\.2553191489361702|0|1|0|0|\n",
        "|7|-0\\.6052631578947368|0|1\\.0|0\\.0|-0\\.2553191489361702|0|1|0|0|\n",
        "|8|-0\\.23684210526315788|0|1\\.0|0\\.0|-0\\.2127659574468085|0|0|0|1|\n",
        "|9|-0\\.23684210526315788|0|0\\.0|0\\.0|-0\\.5531914893617021|0|0|0|1|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSESSeTbVJK1"
      },
      "source": [
        "# II. Not all NaNs the same\n",
        "\n",
        "The question is how did the NaN end up there in the first place? Here is a general classification.\n",
        "\n",
        "* Missing completely at random (MCAR). Cannot be tied to any known variable or event. Perhaps random data entry error. Contains no interesting information.\n",
        "\n",
        "* Missing at random (MAR). A bit of a misnomer, the missing value is linked to a known cause. Perhaps one of the ship staff was bad at filling in data. So there is a cause: careless staff. But we have lost that information by this time.\n",
        "\n",
        "* Not missing at random (NMAR). Maybe some passengers did not want to give their nationality because of fear of discrimination. So certain nationalities go missing. We might be able to infer a value from other columns, e.g., Fare, Class.\n",
        "\n",
        "The question is whether we can detect these 3 cases? If we have a NaN in the Age column, can I differentiate among these 3?\n",
        "\n",
        "We will end up doing a bit of NMAR: try to infer values using other columns. But first let's look at a few simpler approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kc8-e8hgPFz"
      },
      "source": [
        "# III. Simplest approach: deletion\n",
        "\n",
        "We could simply delete all rows that have a NaN in any column. Could do it with one line of pandas:\n",
        "<pre>\n",
        "dropped_table = titanic_features.dropna(axis=0)  #axis=0 says rows\n",
        "</pre>\n",
        "But I don't like doing that. Especially with smallish datasets. It loses information. I'd rather try to repair such rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhh40b19fTnw"
      },
      "source": [
        "# IV. Use stats to replace NaNs\n",
        "\n",
        "I'll cover 2 general ways to impute a value for a NaN. The first is to use descriptive statistics for each column. So if we have NaNs in the `Age` column, compute the mean of the column and use it to replace the NaNs. Could also use the median or the mode.\n",
        "\n",
        "The code is simple:\n",
        "<pre>\n",
        "transformed_df['Fare'] = transformed_df['Fare'].fillna(value=transformed_df['Fare'].mean())\n",
        "</pre>\n",
        "And here is the good news. We can use one of our existing transformers to do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_16QPW2idAR"
      },
      "outputs": [],
      "source": [
        "scaler = CustomMappingTransformer('Fare', {np.nan: transformed_df['Fare'].mean()})  #need np.nan to match a NaN in table\n",
        "new_df = scaler.fit_transform(transformed_df)\n",
        "print(transformed_df['Fare'].isna().sum(), new_df['Fare'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEPge1IplmtF"
      },
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voB09RLPj3PR"
      },
      "source": [
        "## sklearn has mean transformer built-in\n",
        "\n",
        "It will compute the means for all the columns and fill in NaN values for us in one fell swoop. Cool.\n",
        "\n",
        "<pre>\n",
        "means = MeanImputerTransformer()\n",
        "new_df = means.fit_transform(transformed_df)\n",
        "</pre>\n",
        "\n",
        "I won't give this as a challenge, but I hope you feel confident you could write it if you needed to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yam-NuWI-h26"
      },
      "source": [
        "## The problem\n",
        "\n",
        "Using things like the mean assumes a normal distribution. That may be true of the `Age` column, but not the `Fare`. I think I have a better way.\n",
        "\n",
        "The new way will look at rows instead of columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvPhqDTWtuLe"
      },
      "source": [
        "# V. Use crowd sourcing to replace NaNs\n",
        "\n",
        "In essence, if we have a NaN in a row, use the other rows to infer the value. So if we have a NaN for the age of Mr. Smith, look at all the other rows. See if we can find characteristics from these other rows that will allow us to fill in a value for Smith. For instance, if Smith is in first class, married and paid 50 pounds for his ticket, find other males who were in first class, married, and paid a similar amount. Get their ages. Maybe average to get Smith's age.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-RsT5vqmlp3"
      },
      "source": [
        "## K Nearest Neighbors (KNN)\n",
        "\n",
        "The good news is that there is a well-known technique for doing what I described: finding the rows closest to Smith and then averaging results. It is called KNN. And sklearn has an imputer built on it. Hurray.\n",
        "\n",
        "I'll give you a brief intro to KNN and then see it in action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_r_2g62d3KM"
      },
      "source": [
        "\n",
        "## First insight: recast as a Geometry problem\n",
        "<img src='https://www.dropbox.com/s/9fcc1crlxp19ijt/major_section.png?raw=1' width='300'>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhtR4U9h3ehW"
      },
      "source": [
        "\n",
        "Our goal is to compute how \"similar\" 2 rows (2 lists of numbers) are.\n",
        "Let's take the view that each list of numbers is actually a point in 8-dimension space. Sounds kind of scary already! Let me give you the intuition pretending we only have 2 numbers in each list. Call the two lists A and B. We can view each list as a point on a 2D plot as shown below. Ignore the angle theta for now.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/7rtuzw37hgl1oi8/ed_vs_cos.png?raw=1' height=300>\n",
        "\n",
        "We can use the distance d as a measure of their similarity. The smaller the distance, the closer they are. If the distance is 0, they are the same point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os8rW0ZcxJjD"
      },
      "source": [
        "### Good news!\n",
        "<img src='https://www.sapaviva.com/wp-content/uploads/2017/06/6S.-Euclid-of-Alexandria-ca.320-275-BC-225x225.jpg' height='100'>\n",
        "\n",
        "Euclid (circa 300BC) figured it out for us. He came up with a formula for computing d. It defines the \"Euclidean distance\" between 2 points (here called p and q instead of A and B) as follows.\n",
        "\n",
        "  <p>\n",
        "<img src='https://www.dropbox.com/s/9wao0kf3u32i3e9/euclidian.png?raw=1'>\n",
        "\n",
        "Linking our plot above into this:\n",
        "\n",
        "* n = 2. We have 2 values in each list.\n",
        "* q1 is the same as x1, p1 is the same as x2.\n",
        "* q2 is the same as y1, p2 is the same as y2.\n",
        "\n",
        "The capital (Greek) letter sigma represents summation. I'll write the code out long hand for you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V-JJvuH_dxL"
      },
      "outputs": [],
      "source": [
        "p = [1,3]  #just guessing from the plot\n",
        "q = [3,2]  #ditto\n",
        "n = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwcaAry6_vxN"
      },
      "outputs": [],
      "source": [
        "greek_sigma = (p[0]-q[0])**2 + (p[1]-q[1])**2\n",
        "greek_sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJsjxmdlAT92"
      },
      "outputs": [],
      "source": [
        "d = greek_sigma**.5\n",
        "d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGtnHTdhoURe"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "\n",
        "Rewrite code above to use a list comprehension that will compute distance for any 2 lists of numbers p and q.\n",
        "\n",
        "Answer: 2.23606797749979 for p and q above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyJuiIM1on8W"
      },
      "outputs": [],
      "source": [
        "#I did it in one line by summing list comprehension than taking square root.\n",
        "#Answer: 2.23606797749979\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9WxonxxBHtc"
      },
      "source": [
        "### Does 2.23606797749979 mean the 2 points are close?\n",
        "\n",
        "Euclidean distance just gives you the distance. Makes no value judgement. We do know that a distance of 0 says the points are exactly the same. So that is the min value. But there is no limit on the upper value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksDu7F8KYa3p"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "Go ahead and try your solution on the first 2 rows from transformed_df.\n",
        "\n",
        "\n",
        "###My answer: 1.4821474866516537\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDpPQJvO3gEW"
      },
      "outputs": [],
      "source": [
        "row1 = transformed_df.loc[0].to_list()\n",
        "row2 = transformed_df.loc[1].to_list()\n",
        "print(row1)  #[0.5526315789473685, 0.0, 1.0, 0.0, -0.2553191489361702, 0.0, 0.0, 0.0, 1.0]\n",
        "print(row2)  #[-0.5, 0.0, 0.0, 0.0, -0.5531914893617021, 0.0, 0.0, 0.0, 1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSnkVoY3eRPU"
      },
      "outputs": [],
      "source": [
        "#you can use your solution from above here.\n",
        "#Answer: #1.4821474866516537\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsEQYYMNz8D6"
      },
      "source": [
        "## We have a similarity!\n",
        "\n",
        "We checked a person (row1) with another person (row2) and found they had similarity score of `1.6`. But wait. Is that good? I'm going to punt on that question for now. What I want to do with KNN is find the Euclidean distance between row1 and every other row. So if there are 2000 people/rows , I will come up with a list of 1999 distances. I then choose the ones with the smallest distance. These become my experts.\n",
        "\n",
        "Let's pretend that we are dubious of the `Married` column in row1. We want the experts opinion on what the married value should be for row1.\n",
        "The next step is to have the experts vote on the `Married` value. If most of the experts have a 0 (actually a value <.5) in `Married`, then the voting result is 0. If most have 1 (a value >=.5), then voting result is 1. I take the vote result and that is my prediction.\n",
        "\n",
        "I was a bit vague with this: choose the ones with the smallest distance. How many should I choose? That is where the K in KNN comes in. I choose the top K. You get to choose the value of K. A typical value is 5. But it could easily be the case that 11 or even higher will give better results. With binary columns like `Married`, I like to use an odd number to avoid ties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtw7NK4bpumR"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "\n",
        "Among the 4 rows with indices 6 through 9, find the 4 distances to row1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVR7GbH3qEQm"
      },
      "outputs": [],
      "source": [
        "row1 = transformed_df.loc[0].to_list()  #row at indice 0\n",
        "transformed_df[6:10]  #here are the 4 rows we will compare (differs from video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBcPQGGjSVHN"
      },
      "source": [
        "I used a combo of a for loop and list comprehension to create `diffs` (list of pairs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIpkoRcGqvv5"
      },
      "outputs": [],
      "source": [
        "#your code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fqUM_ptSQ2L"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(diffs)  #[(6, 1.4297976533901184), (7, 1.8277637214931934), (8, 0.7906196760559859), (9, 1.3084328906182103)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JajGfXtlrriH"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "Ok, use the `sorted` function to sort on distances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WH5ybDWrwu9"
      },
      "outputs": [],
      "source": [
        "#here is answer from last quiz in case you did not get it.\n",
        "\n",
        "diffs = [(6, 1.4297976533901184), (7, 1.8277637214931934), (8, 0.7906196760559859), (9, 1.3084328906182103)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPwy3r5hr8nF"
      },
      "outputs": [],
      "source": [
        "#sort on distance (2nd item). Reminder: sorted function takes a key argument so you can sort on 2nd item in pair\n",
        "\n",
        "sdist =\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdsiSy7pS0Eq"
      },
      "outputs": [],
      "source": [
        "print(sdist)  #[(8, 0.7906196760559859), (9, 1.3084328906182103), (6, 1.4297976533901184), (7, 1.8277637214931934)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmFreDnp5jd"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "Pull off top k rows, i.e., just their indices. I came up with the list `[8, 9, 6]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJJpHNElsuVv"
      },
      "outputs": [],
      "source": [
        "#now get indices of top k rows in terms of closeness to row 1\n",
        "k = 3\n",
        "#your code below\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNu6PkklS31A"
      },
      "outputs": [],
      "source": [
        "knn_row_indices  #[8, 9, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUcJRM8n7iPN"
      },
      "source": [
        "## Let's say we did not know the marital status of row1\n",
        "\n",
        "Let our top 3 rows vote!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBkVO_i57sjq"
      },
      "outputs": [],
      "source": [
        "the_votes = [transformed_df.loc[i, 'Married'] for i in knn_row_indices]\n",
        "married_average = sum(the_votes)/len(the_votes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEAC11X_THWm"
      },
      "outputs": [],
      "source": [
        "(the_votes, married_average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afmv-qnqqp0g"
      },
      "source": [
        "## Votes correct?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvabD29yqsw5"
      },
      "outputs": [],
      "source": [
        "transformed_df.loc[0, 'Married']  #real answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVLXL7ByqX_E"
      },
      "source": [
        "## I hope you see I only have to compute `sdist` once\n",
        "\n",
        "Then I can choose how many to pull off the top (i.e., k) to vote."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcsXB01v3G5l"
      },
      "source": [
        "\n",
        "## cosine similarity\n",
        "<img src='https://www.dropbox.com/s/9fcc1crlxp19ijt/major_section.png?raw=1' width='300'>\n",
        "\n",
        "We have been using Euclidean Distance to measure the similarity of 2 lists of numbers. Are there alternatives? Yes. One fairly common measure is *cosine similarity*. Let's look at this diagram again.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/7rtuzw37hgl1oi8/ed_vs_cos.png?raw=1' height=300>\n",
        "\n",
        "We know that d represents the euclidean distance. But you also see the greek letter theta that measures an angle. We got the angle by drawing 2 lines both starting at (0,0). One line goes to A and the other line goes to B. What I want to do is measure the cosine of the angle (theta) that I get when I draw these lines.\n",
        "\n",
        "Cosine, residing in both Geometry and Trigonometry, brings new jargon. The lines are called *vectors*. I find it all kind of interesting. Nothing has really changed on our side. We have 2 lists of numbers. But we can take different mathematical perspectives (distance versus angle) to get more abstract views. And this gives us mathematical tools we can use. Cool.\n",
        "\n",
        "I know you probably have the formula for the cosine of theta sribbled somewhere from your math courses. But just in case, I'll give it to you below.\n",
        "\n",
        "<img src='https://www.dropbox.com/s/oi1ttx99hf0uejn/cosine.png?raw=1'>\n",
        "\n",
        "**What are A and B in this formula?** They are 2 lists of values, one from a row in `nan_table` and one from a row in `crowd_table`.\n",
        "\n",
        "Looking at the right-hand side of the formula, **what are A-sub-i and B-sub-i**? In Python, they would be A[i] and B[i].\n",
        "\n",
        "**What is the greek letter sigma?** We have seen it before. It is summation.\n",
        "There is one tricky part. You can see that on the sigmas they have i=1, meaning they assume that the lists are indexed 1,2,3, etc. As any sane person would do it. But you know in Python land, you have to have i=0.\n",
        "\n",
        "\n",
        "The official description of the left-hand version of the formula is for the **numerator** we are taking the *dot-product* of 2 vectors and for the **denominator** we are multiplying the *L2 norms* of the 2 vectors (parallel bars around A and B). The formula is nice because you can see how the jargony terms translate into algebra and things we know how to do in pure Python. I can also tell you that `numpy` knows how to deal with the dot-product and L2 norms directly. So when you import numpy, you bring in another abstraction level, one that knows about *linear algebra*.\n",
        "\n",
        "One nice thing about the cosine is that it is bounded between -1 and 1. A 1 means the same point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFNKsVsd_f_g"
      },
      "source": [
        "# VIII. The KNNImputer\n",
        "\n",
        "The folks at sklearn have built-in a transformer that will impute all the NaN values in our table using KNN. And get this - it even works if a row has more than one NaN value! Nice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmMdP-Z9p98z"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5,        #a rough guess\n",
        "                     weights=\"uniform\",    #could alternatively have distance factor in\n",
        "                     add_indicator=False)  #do not add extra column for NaN\n",
        "\n",
        "imputed_data = imputer.fit_transform(transformed_df)  #instantiate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqb-JH2qWRRZ"
      },
      "source": [
        "### Did we get them all?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqRicVn3V9UR"
      },
      "outputs": [],
      "source": [
        "np.isnan(imputed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngJAl8jxWFL7"
      },
      "outputs": [],
      "source": [
        "np.isnan(imputed_data).any()  #Any Trues in there?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-2gQD-kMFg"
      },
      "source": [
        "I used 3 parameters in above code. Let's look at the larger parameter list (and the defaults).\n",
        "\n",
        "* `n_neighbors`, default=`5`.\n",
        "Number of neighboring rows to use for imputation. If the available rows is less than this value, then what is available is used.\n",
        "\n",
        "* `weights`, default=`’uniform’`. All columns treated the same with uniform. It is possible to weight certain columns to make them more important.\n",
        "\n",
        "* `metric`, default=`’nan_euclidean’`.\n",
        "Distance metric for searching neighbors. If we want cosine instead, it is possible to write our own function and pass it as a value here. Have to be careful that our function can deal with NaN values. Since I omitted it, I get default.\n",
        "\n",
        "* `add_indicator`, default=`False`. This will add new binary columns to indicate a cell with a imputed value. See discussion below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XjunLygBvJ0"
      },
      "source": [
        "## Problem is we have numpy matrix as output\n",
        "\n",
        "Even though we can take dataframe as input, cannot get anything but numpy matrix as output. I'll ask you to fix that in a challenge by wrapping KNNImputer in your own class. This should be kind of standard by now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTDUwlxgCDs1"
      },
      "source": [
        "## Important note on `add_indicator`\n",
        "\n",
        "If `add_indicator` is set to True, it will create a whole new column for each column that has at least one NaN. Might be easiest to look at example. Here is table that has NaNs in first 3 columns but not in 4th."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grIGLsCOxpps"
      },
      "outputs": [],
      "source": [
        "data = [[1,2,np.nan,10],\n",
        "        [np.nan,3,np.nan,10],\n",
        "        [5,np.nan,6,10],\n",
        "        [7,8,9,10]]\n",
        "test_df = pd.DataFrame(data, columns=['a','b','c','d'])\n",
        "test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3sWmNG72ynO"
      },
      "outputs": [],
      "source": [
        "imputer = KNNImputer(add_indicator=True)\n",
        "data = imputer.fit_transform(test_df)\n",
        "pd.DataFrame(data, columns=['a','b','c','d','nan_a', 'nan_b', 'nan_c'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwLO-4gezpW4"
      },
      "source": [
        "As you can see, 3 new columns that mark whether a row had a NaN in a column. There are two reasons I don't like this.\n",
        "\n",
        "1. I said we were going to treat NaNs as carrying no information. So we do not need to mark them specially. I hope you see if they did carry information, then the new columns would carry that forward even after the NaNs replaced, i.e., a machine learning algorithm might be able to make use of the new columns.\n",
        "\n",
        "2. I don't like adding new columns like this. We may end up with different sized tables between exploration and production. That would be bad.\n",
        "\n",
        "So bottomline is to set `add_indicator=False`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0VQk_bSr5kH"
      },
      "source": [
        "## I'm going to time it because that will become important later\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdO3lYVbZKjK"
      },
      "outputs": [],
      "source": [
        "test_df  #has NaN values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btEMAHsl71kj"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "imputer = KNNImputer(add_indicator=False)\n",
        "data = imputer.fit_transform(test_df)\n",
        "data  #NaNs imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCPWqbdYMzaD"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "data = imputer.fit_transform(transformed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqvOJntMWCT4"
      },
      "source": [
        "### Roughly `5ms` for entire table\n",
        "\n",
        ".005 of a second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPrYK83tDhC8"
      },
      "source": [
        "# IX. Using the `IterativeImputer`\n",
        "\n",
        "I like this imputer quite a bit. It is really a framework or workflow for a more base predictor; you have to pass the base predictor you want as an argument to the imputer. What it then does is finds the first feature/column that has one or more NaNs. It treats this column as the target/label column y. It uses all the other features/columns to predict NaN values in this column.\n",
        "\n",
        "When it is done with one column, i.e., has predicted values for all the NaNs in that column, it moves on to the next. In this way it iterates through all the columns with NaNs and predicts values for each NaN.\n",
        "\n",
        "There are more details, but I would rather put them off for a subsequent chapter. Here is general use:\n",
        "<pre>\n",
        "from sklearn... import some_predictor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "ii_imp = IterativeImputer(\n",
        "    estimator=some_predictor(), max_iter=10, random_state=1234)\n",
        "\n",
        "numpy_matrix = ii_imp.fit_transform(old_df)\n",
        "</pre>\n",
        "What is missing is \"`some_predictor`\". We have to choose what predictor to use. Let's try a few on our small test table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDhDCRE4hh9X"
      },
      "outputs": [],
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "#3 different choices for \"some_predictor\"\n",
        "from sklearn.tree import DecisionTreeRegressor    #alternative 1\n",
        "from sklearn.ensemble import ExtraTreesRegressor  #alternative 2\n",
        "from sklearn.linear_model import BayesianRidge    #alternative 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX9q_-3T1sfG"
      },
      "source": [
        "So I've imported 3 candidates to try for `some_predictor`. We will try each one in turn, first in simple 4 row table then the full Titanic table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSpj2h4GyVq4"
      },
      "source": [
        "### Note: wall time is what we focus on\n",
        "\n",
        "This would be the time your end-user would have to wait for an answer.\n",
        "\n",
        "**Caveat**: wall time is machine dependent. If you are running on a machine with multiple cores (like Colab) then your wall time could easily be (much) smaller than CPU time given parallelism is in play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pft1S_ugilsF"
      },
      "source": [
        "## Start with `DecisionTreeRegressor`\n",
        "\n",
        "A plain decision tree is used. We will discuss decision trees in more detail later in the course.\n",
        "\n",
        "[Docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EomUF5HSY4PB"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqvIh9mKgUAv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "estimator = DecisionTreeRegressor()  #instantiate predictor\n",
        "ii_imp = IterativeImputer(estimator=estimator, max_iter=10, random_state=1234)  #instantiate imputer\n",
        "\n",
        "data = ii_imp.fit_transform(test_df)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVmIJmOkLTKL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "data = ii_imp.fit_transform(transformed_df)  #4.5 times slower than knn (differs from video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG6OXTJ6fCFy"
      },
      "source": [
        "## Next up is `ExtraTreesRegressor`\n",
        "\n",
        "Briefly, it uses a \"forest\" of decision trees to make predictions. Each tree gets a vote. It is a twiddle on something you may have heard: Random Forests. It generally outshines plain decision trees.\n",
        "\n",
        "[Docs](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC8goyeffS2s"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okMFUGilfwvo"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "ii_imp = IterativeImputer(estimator=ExtraTreesRegressor(), max_iter=10, random_state=1234)\n",
        "\n",
        "data = ii_imp.fit_transform(test_df)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLtn7ui-MOEt"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "data = ii_imp.fit_transform(transformed_df)   #11.8K vs 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR8cb9_khz-f"
      },
      "source": [
        "## Finally we have `BayesianRidge`\n",
        "\n",
        "This is a fancy form of regression. And yes, I am putting it off and regression until a later chapter. I mostly want to illustrate that you can try different methods using the `IterativeImputer`. We first looked at trees. Now switching to (fancy) regression.\n",
        "\n",
        "[Docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html)\n",
        "\n",
        "If you just can't wait until later, here is a [good tutorial](https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/) on Ridge Regression (and Lasso as well). And a [tutorial on Bayesian regression](https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZCPHiknf7sR"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "ii_imp = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=1234)\n",
        "\n",
        "data = ii_imp.fit_transform(test_df)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cpUqPTONpna"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "data = ii_imp.fit_transform(transformed_df)   #slower (note parallelism in play by comparing to CPU time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZZmw_mRjNV-"
      },
      "source": [
        "# X. Which is best?\n",
        "\n",
        "My gut reaction is that the `IterativeImputer` with `ExtraTreesRegressor` is the best. I'd have to do a more sophisticated analysis to back that up, e.g., place some synthetic NaN values in the table but remember their actual value and compare with imputation.\n",
        "\n",
        "The problem is the time the best takes. When I ran it on the actual Titanic data (a very small dataset), it was taking 2 seconds. That may be too slow if we are running a web-site where a user is waiting for a response. Because of this I am going to push ahead with the `KNNImputer`. It is the fastest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PYA9mw6mcvV"
      },
      "source": [
        "# Challenge 1\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "<img src='https://www.dropbox.com/s/oi1ttx99hf0uejn/cosine.png?raw=1'>\n",
        "\n",
        "Please compute the cosine similarity between the first 2 rows in `transformed_df`. If you are good at reading linear algebra notation, then know that numpy supplies:\n",
        "\n",
        "* A method for doing dot product.\n",
        "\n",
        "* A method for computing L2 norm.\n",
        "\n",
        "If you are shaky on the notation, you can resort to using 3 list comprehensions for the 3 sums.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCe3qbFWlRsD"
      },
      "outputs": [],
      "source": [
        "row1 = transformed_df.loc[0].to_list()\n",
        "row2 = transformed_df.loc[1].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqWgzMHB7PQ0"
      },
      "outputs": [],
      "source": [
        "#compute and place answer in r1r2_cosine_similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT96Z7h3YJ28"
      },
      "outputs": [],
      "source": [
        "r1r2_cosine_similarity  #0.4503419058458463"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxLPt_j89PPw"
      },
      "source": [
        "Here is an oracle, although a little complicated in its details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgB267yn8xHe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(np.array(row1).reshape(1, -1), np.array(row2).reshape(1, -1))[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVQ1bAu84ZiE"
      },
      "source": [
        "### Can we use the `cosine_similarity` function with KNNImputer?\n",
        "\n",
        "Yes and no. Yes, we could pass it in as a parameter when we instantiate. No, it won't work because it does not handle NaN values well.\n",
        "\n",
        "We could write our own `nan_cosine_similarity` function that does handle NaN values and then pass it in, but I won't ask you to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJGn0De5vl4p"
      },
      "source": [
        "# Challenge 2\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Can we just plug the `KNNImputer` into our pipepline? One problem is that it does not return a dataframe but a numpy array. But there is a way around that. I can use `set_config` to tell sklearn that I want all its built-in transformers to input and output dataframes. Cool, check it out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtu9p1Crhor8"
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config  #add both of these to your library before you define any transformers\n",
        "\n",
        "set_config(transform_output=\"pandas\")  #have all transformers (including KNNImputer) now deal in dataframes versus numpy arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kb9iITXJB6O"
      },
      "source": [
        "So that problem is solved. However, there is one more issue. I do not want `add_indicator` to be settable to `True`. It will totally mess up our data later. So I am still going to ask you to write a custom transformer that basically wraps `KNNImputer` and hard-codes `add_indicator` to be set to `False`. For those of you who have taken an FP class, this is in the flavor of a currying operation, albeit in OOP land versus FP land.\n",
        "\n",
        "Note that you will need to import `KNNImputer` before defining the class; you will need to create an instance of it in the `__init__` method. So make sure this import (and all the imports you see in our notebooks) end up in your library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YmWc1rc6TWt"
      },
      "source": [
        "### One other constraint\n",
        "\n",
        "`KNNImputer` is set up to have a `fit` method: it will cause an error if `transform` is called before `fit`. Make sure your class is set up the same. Your `fit` method should simply call the `KNNImputer` `fit` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOGipcEfWNqu"
      },
      "source": [
        "### A fancy type hint\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Trying to ensure n_neighbors positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5osTJufU-jc"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "PositiveInta = Annotated[int, lambda x: x > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSXxaVDmVyjq"
      },
      "outputs": [],
      "source": [
        "from annotated_types import Gt\n",
        "\n",
        "PositiveIntb = Annotated[int, Gt(0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRJxQODdVHpY"
      },
      "outputs": [],
      "source": [
        "def foo(x: PositiveIntb):\n",
        "  print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLfWEs3eV7WX"
      },
      "outputs": [],
      "source": [
        "foo(0)   #should warn with red line but does not - hmmmmm\n",
        "foo(.4)  #should warn with red line and does"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7zhdIJ8VD02"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLXmdqaiWs8K"
      },
      "outputs": [],
      "source": [
        "class CustomKNNTransformer(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"Imputes missing values using KNN.\n",
        "\n",
        "  This transformer wraps the KNNImputer from scikit-learn and hard-codes\n",
        "  add_indicator to be False. It also ensures that the input and output\n",
        "  are pandas DataFrames.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  n_neighbors : int, default=5\n",
        "      Number of neighboring samples to use for imputation.\n",
        "  weights : {'uniform', 'distance'}, default='uniform'\n",
        "      Weight function used in prediction. Possible values:\n",
        "      \"uniform\" : uniform weights. All points in each neighborhood\n",
        "      are weighted equally.\n",
        "      \"distance\" : weight points by the inverse of their distance.\n",
        "      in this case, closer neighbors of a query point will have a\n",
        "      greater influence than neighbors which are further away.\n",
        "  \"\"\"\n",
        "  #your code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5oJXrNi7aVD"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4qMP1Ib86CA"
      },
      "source": [
        "### Make sure `fit` called before `transform`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM33RpY8Qd0y"
      },
      "outputs": [],
      "source": [
        "#Hint: see attributes of KNNImputer that are set after fitting. Can check with hasattr function.\n",
        "\n",
        "my_knn_imputer = CustomKNNTransformer()\n",
        "new_df = my_knn_imputer.transform(test_df)  #AssertionError: NotFittedError: This CustomKNNTransformer instance is not fitted yet. Call \"fit\" with appropriate arguments before using this estimator.\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jT4OODJZhWT"
      },
      "source": [
        "### Check K>samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv-VG5ECZhWU"
      },
      "outputs": [],
      "source": [
        "my_knn_imputer = CustomKNNTransformer(n_neighbors=len(test_df)+1)\n",
        "my_knn_imputer.fit(test_df)  #Warning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFeGQmDhX7aS"
      },
      "source": [
        "### Check fit versus transform columns mismatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aags8rJPYHqw"
      },
      "outputs": [],
      "source": [
        "my_knn_imputer = CustomKNNTransformer()\n",
        "my_knn_imputer.fit(test_df)\n",
        "my_knn_imputer.transform(test_df.drop(columns='c'))  # Warning and Column names mismatch error\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtNiJm3aFwZ"
      },
      "source": [
        "### Normal test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wwc0nk7635V"
      },
      "outputs": [],
      "source": [
        "my_knn_imputer = CustomKNNTransformer()\n",
        "new_df = my_knn_imputer.fit_transform(test_df)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f58EURv8tpyi"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/q54k7qetwarr5mz/Screen%20Shot%202022-01-20%20at%202.58.28%20PM.png?raw=1' height=150>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKYlqj_0aEfN"
      },
      "source": [
        "### A few more tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw18MkshaImV"
      },
      "outputs": [],
      "source": [
        "my_knn_imputer2 = CustomKNNTransformer(n_neighbors=2)\n",
        "new_df = my_knn_imputer2.fit_transform(test_df)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3hLDuv-cGUs"
      },
      "source": [
        "<pre>\n",
        "\n",
        "a\tb\tc\td\n",
        "0\t1.0\t2.0\t7.5\t10.0\n",
        "1\t3.0\t3.0\t7.5\t10.0\n",
        "2\t5.0\t5.5\t6.0\t10.0\n",
        "3\t7.0\t8.0\t9.0\t10.0\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GKheGOYaVVM"
      },
      "outputs": [],
      "source": [
        "my_knn_imputer3 = CustomKNNTransformer(n_neighbors=2, weights='distance')\n",
        "new_df = my_knn_imputer3.fit_transform(test_df)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrnE9oZVcNDH"
      },
      "source": [
        "|index|a|b|c|d|\n",
        "|---|---|---|---|---|\n",
        "|0|1\\.0|2\\.0|7\\.098076211353315|10\\.0|\n",
        "|1|5\\.0|3\\.0|6\\.0|10\\.0|\n",
        "|2|5\\.0|3\\.0|6\\.0|10\\.0|\n",
        "|3|7\\.0|8\\.0|9\\.0|10\\.0|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XGp7PJEadMC"
      },
      "outputs": [],
      "source": [
        "my_knn_imputer4 = CustomKNNTransformer(n_neighbors=1, weights='distance')\n",
        "new_df = my_knn_imputer4.fit_transform(test_df)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2EMgC_tASuW"
      },
      "source": [
        "|index|a|b|c|d|\n",
        "|---|---|---|---|---|\n",
        "|0|1\\.0|2\\.0|6\\.0|10\\.0|\n",
        "|1|5\\.0|3\\.0|6\\.0|10\\.0|\n",
        "|2|5\\.0|3\\.0|6\\.0|10\\.0|\n",
        "|3|7\\.0|8\\.0|9\\.0|10\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCBZo1s6nbHA"
      },
      "source": [
        "Go ahead and try it on `transformed_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_NIyGDvg8at"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "new_df = my_knn_imputer.fit_transform(transformed_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxjBtpJv5Mic"
      },
      "outputs": [],
      "source": [
        "new_df.describe(include='all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZzEbG3woEZR"
      },
      "source": [
        "|index|count|mean|std|min|25%|50%|75%|max|\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "|Age|1313\\.0|0\\.05787870285004208|0\\.753051356322997|-1\\.5526315789473684|-0\\.4473684210526316|0\\.02631578947368421|0\\.5526315789473685|2\\.289473684210526|\n",
        "|Gender|1313\\.0|0\\.3488194973343488|0\\.47677833844440637|0\\.0|0\\.0|0\\.0|1\\.0|1\\.0|\n",
        "|Class|1313\\.0|1\\.3972581873571972|1\\.0428875432917633|0\\.0|1\\.0|1\\.0|2\\.0|3\\.0|\n",
        "|Married|1313\\.0|0\\.34364051789794364|0\\.4749754473150905|0\\.0|0\\.0|0\\.0|1\\.0|1\\.0|\n",
        "|Fare|1313\\.0|0\\.5221593557064381|1\\.222958326660415|-0\\.5531914893617021|-0\\.2553191489361702|0\\.0|0\\.7659574468085106|3\\.74468085106383|\n",
        "|Joined\\_Belfast|1313\\.0|0\\.06473724295506474|0\\.24615539898364328|0\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|Joined\\_Cherbourg|1313\\.0|0\\.19268849961919268|0\\.3945607792644899|0\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|Joined\\_Queenstown|1313\\.0|0\\.06930693069306931|0\\.2540721241868543|0\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|Joined\\_Southampton|1313\\.0|0\\.6732673267326733|0\\.4691972932315876|0\\.0|0\\.0|1\\.0|1\\.0|1\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWAhk9ovoL2Y"
      },
      "source": [
        "### Looks good\n",
        "\n",
        "No NaNs remaining (`count` is `1313` for all columns).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oel0fTVBlpD"
      },
      "source": [
        "# Challenge 3\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Try it on customer data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mV_4aTzo1Bi"
      },
      "outputs": [],
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQPM6PqZXgmAHfRYTcDZseyALRyVwkBtKEo_rtaKq_C7T0jycWxH6QVEzTzJCRA0m8Vz0k68eM9tDm-/pub?output=csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEQwkk2Co1Bi"
      },
      "outputs": [],
      "source": [
        "customers_df = pd.read_csv(url)\n",
        "customers_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53sJ-STTZUDl"
      },
      "outputs": [],
      "source": [
        "customers_features = customers_df.drop(columns='Rating')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmcc1oHxrGsE"
      },
      "source": [
        "## Step 1. Build pipeline and add imputer\n",
        "\n",
        "I count 8 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYxtig9qo1Bj"
      },
      "outputs": [],
      "source": [
        "#Build pipeline and include scalers from last chapter and imputer from this\n",
        "customer_transformer = Pipeline(steps=[\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oyzgUkprMbP"
      },
      "source": [
        "## Step 2. Test out  `KNNTransformer`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNzpsDllB_s0"
      },
      "outputs": [],
      "source": [
        "transformed_customer_df = customer_transformer.fit_transform(customers_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMNP5Sg4qtKA"
      },
      "outputs": [],
      "source": [
        "transformed_customer_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n0JMOYwMPOS"
      },
      "source": [
        "|index|Gender|Experience Level|Time Spent|Age|OS\\_Android|OS\\_iOS|ISP\\_AT&amp;T|ISP\\_Cox|ISP\\_HughesNet|ISP\\_Xfinity|\n",
        "|---|---|---|---|---|---|---|---|---|---|---|\n",
        "|0|1\\.0|1\\.0|0\\.7184632599776198|-0\\.041379310344827586|0\\.0|1\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|1|0\\.0|1\\.0|-1\\.6057441253263711|-0\\.6896551724137931|1\\.0|0\\.0|0\\.0|1\\.0|0\\.0|0\\.0|\n",
        "|2|1\\.0|1\\.0|0\\.6202909362178289|-0\\.7586206896551724|0\\.0|0\\.0|0\\.0|1\\.0|0\\.0|0\\.0|\n",
        "|3|1\\.0|1\\.0|-0\\.5315180902648265|-0\\.4827586206896552|1\\.0|0\\.0|0\\.0|0\\.0|0\\.0|1\\.0|\n",
        "|4|1\\.0|1\\.0|0\\.7814248414770603|-0\\.13793103448275862|0\\.0|1\\.0|0\\.0|0\\.0|0\\.0|1\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ayicj3o1Bo"
      },
      "source": [
        "# Challenge 4\n",
        "<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n",
        "\n",
        "Add `CustomKNNTransformer` to your library. It is what we will use in our pipeline to do imputation.\n",
        "\n",
        "As a preprocessing step, I'd recommend asking Gemini to do full documentation and type hints on it and add that to your library.\n",
        "\n",
        "Also update your pipeline."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
