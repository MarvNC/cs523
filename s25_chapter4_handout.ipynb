{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX_AHUIIpBrz"
      },
      "source": [
        "# Preface\n",
        "\n",
        "I updated the challenges slightly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzDiYMkiYgRS"
      },
      "source": [
        "<center>\n",
        "<h1>Chapter Four</h1>\n",
        "</center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "## LEARNING OBJECTIVES:\n",
        "- Semi-deep dive into the wrangling of outliers in your data. Introduce a set of alternative methods to consider.\n",
        "- Capture several in custom Transformers.\n",
        "- Place them correctly in Pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npa06nZLAvs-"
      },
      "source": [
        "# I. Outliers\n",
        "\n",
        "We will look at both identifying and handling outliers in a column. I'll introduce 2 different (but related) identification methods. For handling, we will use the same simple method.\n",
        "\n",
        "Be aware we could also look for entire rows as outliers: [Sample outliers](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor). Interesting, but we will focus on outliers in a single cell in a column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztVnEk6C1pPb"
      },
      "source": [
        "## Set-up\n",
        "\n",
        "First bring in your library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ6MOQmuVewi"
      },
      "outputs": [],
      "source": [
        "github_name = 'smith'\n",
        "repo_name = 'mlops'\n",
        "source_file = 'library.py'\n",
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZqcJSxA6v1y"
      },
      "source": [
        "## Make sure transformers from chapter 3 are in your library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAUvxVCo5bta"
      },
      "outputs": [],
      "source": [
        "type(CustomMappingTransformer), type(CustomOHETransformer)  #(type, type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxXGgriLK4Pl"
      },
      "source": [
        "## Now titanic dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl6YCVfp1uyT"
      },
      "outputs": [],
      "source": [
        "\n",
        "url = 'https://raw.githubusercontent.com/fickas/asynch_models/refs/heads/main/datasets/titanic_trimmed.csv'\n",
        "\n",
        "titanic_table = pd.read_csv(url)  #using our new package to read in an entire dataset - the coolest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoSy4udQ1uyT"
      },
      "outputs": [],
      "source": [
        "titanic_table.head()  #print first 5 rows of the table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hM3yv_gNjiv"
      },
      "source": [
        "## Remove label column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDzt6jKTNoAo"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic_table.drop(columns='Survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJHlx2tTBNNq"
      },
      "source": [
        "## Wrangle using your pipeline\n",
        "\n",
        "Should be in your library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-eO6gReCT-8"
      },
      "outputs": [],
      "source": [
        "transformed_df = titanic_transformer.fit_transform(titanic_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR2eIY3VUx86"
      },
      "outputs": [],
      "source": [
        "transformed_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y1huouejzY5"
      },
      "source": [
        "# II. Wrangle outliers\n",
        "<img src='https://www.dropbox.com/s/9fcc1crlxp19ijt/major_section.png?raw=1' width='300'>\n",
        "\n",
        "We spent last week wrangling our string columns into numeric form. In this chapter, we will focus on the numeric columns and look at outliers in those columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl7J-ZPU-4xj"
      },
      "source": [
        "## The outlier problem\n",
        "\n",
        "Outliers are data values that are extreme, either on high end or on low end. What is meant by \"extreme\" depends on the outlier-detection methods we use. We will explore two.\n",
        "\n",
        "The problem  with outliers are twofold:\n",
        "\n",
        "1. They may be mistakes. When data was being entered, a typo was made. Looking at the Titanic data, there is a fare of 512. Perhaps that should have been 51.2 but a typo was made. So an outlier might not be a legitimate value.\n",
        "\n",
        "2. Even if they are legit,\n",
        "outliers can screw up our prediction models. Some machine learning models will give too much credence to outliers and skew results.\n",
        "\n",
        "Our goal is to (a) identify outliers, and then (b) transform outliers into a more acceptable form. I am going to use two standard identification approaches called the 3Sigma rule and the Tukey (\"two key\") rule. However, there are  others to choose from. Here is a start if you are interested: [Variety of outlier methods](https://towardsdatascience.com/detecting-and-treating-outliers-in-python-part-1-4ece5098b755). Here are a few more sophisticated approaches: [More sophisticated outlier methods](https://medium.com/swlh/top-five-methods-to-identify-outliers-in-data-2777a87dd7fe)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGEJpcfpkTR8"
      },
      "source": [
        "# III. The 3 Sigma rule\n",
        "\n",
        "The first outlier identification we will look at is called 3 Sigma, where Sigma stands for standard deviation. It's relatively simple. Given a column like Fare, find the mean and standard deviation. Set boundaries as follows: low boundary is `mean - 3 * sigma`; the high boundary is `mean + 3 * sigma`.\n",
        "Values beyond that boundary wall may be considered outliers. So we add, to the mean, 3 times the standard deviation to get the upper boundary and minus 3 times the standard deviation to get the lower boundary.\n",
        "The lowercase Greek letter mu (µ) represents the mean.\n",
        "\n",
        "<img src='https://www.dropbox.com/scl/fi/t6ja1id9ybmnol3t0zblb/Screenshot-2025-01-03-at-9.25.18-AM.png?rlkey=t8n52kfd4b015yb6o6h7au587&raw=1' height=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUV3oBwgClFw"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/8x575mvbi1xumje/cash_line.png?raw=1' height=3 width=500><br>\n",
        "<img src='https://www.gannett-cdn.com/-mm-/56cbeec8287997813f287995de67747ba5e101d5/c=9-0-1280-718/local/-/media/2018/02/15/Phoenix/Phoenix/636542954131413889-image.jpg'\n",
        "height=50 align=center>\n",
        "\n",
        "\n",
        "Let's write a function that will compute the low and high boundaries of a column.\n",
        "\n",
        "Return a tuple of low boundary and high boundary.\n",
        "\n",
        "Hint: no loops needed if can find correct pandas methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5U_ctpzi-FI"
      },
      "outputs": [],
      "source": [
        "# I'll get us started\n",
        "\n",
        "def compute_3sigma_boundaries(df: pd.DataFrame, column_name: str) -> Tuple[float, float]:\n",
        "  \"\"\"Computes the low and high boundaries of a column using the 3 Sigma rule.\n",
        "  Given a column like Fare, find the mean and standard deviation. Set boundaries as follows:\n",
        "  low boundary is `mean - 3 * sigma`; the high boundary is `mean + 3 * sigma`.\n",
        "  Values beyond that boundary wall may be considered outliers.\n",
        "  The lowercase Greek letter mu (µ) represents the mean.\n",
        "\n",
        "  Args:\n",
        "    df: The input DataFrame.\n",
        "    column_name: The name of the column to compute boundaries for.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the low and high boundaries.\n",
        "  \"\"\"\n",
        "  assert isinstance(df, pd.core.frame.DataFrame), f'expected Dataframe but got {type(df)} instead.'\n",
        "  assert column_name in df.columns.to_list(), f'unknown column {column_name}'\n",
        "  assert pd.api.types.is_numeric_dtype(df[column_name]), f'expected int or float in column {column_name}'\n",
        "\n",
        "\n",
        "  #your code below\n",
        "\n",
        "  # Compute the mean and standard deviation of the column\n",
        "\n",
        "\n",
        "  # Compute the low and high boundaries\n",
        "\n",
        "\n",
        "  # Return the boundaries\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ihcGWSFHE8"
      },
      "source": [
        "## Test on the Fare column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L_65G3kFLKo"
      },
      "outputs": [],
      "source": [
        "s3min, s3max = compute_3sigma_boundaries(transformed_df, 'Fare')\n",
        "(s3min,s3max)  #(-126.58295661603658, 188.92587042229135)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZc--i91Gi-B"
      },
      "outputs": [],
      "source": [
        "(transformed_df['Fare'].min(), transformed_df['Fare'].max())  #(0.0, 512.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_9NpZHPGxQM"
      },
      "source": [
        "### High outliers\n",
        "\n",
        "Nothing below -126 in fare, but some fares over 189. Let's plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbDYfssS_KKi"
      },
      "outputs": [],
      "source": [
        "ax = transformed_df['Fare'].plot(kind='hist', figsize=(10,8), grid=True, logy=True)  #what we have seen before\n",
        "ax.axvline(x=s3min, ymin=0, ymax=1, color='r', linestyle='--', lw=2)  #low fence\n",
        "ax.axvline(x=s3max, ymin=0, ymax=1, color='r', linestyle='--', lw=2)  #high fence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fth7-PGeBHIo"
      },
      "source": [
        "We can double-check what we see with a couple of pandas methods. For more see here: [skew and kurtosis](https://medium.com/@atanudan/kurtosis-skew-function-in-pandas-aa63d72e20de)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ2DbU_JArJF"
      },
      "outputs": [],
      "source": [
        "transformed_df['Fare'].skew()  #4.2823355053978585 greater than 1 highly skewed right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNj6f7cLAzMI"
      },
      "outputs": [],
      "source": [
        "transformed_df['Fare'].kurtosis()  #26.020798741121023 greater than 0 = \"heavy tails/outliers\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQvIZTetAf0v"
      },
      "source": [
        "It is skewed. And has a heavy tail, meaning lots of values near extremes. I am going to plow ahead with using 3sigma but in reality might be better off  \"unskewing\" the column before employing 3Sigma, e.g., [skew handling](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLVZQtW-LYcJ"
      },
      "source": [
        "### Let's try Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ1rU6dPLhVm"
      },
      "outputs": [],
      "source": [
        "s3min, s3max = compute_3sigma_boundaries(transformed_df, 'Age')\n",
        "(s3min,s3max)  #(-11.466448491618326, 74.6152214977533)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWCmzNDbLcim"
      },
      "outputs": [],
      "source": [
        "ax = transformed_df['Age'].plot(kind='hist', figsize=(10,8), grid=True, logy=True)\n",
        "ax.axvline(x=s3min, ymin=0, ymax=1, color='r', linestyle='--', lw=2)  #low fence\n",
        "ax.axvline(x=s3max, ymin=0, ymax=1, color='r', linestyle='--', lw=2)  #high fence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpUfH1zTM3nM"
      },
      "outputs": [],
      "source": [
        "# Skewness - see skew method doc for more info\n",
        "\n",
        "transformed_df['Age'].skew()  #0.14808832680399128 less than 1 much less skew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0PcWo34M3nN"
      },
      "outputs": [],
      "source": [
        "# Kurtosis - see kurtosis method doc for more info\n",
        "\n",
        "transformed_df['Age'].kurtosis()  #-0.1815192902344882 close to normal if 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BUf6rYoOgTS"
      },
      "source": [
        "# Clip outliers\n",
        "\n",
        "Ok, we found outliers. Now what? How can we deal with them? I am going to suggest a very easy fix. Move the outliers to the boundary.\n",
        "\n",
        "pandas has a method `clip` that is handy for what we want to do: move outliers to their boundaries. Check it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEDV-C0lPAZS"
      },
      "outputs": [],
      "source": [
        "minb, maxb = compute_3sigma_boundaries(transformed_df, 'Fare')\n",
        "transformed_df['Clipped_Fare'] = transformed_df['Fare'].clip(lower=minb, upper=maxb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pxzmzd1N1mJ"
      },
      "outputs": [],
      "source": [
        "s3min, s3max = compute_3sigma_boundaries(transformed_df, 'Fare')\n",
        "ax = transformed_df['Clipped_Fare'].plot(kind='hist', figsize=(10,8), grid=True, logy=True)\n",
        "ax.axvline(x=s3min, ymin=0, ymax=1, color='r', linestyle='--', lw=2)  #low fence\n",
        "ax.axvline(x=s3max, ymin=0, ymax=1, color='r', linestyle='--', lw=2)  #high fence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2MN4jL0EAkf"
      },
      "source": [
        "# IV. The Tukey (two key) method and the boxplot\n",
        "\n",
        "Let's assume we do not like the 3Sigma method and want to try an alternative.\n",
        "In a box plot, introduced by John Tukey in 1970, the data is divided into quartiles. It usually shows a rectangular box representing 25%-75% of a sample’s observations, extended by so-called whiskers that provide a measure of outlierness. Observations shown outside of the whiskers are outliers (explained in more detail below).\n",
        "\n",
        "What is interesting about Tukey is that he decided to define 2 walls/fences on either side. Items between the fences were viewed as \"possible\" outliers; items beyond the outer fence were viewed as \"probable\" outliers. Of course this all depends on the subjective meaning of possible and probable.\n",
        "\n",
        "##The quartiles (25%, 50%, 75%)\n",
        "\n",
        "These can be useful for determining the distribution of a column. And also looking for outliers. Let's say I sort the `Age` column into an ascending list. The 25% quartile tells me where I would have to set the value to get 25% of the data below that value (or on the left of the list). There is a pandas method for giving us the values for the quartiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nue2RmK9TITf"
      },
      "outputs": [],
      "source": [
        "transformed_df['Age'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIN-f28DlK5y"
      },
      "source": [
        "## The boxplot (with modifications)\n",
        "\n",
        "There is a pandas method, `boxplot`, for plotting some of the Tukey info we need. In particlar, it will give us the inner fences (called whiskers) of Tukey. What we need to add are the outer fences. Then everything between inner and outer is possible outliers and everything beyond outer is probable outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgqIQrrta5rE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "column='Age'\n",
        "fig, ax = plt.subplots(1,1, figsize=(3,9))\n",
        "transformed_df.boxplot(column, vert=True, ax=ax, grid=True)  #normal boxplot\n",
        "#now add on outer fences\n",
        "q1 = transformed_df[column].quantile(0.25)\n",
        "q3 = transformed_df[column].quantile(0.75)\n",
        "iqr = q3-q1\n",
        "outer_low = q1-3*iqr\n",
        "outer_high = q3+3*iqr\n",
        "ax.scatter(1, outer_low, c='red', label='outer_low', marker=\"D\", linewidths=5)\n",
        "ax.text(1.1,  outer_low, \"Outer fence\")\n",
        "ax.scatter(1, outer_high, c='red', label='outer_high', marker=\"D\", linewidths=5)\n",
        "ax.text(1.1,  outer_high, \"Outer fence\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jNLytjAUIsQ"
      },
      "source": [
        "### Rotating 90\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*2c21SkzJMf3frPXPAR_gZA.png' height=200>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbLj4uJIQh7p"
      },
      "source": [
        "### No probables for `Age`\n",
        "\n",
        "We have a set of possible points, but nothing beyond outer fences. Let's check out `Fare`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR_Pjvn4RGle"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "column='Fare'\n",
        "fig, ax = plt.subplots(1,1, figsize=(3,9))\n",
        "transformed_df.boxplot(column, vert=True, ax=ax, grid=True)  #normal boxplot\n",
        "# now add on outer fences\n",
        "q1 = transformed_df[column].quantile(0.25)\n",
        "q3 = transformed_df[column].quantile(0.75)\n",
        "iqr = q3-q1\n",
        "outer_low = q1-3*iqr\n",
        "outer_high = q3+3*iqr\n",
        "ax.scatter(1, outer_low, c='red', label='outer_low', marker=\"D\", linewidths=5)\n",
        "ax.text(1.1,  outer_low, \"Outer fence\")\n",
        "ax.scatter(1, outer_high, c='red', label='outer_high', marker=\"D\", linewidths=5)\n",
        "ax.text(1.1,  outer_high, \"Outer fence\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1cC-uHWRMIp"
      },
      "source": [
        "### Lots of probables for `Fare`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zDsVnhKstZE"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Another way to deal with outliers is to remove any row that has an outlier value. Out of sight, out of mind.\n",
        "\n",
        "Let's say we want to remove all rows who have a value in the `Fare` column that is greater than `outer_high`. I'll create a new dataframe called `clean_titanic` where this is the case.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXChwgsrD_V"
      },
      "source": [
        "## Step 1. Create a new table with rows with high **probables** removed\n",
        "\n",
        "Pandas `query` method should be all you need.\n",
        "\n",
        "**But important note**: the `query` method will also drop rows with `NaN` values. If you use something other than `query` to drop (e.g., the `drop` method), this may not be the case and you could end up with results slightly different than 1229 - there are two `NaN` values in the `Fare` column so you could get 1231. That's ok too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bflAkbFg6pNw"
      },
      "source": [
        "### remove\n",
        "<pre>\n",
        "clean_titanic = transformed_df.drop(transformed_df[transformed_df.Fare > outer_high].index).reset_index(drop=True)  #1231\n",
        "</pre>\n",
        "\n",
        "Here is an alternative solution (you could also use `@outer_high` without f string):\n",
        "\n",
        "<pre>\n",
        "clean2_titanic = transformed_df.query(f'Fare <= {outer_high}').reset_index(drop=True)  #2093\n",
        "</pre>\n",
        "\n",
        "So it appears that the query method drops the two NaN columns while the drop method retains them. Interesting! I believe most of you were using `.loc` instead of `.query` but getting same results.\n",
        "\n",
        "For future reference, I would prefer not to drop the rows with NaN values. So when working with rows, use `drop`.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2aY7R1eXpmZ"
      },
      "outputs": [],
      "source": [
        "transformed_df['Fare'].isna().sum()  #2 NaNs in the column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgLbC67HrqVd"
      },
      "outputs": [],
      "source": [
        "cleaned_titanic = transformed_df.query(f'Fare <= {outer_high}')  #or transformed_df.query('Fare <= @outer_high')\n",
        "len(cleaned_titanic)   #1229"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17EPezGir03p"
      },
      "outputs": [],
      "source": [
        "cleaned_titanic.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FIe0j2wwNom"
      },
      "source": [
        "## Step 2. Reset index so no gaps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evcE6-O6sHW4"
      },
      "outputs": [],
      "source": [
        "clean2_titanic = cleaned_titanic.reset_index(drop=True)\n",
        "len(clean2_titanic)  #1229"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxLbA0c6sUa1"
      },
      "outputs": [],
      "source": [
        "clean2_titanic.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-BeczUsvM3"
      },
      "source": [
        "### What I see\n",
        "\n",
        "|index|Age|Gender|Class|Married|Fare|Joined\\_Belfast|Joined\\_Cherbourg|Joined\\_Queenstown|Joined\\_Southampton|Clipped\\_Fare|\n",
        "|---|---|---|---|---|---|---|---|---|---|---|\n",
        "|1224|4\\.0|0|1\\.0|0\\.0|22\\.0|0|1|0|0|22\\.0|\n",
        "|1225|2\\.0|1|1\\.0|0\\.0|22\\.0|0|1|0|0|22\\.0|\n",
        "|1226|23\\.0|1|1\\.0|1\\.0|22\\.0|0|1|0|0|22\\.0|\n",
        "|1227|22\\.0|1|3\\.0|0\\.0|61\\.0|0|0|0|1|61\\.0|\n",
        "|1228|27\\.0|0|1\\.0|0\\.0|7\\.0|0|1|0|0|7\\.0|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-0eVC_5LxBX"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "Build a CustomSigma3Transformer and add it to your library. Test it out below. Use `clip` to do transformation on named column.\n",
        "\n",
        "As with `CustomPearsonTransformer`, the main work should be done by the `fit` method, which computes the two walls. The `transform` method simply clips and resets index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5jm6YoGQiYP"
      },
      "source": [
        "### First define it in notebook and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWYYs9m3L455"
      },
      "outputs": [],
      "source": [
        "class CustomSigma3Transformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A transformer that applies 3-sigma clipping to a specified column in a pandas DataFrame.\n",
        "\n",
        "    This transformer follows the scikit-learn transformer interface and can be used in\n",
        "    a scikit-learn pipeline. It clips values in the target column to be within three standard\n",
        "    deviations from the mean.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    target_column : Hashable\n",
        "        The name of the column to apply 3-sigma clipping on.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    high_wall : Optional[float]\n",
        "        The upper bound for clipping, computed as mean + 3 * standard deviation.\n",
        "    low_wall : Optional[float]\n",
        "        The lower bound for clipping, computed as mean - 3 * standard deviation.\n",
        "    \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i8v8nYGvfuo"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkMYV83zyN1H"
      },
      "outputs": [],
      "source": [
        "sigma3_transformer = CustomSigma3Transformer('fare')\n",
        "new_df1 = sigma3_transformer.fit_transform(transformed_df)  #AssertionError: unknown column fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXWsEHVzvclU"
      },
      "outputs": [],
      "source": [
        "sigma3_transformer = CustomSigma3Transformer('Fare')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duSehWe_vk8N"
      },
      "outputs": [],
      "source": [
        "new_df1 = sigma3_transformer.transform(transformed_df)  #AssertionError: Sigma3Transformer.fit has not been called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abVo7CtpL5kT"
      },
      "outputs": [],
      "source": [
        "new_df1 = sigma3_transformer.fit_transform(transformed_df)\n",
        "(new_df1['Fare'].min(), new_df1['Fare'].max())  #(0.0, 188.92587042229135)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eZc1ZySMRG9"
      },
      "source": [
        "### Now add to your library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFRwz1IwUvj0"
      },
      "source": [
        "# Challenge 3\n",
        "\n",
        "Build a `CustomTukeyTransformer` and add it to your library. Test it out below. Use `clip` to do transformation on named column.\n",
        "\n",
        "Note that Tukey gives us more options for parameters. In particular, it defines inner and outer fences. Add that. BTW: this is another reason for keeping the 2 transformers separate. It gets messy defining parameters that only make sense for one set of options.\n",
        "\n",
        "Follow same scheme as with 3sigma: have the `fit` method compute the walls and the `transform` method simply clips and resets index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hFJB8i5Uvj0"
      },
      "source": [
        "### First define it in notebook and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fLJzUelUvj0"
      },
      "outputs": [],
      "source": [
        "class CustomTukeyTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A transformer that applies Tukey's fences (inner or outer) to a specified column in a pandas DataFrame.\n",
        "\n",
        "    This transformer follows the scikit-learn transformer interface and can be used in a scikit-learn pipeline.\n",
        "    It clips values in the target column based on Tukey's inner or outer fences.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    target_column : Hashable\n",
        "        The name of the column to apply Tukey's fences on.\n",
        "    fence : Literal['inner', 'outer'], default='outer'\n",
        "        Determines whether to use the inner fence (1.5 * IQR) or the outer fence (3.0 * IQR).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    inner_low : Optional[float]\n",
        "        The lower bound for clipping using the inner fence (Q1 - 1.5 * IQR).\n",
        "    outer_low : Optional[float]\n",
        "        The lower bound for clipping using the outer fence (Q1 - 3.0 * IQR).\n",
        "    inner_high : Optional[float]\n",
        "        The upper bound for clipping using the inner fence (Q3 + 1.5 * IQR).\n",
        "    outer_high : Optional[float]\n",
        "        The upper bound for clipping using the outer fence (Q3 + 3.0 * IQR).\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> df = pd.DataFrame({'values': [10, 15, 14, 20, 100, 5, 7]})\n",
        "    >>> tukey_transformer = CustomTukeyTransformer(target_column='values', fence='inner')\n",
        "    >>> transformed_df = tukey_transformer.fit_transform(df)\n",
        "    >>> transformed_df\n",
        "    \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwSScroFxTlK"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq3kgJU-zFWN"
      },
      "outputs": [],
      "source": [
        "tukey_transformer = CustomTukeyTransformer('age', 'inner')\n",
        "new_df1 = tukey_transformer.fit_transform(transformed_df)  #AssertionError: TukeyTransformer: unknown column age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDpITFsos0EM"
      },
      "outputs": [],
      "source": [
        "tukey_transformer = CustomTukeyTransformer('Age', 'inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuLx2ELRxab6"
      },
      "outputs": [],
      "source": [
        "new_df1 = tukey_transformer.transform(transformed_df)  #AssertionError: TukeyTransformer.fit has not been called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKqh3pDHxWx3"
      },
      "outputs": [],
      "source": [
        "new_df1 = tukey_transformer.fit_transform(transformed_df)\n",
        "(new_df1['Age'].min(), new_df1['Age'].max())  #(1.0, 69.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIWkZIXjUvj1"
      },
      "outputs": [],
      "source": [
        "tukey_transformer = CustomTukeyTransformer('Fare', 'inner')\n",
        "new_df1 = tukey_transformer.fit_transform(transformed_df)\n",
        "(new_df1['Fare'].min(), new_df1['Fare'].max())  #(0.0, 65.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMeUidF2dQOr"
      },
      "outputs": [],
      "source": [
        "tukey_transformer = CustomTukeyTransformer('Fare', 'outer')\n",
        "new_df1 = tukey_transformer.fit_transform(transformed_df)\n",
        "(new_df1['Fare'].min(), new_df1['Fare'].max())  #(0.0, 101.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njMzszizUvj1"
      },
      "source": [
        "### Now add to your library ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKd68AjRFkeZ"
      },
      "source": [
        "# Challenge 4\n",
        "\n",
        "Add a new last step to your titanic pipeline in your library:\n",
        "\n",
        "<pre>\n",
        "    ('fare', CustomTukeyTransformer(target_column='Fare', fence='outer')),\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEux6UXVe33q"
      },
      "source": [
        "# Challenge 5\n",
        "\n",
        "Try it out on the customers dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsYnVwe0PFWP"
      },
      "outputs": [],
      "source": [
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQPM6PqZXgmAHfRYTcDZseyALRyVwkBtKEo_rtaKq_C7T0jycWxH6QVEzTzJCRA0m8Vz0k68eM9tDm-/pub?output=csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifwDyZPoQiuc"
      },
      "outputs": [],
      "source": [
        "customers_df = pd.read_csv(url)\n",
        "customers_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZhaahqBx72q"
      },
      "outputs": [],
      "source": [
        "customer_features = customers_df.drop(columns='Rating')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g65WbXM_Av9r"
      },
      "source": [
        "### First copy your pipeline code from Chapter 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjuCjQURWh92"
      },
      "outputs": [],
      "source": [
        "# Chapter 2 pipeline for customer\n",
        "customer_transformer ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdVgjFc4eU6L"
      },
      "outputs": [],
      "source": [
        "transformed_customer_df = customer_transformer.fit_transform(customer_features)\n",
        "transformed_customer_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NlZ9nLouajZ"
      },
      "source": [
        "### What I see\n",
        "\n",
        "<img src='https://www.dropbox.com/s/2n3papl1ujdhysa/Screen%20Shot%202022-10-06%20at%201.59.13%20PM.png?raw=1' height=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXhWq665yfx1"
      },
      "source": [
        "## Step 5.1 Try Tukey on `Time Spent`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH0KsXt9yuL-"
      },
      "outputs": [],
      "source": [
        "(transformed_customer_df['Time Spent'].min(), transformed_customer_df['Time Spent'].max())  #(62.43, 144.95)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6v-GoSLy-Ge"
      },
      "outputs": [],
      "source": [
        "tukey_transformer = CustomTukeyTransformer('Time Spent', 'inner')\n",
        "new_df1 = tukey_transformer.fit_transform(transformed_customer_df)\n",
        "(new_df1['Time Spent'].min(), new_df1['Time Spent'].max())  #(66.97, 120.59)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfJjT6w60vuM"
      },
      "source": [
        "Looks like values clipped on either side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZC4L76l0Wv8"
      },
      "source": [
        "## Step 5.2 Try Sigma3 on `Time Spent`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vqbSM9J0WwF"
      },
      "outputs": [],
      "source": [
        "(transformed_customer_df['Time Spent'].min(), transformed_customer_df['Time Spent'].max())  #(62.43, 144.95)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuKpd7xN0WwF"
      },
      "outputs": [],
      "source": [
        "sima3_transformer = CustomSigma3Transformer('Time Spent')\n",
        "new_df1 = sima3_transformer.fit_transform(transformed_customer_df)\n",
        "(new_df1['Time Spent'].min(), new_df1['Time Spent'].max())  #(62.43, 127.97660489351867)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwCB3l0y001Q"
      },
      "source": [
        "Looks like only clipped on high side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyOmvLQx1EtX"
      },
      "source": [
        "### Reminder\n",
        "\n",
        "Save both your clipping transformers to your library so we can use them next chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bldI87pbTu1u"
      },
      "source": [
        "# Challenge 6\n",
        "\n",
        "Go ahead and add this as last step in your customer pipeline:\n",
        "\n",
        "<pre>\n",
        "    ('time spent', CustomTukeyTransformer('Time Spent', 'inner')),\n",
        "</pre>\n",
        "\n",
        "Edit your GitHub library then reload it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pc7cbIyDzgz"
      },
      "outputs": [],
      "source": [
        "url = f'https://raw.githubusercontent.com/{github_name}/{repo_name}/main/{source_file}'\n",
        "!rm $source_file\n",
        "!wget $url\n",
        "%run -i $source_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GqdIMs8UA7f"
      },
      "outputs": [],
      "source": [
        "transformed_customer_df = customer_transformer.fit_transform(customer_features)\n",
        "transformed_customer_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA_Vl9mVy-vE"
      },
      "source": [
        "|index|Gender|Experience Level|Time Spent|Age|OS\\_Android|OS\\_iOS|ISP\\_AT&amp;T|ISP\\_Cox|ISP\\_HughesNet|ISP\\_Xfinity|\n",
        "|---|---|---|---|---|---|---|---|---|---|---|\n",
        "|0|1\\.0|1\\.0|NaN|NaN|0|1|0|0|0|1|\n",
        "|1|0\\.0|1\\.0|71\\.97|50\\.0|1|0|0|1|0|0|\n",
        "|2|1\\.0|1\\.0|101\\.81|49\\.0|0|0|0|1|0|0|\n",
        "|3|1\\.0|1\\.0|86\\.37|53\\.0|1|0|0|0|0|1|\n",
        "|4|1\\.0|1\\.0|103\\.97|58\\.0|0|1|0|0|0|1|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgEYHqpAEEe7"
      },
      "outputs": [],
      "source": [
        "# Note difference after adding Tukey step\n",
        "\n",
        "(transformed_customer_df['Time Spent'].min(), transformed_customer_df['Time Spent'].max())  #(66.97, 120.59)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
